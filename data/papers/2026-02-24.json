{
  "date": "2026-02-24",
  "fetched_at": "2026-02-27T09:27:20.962196+09:00",
  "papers": [
    {
      "id": "2602.20974",
      "title": "MAST: A Multi-fidelity Augmented Surrogate model via Spatial Trust-weighting",
      "abstract": "In engineering design and scientific computing, computational cost and predictive accuracy are intrinsically coupled. High-fidelity simulations provide accurate predictions but at substantial computational costs, while lower-fidelity approximations offer efficiency at the expense of accuracy. Multi-fidelity surrogate modelling addresses this trade-off by combining abundant low-fidelity data with sparse high-fidelity observations. However, existing methods suffer from expensive training cost or rely on global correlation assumptions that often fail in practice to capture how fidelity relationships vary across the input space, leading to poor performance particularly under tight budget constraints. We introduce MAST, a method that blends corrected low-fidelity observations with high-fidelity predictions, trusting high-fidelity near observed samples and relying on corrected low-fidelity elsewhere. MAST achieves this through explicit discrepancy modelling and distance-based weighting with closed-form variance propagation, producing a single heteroscedastic Gaussian process. Across multi-fidelity synthetic benchmarks, MAST shows a marked improvement over the current state-of-the-art techniques. Crucially, MAST maintains robust performance across varying total budget and fidelity gaps, conditions under which competing methods exhibit significant degradation or unstable behaviour.",
      "authors": [
        "Ahmed Mohamed Eisa Nasr",
        "Haris Moazam Sheikh"
      ],
      "url": "https://arxiv.org/abs/2602.20974",
      "published": "2026-02-24T14:57:22+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20973",
      "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving",
      "abstract": "To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.",
      "authors": [
        "Yuliang Ji",
        "Fuchen Shen",
        "Jian Wu",
        "Qiujie Xie",
        "Yue Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.20973",
      "published": "2026-02-24T14:53:34+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20971",
      "title": "Does Order Matter : Connecting The Law of Robustness to Robust Generalization",
      "abstract": "Bubeck and Sellke (2021) pose as an open problem the connection between the law of robustness and robust generalization. The law of robustness states that overparameterization is necessary for models to interpolate robustly; in particular, robust interpolation requires the learned function to be Lipschitz. Robust generalization asks whether small robust training loss implies small robust test loss. We resolve this problem by explicitly connecting the two for arbitrary data distributions. Specifically, we introduce a nontrivial notion of robust generalization error and convert it into a lower bound on the expected Rademacher complexity of the induced robust loss class. Our bounds recover the $Ω(n^{1/d})$ regime of Wu et al. (2023) and show that, up to constants, robust generalization does not change the order of the Lipschitz constant required for smooth interpolation. We conduct experiments to probe the predicted scaling with dataset size and model capacity, testing whether empirical behavior aligns more closely with the predictions of Bubeck and Sellke (2021) or Wu et al. (2023). For MNIST, we find that the lower-bound Lipschitz constant scales on the order predicted by Wu et al. (2023). Informally, to obtain low robust generalization error, the Lipschitz constant must lie in a range that we bound, and the allowable perturbation radius is linked to the Lipschitz scale.",
      "authors": [
        "Himadri Mandal",
        "Vishnu Varadarajan",
        "Jaee Ponde",
        "Aritra Das",
        "Mihir More",
        "Debayan Gupta"
      ],
      "url": "https://arxiv.org/abs/2602.20971",
      "published": "2026-02-24T14:52:20+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20967",
      "title": "Training-Free Intelligibility-Guided Observation Addition for Noisy ASR",
      "abstract": "Automatic speech recognition (ASR) degrades severely in noisy environments. Although speech enhancement (SE) front-ends effectively suppress background noise, they often introduce artifacts that harm recognition. Observation addition (OA) addressed this issue by fusing noisy and SE enhanced speech, improving recognition without modifying the parameters of the SE or ASR models. This paper proposes an intelligibility-guided OA method, where fusion weights are derived from intelligibility estimates obtained directly from the backend ASR. Unlike prior OA methods based on trained neural predictors, the proposed method is training-free, reducing complexity and enhances generalization. Extensive experiments across diverse SE-ASR combinations and datasets demonstrate strong robustness and improvements over existing OA baselines. Additional analyses of intelligibility-guided switching-based alternatives and frame versus utterance-level OA further validate the proposed design.",
      "authors": [
        "Haoyang Li",
        "Changsong Liu",
        "Wei Rao",
        "Hao Shi",
        "Sakriani Sakti",
        "Eng Siong Chng"
      ],
      "url": "https://arxiv.org/abs/2602.20967",
      "published": "2026-02-24T14:46:54+00:00",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ]
    },
    {
      "id": "2602.20966",
      "title": "Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models",
      "abstract": "This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact?   We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences.   The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.",
      "authors": [
        "Paola Merlo",
        "Chunyang Jiang",
        "Giuseppe Samo",
        "Vivi Nastase"
      ],
      "url": "https://arxiv.org/abs/2602.20966",
      "published": "2026-02-24T14:45:08+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20958",
      "title": "EKF-Based Depth Camera and Deep Learning Fusion for UAV-Person Distance Estimation and Following in SAR Operations",
      "abstract": "Search and rescue (SAR) operations require rapid responses to save lives or property. Unmanned Aerial Vehicles (UAVs) equipped with vision-based systems support these missions through prior terrain investigation or real-time assistance during the mission itself. Vision-based UAV frameworks aid human search tasks by detecting and recognizing specific individuals, then tracking and following them while maintaining a safe distance. A key safety requirement for UAV following is the accurate estimation of the distance between camera and target object under real-world conditions, achieved by fusing multiple image modalities. UAVs with deep learning-based vision systems offer a new approach to the planning and execution of SAR operations. As part of the system for automatic people detection and face recognition using deep learning, in this paper we present the fusion of depth camera measurements and monocular camera-to-body distance estimation for robust tracking and following. Deep learning-based filtering of depth camera data and estimation of camera-to-body distance from a monocular camera are achieved with YOLO-pose, enabling real-time fusion of depth information using the Extended Kalman Filter (EKF) algorithm. The proposed subsystem, designed for use in drones, estimates and measures the distance between the depth camera and the human body keypoints, to maintain the safe distance between the drone and the human target. Our system provides an accurate estimated distance, which has been validated against motion capture ground truth data. The system has been tested in real time indoors, where it reduces the average errors, root mean square error (RMSE) and standard deviations of distance estimation up to 15,3\\% in three tested scenarios.",
      "authors": [
        "Luka Šiktar",
        "Branimir Ćaran",
        "Bojan Šekoranja",
        "Marko Švaco"
      ],
      "url": "https://arxiv.org/abs/2602.20958",
      "published": "2026-02-24T14:37:36+00:00",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20951",
      "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis",
      "abstract": "Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.",
      "authors": [
        "Jaehyun Park",
        "Minyoung Ahn",
        "Minkyu Kim",
        "Jonghyun Lee",
        "Jae-Gil Lee",
        "Dongmin Park"
      ],
      "url": "https://arxiv.org/abs/2602.20951",
      "published": "2026-02-24T14:34:13+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20947",
      "title": "Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation",
      "abstract": "The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.",
      "authors": [
        "Thorbjørn Mosekjær Iversen",
        "Zebin Duan",
        "Frederik Hagelskjær"
      ],
      "url": "https://arxiv.org/abs/2602.20947",
      "published": "2026-02-24T14:31:28+00:00",
      "categories": [
        "cs.LG",
        "cs.CV"
      ]
    },
    {
      "id": "2602.20946",
      "title": "Some Simple Economics of AGI",
      "abstract": "For millennia, human cognition was the primary engine of progress on Earth. As AI decouples cognition from biology, the marginal cost of measurable execution falls to zero, absorbing any labor capturable by metrics--including creative, analytical, and innovative work. The binding constraint on growth is no longer intelligence but human verification bandwidth: the capacity to validate, audit, and underwrite responsibility when execution is abundant. We model the AGI transition as the collision of two racing cost curves: an exponentially decaying Cost to Automate and a biologically bottlenecked Cost to Verify. This structural asymmetry widens a Measurability Gap between what agents can execute and what humans can afford to verify. It also drives a shift from skill-biased to measurability-biased technical change. Rents migrate to verification-grade ground truth, cryptographic provenance, and liability underwriting--the ability to insure outcomes rather than merely generate them. The current human-in-the-loop equilibrium is unstable: eroded from below as apprenticeship collapses (Missing Junior Loop) and from within as experts codify their obsolescence (Codifier's Curse). Unverified deployment becomes privately rational--a Trojan Horse externality. Unmanaged, these forces pull toward a Hollow Economy. Yet by scaling verification alongside agentic capabilities, the forces that threaten collapse become the catalyst for unbounded discovery and experimentation--an Augmented Economy. We derive a practical playbook for individuals, companies, investors, and policymakers. Today's defining challenge is not the race to deploy the most autonomous systems; it is the race to secure the foundations of their oversight. Only by scaling our bandwidth for verification alongside our capacity for execution can we ensure that the intelligence we have summoned preserves the humanity that initiated it.",
      "authors": [
        "Christian Catalini",
        "Xiang Hui",
        "Jane Wu"
      ],
      "url": "https://arxiv.org/abs/2602.20946",
      "published": "2026-02-24T14:29:45+00:00",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "cs.SI"
      ]
    },
    {
      "id": "2602.20945",
      "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
      "abstract": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.",
      "authors": [
        "Taiqiang Wu",
        "Zenan Xu",
        "Bo Zhou",
        "Ngai Wong"
      ],
      "url": "https://arxiv.org/abs/2602.20945",
      "published": "2026-02-24T14:28:16+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20937",
      "title": "Extending $μ$P: Spectral Conditions for Feature Learning Across Optimizers",
      "abstract": "Several variations of adaptive first-order and second-order optimization methods have been proposed to accelerate and scale the training of large language models. The performance of these optimization routines is highly sensitive to the choice of hyperparameters (HPs), which are computationally expensive to tune for large-scale models. Maximal update parameterization $(μ$P$)$ is a set of scaling rules which aims to make the optimal HPs independent of the model size, thereby allowing the HPs tuned on a smaller (computationally cheaper) model to be transferred to train a larger, target model. Despite promising results for SGD and Adam, deriving $μ$P for other optimizers is challenging because the underlying tensor programming approach is difficult to grasp. Building on recent work that introduced spectral conditions as an alternative to tensor programs, we propose a novel framework to derive $μ$P for a broader class of optimizers, including AdamW, ADOPT, LAMB, Sophia, Shampoo and Muon. We implement our $μ$P derivations on multiple benchmark models and demonstrate zero-shot learning rate transfer across increasing model width for the above optimizers. Further, we provide empirical insights into depth-scaling parameterization for these optimizers.",
      "authors": [
        "Akshita Gupta",
        "Marieme Ngom",
        "Sam Foreman",
        "Venkatram Vishwanath"
      ],
      "url": "https://arxiv.org/abs/2602.20937",
      "published": "2026-02-24T14:17:51+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20934",
      "title": "Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence",
      "abstract": "The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a \"Reasoning Kernel\" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.",
      "authors": [
        "ChengYou Li",
        "XiaoDong Liu",
        "XiangBao Meng",
        "XinYu Zhao"
      ],
      "url": "https://arxiv.org/abs/2602.20934",
      "published": "2026-02-24T14:12:21+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20932",
      "title": "Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels",
      "abstract": "An electroencephalogram (EEG) records the spatially averaged electrical activity of neurons in the brain, measured from the human scalp. Prior studies have explored EEG-based classification of objects or concepts, often for passive viewing of briefly presented image or video stimuli, with limited classes. Because EEG exhibits a low signal-to-noise ratio, recognizing fine-grained representations across a large number of classes remains challenging; however, abstract-level object representations may exist. In this work, we investigate whether EEG captures object representations across multiple hierarchical levels, and propose episodic analysis, in which a Machine Learning (ML) model is evaluated across various, yet related, classification tasks (episodes). Unlike prior episodic EEG studies that rely on fixed or randomly sampled classes of equal cardinality, we adopt hierarchy-aware episode sampling using WordNet to generate episodes with variable classes of diverse hierarchy. We also present the largest episodic framework in the EEG domain for detecting observed text from EEG signals in the PEERS dataset, comprising $931538$ EEG samples under $1610$ object labels, acquired from $264$ human participants (subjects) performing controlled cognitive tasks, enabling the study of neural dynamics underlying perception, decision-making, and performance monitoring.   We examine how the semantic abstraction level affects classification performance across multiple learning techniques and architectures, providing a comprehensive analysis. The models tend to improve performance when the classification categories are drawn from higher levels of the hierarchy, suggesting sensitivity to abstraction. Our work highlights abstraction depth as an underexplored dimension of EEG decoding and motivates future research in this direction.",
      "authors": [
        "Anupam Sharma",
        "Harish Katti",
        "Prajwal Singh",
        "Shanmuganathan Raman",
        "Krishna Miyapuram"
      ],
      "url": "https://arxiv.org/abs/2602.20932",
      "published": "2026-02-24T14:10:41+00:00",
      "categories": [
        "cs.LG",
        "cs.HC",
        "eess.SP"
      ]
    },
    {
      "id": "2602.20926",
      "title": "HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG",
      "abstract": "Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\\times$ speedup over leading Graph-based RAG baselines.",
      "authors": [
        "Yuqi Huang",
        "Ning Liao",
        "Kai Yang",
        "Anning Hu",
        "Shengchao Hu",
        "Xiaoxing Wang",
        "Junchi Yan"
      ],
      "url": "https://arxiv.org/abs/2602.20926",
      "published": "2026-02-24T14:05:29+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20924",
      "title": "Airavat: An Agentic Framework for Internet Measurement",
      "abstract": "Internet measurement faces twin challenges: complex analyses require expert-level orchestration of tools, yet even syntactically correct implementations can have methodological flaws and can be difficult to verify. Democratizing measurement capabilities thus demands automating both workflow generation and verification against methodological standards established through decades of research.   We present Airavat, the first agentic framework for Internet measurement workflow generation with systematic verification and validation. Airavat coordinates a set of agents mirroring expert reasoning: three agents handle problem decomposition, solution design, and code implementation, with assistance from a registry of existing tools. Two specialized engines ensure methodological correctness: a Verification Engine evaluates workflows against a knowledge graph encoding five decades of measurement research, while a Validation Engine identifies appropriate validation techniques grounded in established methodologies. Through four Internet measurement case studies, we demonstrate that Airavat (i) generates workflows matching expert-level solutions, (ii) makes sound architectural decisions, (iii) addresses novel problems without ground truth, and (iv) identifies methodological flaws missed by standard execution-based testing.",
      "authors": [
        "Alagappan Ramanathan",
        "Eunju Kang",
        "Dongsu Han",
        "Sangeetha Abdu Jyothi"
      ],
      "url": "https://arxiv.org/abs/2602.20924",
      "published": "2026-02-24T14:04:18+00:00",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "id": "2602.20921",
      "title": "On the Generalization Behavior of Deep Residual Networks From a Dynamical System Perspective",
      "abstract": "Deep neural networks (DNNs) have significantly advanced machine learning, with model depth playing a central role in their successes. The dynamical system modeling approach has recently emerged as a powerful framework, offering new mathematical insights into the structure and learning behavior of DNNs. In this work, we establish generalization error bounds for both discrete- and continuous-time residual networks (ResNets) by combining Rademacher complexity, flow maps of dynamical systems, and the convergence behavior of ResNets in the deep-layer limit. The resulting bounds are of order $O(1/\\sqrt{S})$ with respect to the number of training samples $S$, and include a structure-dependent negative term, yielding depth-uniform and asymptotic generalization bounds under milder assumptions. These findings provide a unified understanding of generalization across both discrete- and continuous-time ResNets, helping to close the gap in both the order of sample complexity and assumptions between the discrete- and continuous-time settings.",
      "authors": [
        "Jinshu Huang",
        "Mingfei Sun",
        "Chunlin Wu"
      ],
      "url": "https://arxiv.org/abs/2602.20921",
      "published": "2026-02-24T13:59:06+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20918",
      "title": "Predicting Sentence Acceptability Judgments in Multimodal Contexts",
      "abstract": "Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.",
      "authors": [
        "Hyewon Jang",
        "Nikolai Ilinykh",
        "Sharid Loáiciga",
        "Jey Han Lau",
        "Shalom Lappin"
      ],
      "url": "https://arxiv.org/abs/2602.20918",
      "published": "2026-02-24T13:54:38+00:00",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.20911",
      "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning",
      "abstract": "Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for better knowledge sharing. SAEF first groups tasks into conceptual clusters based on their semantic relationships. Then, within each cluster, it builds a balanced expert tree by creating new adapters from merging the adapters of similar tasks. At inference time, SAEF finds and activates a set of relevant experts from the forest for any given input. The final prediction is made by combining the outputs of these activated experts, weighted by how confident each expert is. Experiments on several benchmark datasets show that SAEF achieves SOTA performance.",
      "authors": [
        "Ruiqi Liu",
        "Boyu Diao",
        "Hangda Liu",
        "Zhulin An",
        "Fei Wang",
        "Yongjun Xu"
      ],
      "url": "https://arxiv.org/abs/2602.20911",
      "published": "2026-02-24T13:48:13+00:00",
      "categories": [
        "cs.LG",
        "cs.CV"
      ]
    },
    {
      "id": "2602.20904",
      "title": "Transcoder Adapters for Reasoning-Model Diffing",
      "abstract": "While reasoning models are increasingly ubiquitous, the effects of reasoning training on a model's internal mechanisms remain poorly understood. In this work, we introduce transcoder adapters, a technique for learning an interpretable approximation of the difference in MLP computation before and after fine-tuning. We apply transcoder adapters to characterize the differences between Qwen2.5-Math-7B and its reasoning-distilled variant, DeepSeek-R1-Distill-Qwen-7B. Learned adapters are faithful to the target model's internal computation and next-token predictions. When evaluated on reasoning benchmarks, adapters match the reasoning model's response lengths and typically recover 50-90% of the accuracy gains from reasoning fine-tuning. Adapter features are sparsely activating and interpretable. When examining adapter features, we find that only ~8% have activating examples directly related to reasoning behaviors. We deeply study one such behavior -- the production of hesitation tokens (e.g., \"wait\"). Using attribution graphs, we trace hesitation to only ~2.4% of adapter features (5.6k total) performing one of two functions. These features are necessary and sufficient for producing hesitation tokens; removing them reduces response length, often without affecting accuracy. Overall, our results provide insight into reasoning training and suggest transcoder adapters may be useful for studying fine-tuning more broadly.",
      "authors": [
        "Nathan Hu",
        "Jake Ward",
        "Thomas Icard",
        "Christopher Potts"
      ],
      "url": "https://arxiv.org/abs/2602.20904",
      "published": "2026-02-24T13:40:28+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20901",
      "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models",
      "abstract": "Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.",
      "authors": [
        "Yuechen Xie",
        "Xiaoyan Zhang",
        "Yicheng Shan",
        "Hao Zhu",
        "Rui Tang",
        "Rong Wei",
        "Mingli Song",
        "Yuanyu Wan",
        "Jie Song"
      ],
      "url": "https://arxiv.org/abs/2602.20901",
      "published": "2026-02-24T13:38:37+00:00",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20892",
      "title": "Exa-PSD: a new Persian sentiment analysis dataset on Twitter",
      "abstract": "Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.",
      "authors": [
        "Seyed Himan Ghaderi",
        "Saeed Sarbazi Azad",
        "Mohammad Mehdi Jaziriyan",
        "Ahmad Akbari"
      ],
      "url": "https://arxiv.org/abs/2602.20892",
      "published": "2026-02-24T13:28:23+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20878",
      "title": "Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs",
      "abstract": "Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.",
      "authors": [
        "Dhita Putri Pratama",
        "Soyeon Caren Han",
        "Yihao Ding"
      ],
      "url": "https://arxiv.org/abs/2602.20878",
      "published": "2026-02-24T13:20:07+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20877",
      "title": "E-MMKGR: A Unified Multimodal Knowledge Graph Framework for E-commerce Applications",
      "abstract": "Multimodal recommender systems (MMRSs) enhance collaborative filtering by leveraging item-side modalities, but their reliance on a fixed set of modalities and task-specific objectives limits both modality extensibility and task generalization. We propose E-MMKGR, a framework that constructs an e-commerce-specific Multimodal Knowledge Graph E-MMKG and learns unified item representations through GNN-based propagation and KG-oriented optimization. These representations provide a shared semantic foundation applicable to diverse tasks. Experiments on real-world Amazon datasets show improvements of up to 10.18% in Recall@10 for recommendation and up to 21.72% over vector-based retrieval for product search, demonstrating the effectiveness and extensibility of our approach.",
      "authors": [
        "Jiwoo Kang",
        "Yeon-Chang Lee"
      ],
      "url": "https://arxiv.org/abs/2602.20877",
      "published": "2026-02-24T13:19:42+00:00",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20867",
      "title": "SoK: Agentic Skills -- Beyond Tool Use in LLM Agents",
      "abstract": "Agentic systems increasingly rely on reusable procedural capabilities, \\textit{a.k.a., agentic skills}, to execute long-horizon workflows reliably. These capabilities are callable modules that package procedural knowledge with explicit applicability conditions, execution policies, termination criteria, and reusable interfaces. Unlike one-off plans or atomic tool calls, skills operate (and often do well) across tasks.   This paper maps the skill layer across the full lifecycle (discovery, practice, distillation, storage, composition, evaluation, and update) and introduces two complementary taxonomies. The first is a system-level set of \\textbf{seven design patterns} capturing how skills are packaged and executed in practice, from metadata-driven progressive disclosure and executable code skills to self-evolving libraries and marketplace distribution. The second is an orthogonal \\textbf{representation $\\times$ scope} taxonomy describing what skills \\emph{are} (natural language, code, policy, hybrid) and what environments they operate over (web, OS, software engineering, robotics).   We analyze the security and governance implications of skill-based agents, covering supply-chain risks, prompt injection via skill payloads, and trust-tiered execution, grounded by a case study of the ClawHavoc campaign in which nearly 1{,}200 malicious skills infiltrated a major agent marketplace, exfiltrating API keys, cryptocurrency wallets, and browser credentials at scale. We further survey deterministic evaluation approaches, anchored by recent benchmark evidence that curated skills can substantially improve agent success rates while self-generated skills may degrade them. We conclude with open challenges toward robust, verifiable, and certifiable skills for real-world autonomous agents.",
      "authors": [
        "Yanna Jiang",
        "Delong Li",
        "Haiyu Deng",
        "Baihe Ma",
        "Xu Wang",
        "Qin Wang",
        "Guangsheng Yu"
      ],
      "url": "https://arxiv.org/abs/2602.20867",
      "published": "2026-02-24T13:11:38+00:00",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CE",
        "cs.ET"
      ]
    },
    {
      "id": "2602.20859",
      "title": "FinAnchor: Aligned Multi-Model Representations for Financial Prediction",
      "abstract": "Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.",
      "authors": [
        "Zirui He",
        "Huopu Zhang",
        "Yanguang Liu",
        "Sirui Wu",
        "Mengnan Du"
      ],
      "url": "https://arxiv.org/abs/2602.20859",
      "published": "2026-02-24T13:02:09+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.21269",
      "title": "Group Orthogonalized Policy Optimization:Group Policy Optimization as Orthogonal Projection in Hilbert Space",
      "abstract": "We present Group Orthogonalized Policy Optimization (GOPO), a new alignment algorithm for large language models derived from the geometry of Hilbert function spaces. Instead of optimizing on the probability simplex and inheriting the exponential curvature of Kullback-Leibler divergence, GOPO lifts alignment into the Hilbert space L2(pi_k) of square-integrable functions with respect to the reference policy. Within this space, the simplex constraint reduces to a linear orthogonality condition <v, 1> = 0, defining a codimension-one subspace H0. Minimizing distance to an unconstrained target u_star yields the work-dissipation functional J(v) = <g, v> - (mu / 2) ||v||^2, whose maximizer follows directly from the Hilbert projection theorem. Enforcing the boundary v >= -1 produces a bounded Hilbert projection that induces exact sparsity, assigning zero probability to catastrophically poor actions through a closed-form threshold. To connect this functional theory with practice, GOPO projects from infinite-dimensional L2(pi_k) to a finite empirical subspace induced by group sampling. Because group-normalized advantages sum to zero, the Lagrange multiplier enforcing probability conservation vanishes exactly, reducing the constrained projection to an unconstrained empirical loss. The resulting objective has constant Hessian curvature mu I, non-saturating linear gradients, and an intrinsic dead-zone mechanism without heuristic clipping. Experiments on mathematical reasoning benchmarks show that GOPO achieves competitive generalization while maintaining stable gradient dynamics and entropy preservation in regimes where clipping-based methods plateau.",
      "authors": [
        "Wang Zixian"
      ],
      "url": "https://arxiv.org/abs/2602.21269",
      "published": "2026-02-24T12:59:32+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20857",
      "title": "Functional Continuous Decomposition",
      "abstract": "The analysis of non-stationary time-series data requires insight into its local and global patterns with physical interpretability. However, traditional smoothing algorithms, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), lack the ability to perform parametric optimization with guaranteed continuity. In this paper, we propose Functional Continuous Decomposition (FCD), a JAX-accelerated framework that performs parametric, continuous optimization on a wide range of mathematical functions. By using Levenberg-Marquardt optimization to achieve up to $C^1$ continuous fitting, FCD transforms raw time-series data into $M$ modes that capture different temporal patterns from short-term to long-term trends. Applications of FCD include physics, medicine, financial analysis, and machine learning, where it is commonly used for the analysis of signal temporal patterns, optimized parameters, derivatives, and integrals of decomposition. Furthermore, FCD can be applied for physical analysis and feature extraction with an average SRMSE of 0.735 per segment and a speed of 0.47s on full decomposition of 1,000 points. Finally, we demonstrate that a Convolutional Neural Network (CNN) enhanced with FCD features, such as optimized function values, parameters, and derivatives, achieved 16.8% faster convergence and 2.5% higher accuracy over a standard CNN.",
      "authors": [
        "Teymur Aghayev"
      ],
      "url": "https://arxiv.org/abs/2602.20857",
      "published": "2026-02-24T12:58:21+00:00",
      "categories": [
        "eess.SP",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20833",
      "title": "DRESS: A Continuous Framework for Structural Graph Refinement",
      "abstract": "The Weisfeiler-Lehman (WL) hierarchy is a cornerstone framework for graph isomorphism testing and structural analysis. However, scaling beyond 1-WL to 3-WL and higher requires tensor-based operations that scale as O(n^3) or O(n^4), making them computationally prohibitive for large graphs. In this paper, we start from the Original-DRESS equation (Castrillo, Leon, and Gomez, 2018)--a parameter-free, continuous dynamical system on edges--and show that it distinguishes the prism graph from K_{3,3}, a pair that 1-WL provably cannot separate. We then generalize it to Motif-DRESS, which replaces triangle neighborhoods with arbitrary structural motifs and converges to a unique fixed point under three sufficient conditions, and further to Generalized-DRESS, an abstract template parameterized by the choice of neighborhood operator, aggregation function and norm. Finally, we introduce Delta-DRESS, which runs DRESS on each node-deleted subgraph G\\{v}, connecting the framework to the Kelly-Ulam reconstruction conjecture. Both Motif-DRESS and Delta-DRESS empirically distinguish Strongly Regular Graphs (SRGs)--such as the Rook and Shrikhande graphs--that confound 3-WL. Our results establish the DRESS family as a highly scalable framework that empirically surpasses both 1-WL and 3-WL on well-known benchmark graphs, without the prohibitive O(n^4) computational cost.",
      "authors": [
        "Eduar Castrillo Velilla"
      ],
      "url": "https://arxiv.org/abs/2602.20833",
      "published": "2026-02-24T12:18:42+00:00",
      "categories": [
        "cs.DS",
        "cs.LG"
      ]
    },
    {
      "id": "2602.21268",
      "title": "A Dynamic Survey of Soft Set Theory and Its Extensions",
      "abstract": "Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.",
      "authors": [
        "Takaaki Fujita",
        "Florentin Smarandache"
      ],
      "url": "https://arxiv.org/abs/2602.21268",
      "published": "2026-02-24T11:58:45+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20816",
      "title": "Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation",
      "abstract": "The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.",
      "authors": [
        "Sayantan Dasgupta",
        "Trevor Cohn",
        "Timothy Baldwin"
      ],
      "url": "https://arxiv.org/abs/2602.20816",
      "published": "2026-02-24T11:54:06+00:00",
      "categories": [
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20813",
      "title": "Pressure Reveals Character: Behavioural Alignment Evaluation at Depth",
      "abstract": "Evaluating alignment in language models requires testing how they behave under realistic pressure, not just what they claim they would do. While alignment failures increasingly cause real-world harm, comprehensive evaluation frameworks with realistic multi-turn scenarios remain lacking. We introduce an alignment benchmark spanning 904 scenarios across six categories -- Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming -- validated as realistic by human raters. Our scenarios place models under conflicting instructions, simulated tool access, and multi-turn escalation to reveal behavioural tendencies that single-turn evaluations miss. Evaluating 24 frontier models using LLM judges validated against human annotations, we find that even top-performing models exhibit gaps in specific categories, while the majority of models show consistent weaknesses across the board. Factor analysis reveals that alignment behaves as a unified construct (analogous to the g-factor in cognitive research) with models scoring high on one category tending to score high on others. We publicly release the benchmark and an interactive leaderboard to support ongoing evaluation, with plans to expand scenarios in areas where we observe persistent weaknesses and to add new models as they are released.",
      "authors": [
        "Nora Petrova",
        "John Burden"
      ],
      "url": "https://arxiv.org/abs/2602.20813",
      "published": "2026-02-24T11:52:17+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20812",
      "title": "Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset",
      "abstract": "As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.",
      "authors": [
        "Jia-Rui Lin",
        "Yun-Hong Cai",
        "Xiang-Rui Ni",
        "Shaojie Zhou",
        "Peng Pan"
      ],
      "url": "https://arxiv.org/abs/2602.20812",
      "published": "2026-02-24T11:51:21+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20810",
      "title": "POMDPPlanners: Open-Source Package for POMDP Planning",
      "abstract": "We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.",
      "authors": [
        "Yaacov Pariente",
        "Vadim Indelman"
      ],
      "url": "https://arxiv.org/abs/2602.20810",
      "published": "2026-02-24T11:50:04+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20809",
      "title": "Regret-Guided Search Control for Efficient Learning in AlphaZero",
      "abstract": "Reinforcement learning (RL) agents achieve remarkable performance but remain far less learning-efficient than humans. While RL agents require extensive self-play games to extract useful signals, humans often need only a few games, improving rapidly by repeatedly revisiting states where mistakes occurred. This idea, known as search control, aims to restart from valuable states rather than always from the initial state. In AlphaZero, prior work Go-Exploit applies this idea by sampling past states from self-play or search trees, but it treats all states equally, regardless of their learning potential. We propose Regret-Guided Search Control (RGSC), which extends AlphaZero with a regret network that learns to identify high-regret states, where the agent's evaluation diverges most from the actual outcome. These states are collected from both self-play trajectories and MCTS nodes, stored in a prioritized regret buffer, and reused as new starting positions. Across 9x9 Go, 10x10 Othello, and 11x11 Hex, RGSC outperforms AlphaZero and Go-Exploit by an average of 77 and 89 Elo, respectively. When training on a well-trained 9x9 Go model, RGSC further improves the win rate against KataGo from 69.3% to 78.2%, while both baselines show no improvement. These results demonstrate that RGSC provides an effective mechanism for search control, improving both efficiency and robustness of AlphaZero training. Our code is available at https://rlg.iis.sinica.edu.tw/papers/rgsc.",
      "authors": [
        "Yun-Jui Tsai",
        "Wei-Yu Chen",
        "Yan-Ru Ju",
        "Yu-Hung Chang",
        "Ti-Rong Wu"
      ],
      "url": "https://arxiv.org/abs/2602.20809",
      "published": "2026-02-24T11:49:59+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.21267",
      "title": "A Systematic Review of Algorithmic Red Teaming Methodologies for Assurance and Security of AI Applications",
      "abstract": "Cybersecurity threats are becoming increasingly sophisticated, making traditional defense mechanisms and manual red teaming approaches insufficient for modern organizations. While red teaming has long been recognized as an effective method to identify vulnerabilities by simulating real-world attacks, its manual execution is resource-intensive, time-consuming, and lacks scalability for frequent assessments. These limitations have driven the evolution toward auto-mated red teaming, which leverages artificial intelligence and automation to deliver efficient and adaptive security evaluations. This systematic review consolidates existing research on automated red teaming, examining its methodologies, tools, benefits, and limitations. The paper also highlights current trends, challenges, and research gaps, offering insights into future directions for improving automated red teaming as a critical component of proactive cybersecurity strategies. By synthesizing findings from diverse studies, this review aims to provide a comprehensive understanding of how automation enhances red teaming and strengthens organizational resilience against evolving cyber threats.",
      "authors": [
        "Shruti Srivastava",
        "Kiranmayee Janardhan",
        "Shaurya Jauhari"
      ],
      "url": "https://arxiv.org/abs/2602.21267",
      "published": "2026-02-24T11:48:07+00:00",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20805",
      "title": "Assessing the Impact of Speaker Identity in Speech Spoofing Detection",
      "abstract": "Spoofing detection systems are typically trained using diverse recordings from multiple speakers, often assuming that the resulting embeddings are independent of speaker identity. However, this assumption remains unverified. In this paper, we investigate the impact of speaker information on spoofing detection systems. We propose two approaches within our Speaker-Invariant Multi-Task framework, one that models speaker identity within the embeddings and another that removes it. SInMT integrates multi-task learning for joint speaker recognition and spoofing detection, incorporating a gradient reversal layer. Evaluated using four datasets, our speaker-invariant model reduces the average equal error rate by 17% compared to the baseline, with up to 48% reduction for the most challenging attacks (e.g., A11).",
      "authors": [
        "Anh-Tuan Dao",
        "Driss Matrouf",
        "Nicholas Evans"
      ],
      "url": "https://arxiv.org/abs/2602.20805",
      "published": "2026-02-24T11:45:41+00:00",
      "categories": [
        "cs.SD",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20804",
      "title": "Probing Dec-POMDP Reasoning in Cooperative MARL",
      "abstract": "Cooperative multi-agent reinforcement learning (MARL) is typically framed as a decentralised partially observable Markov decision process (Dec-POMDP), a setting whose hardness stems from two key challenges: partial observability and decentralised coordination. Genuinely solving such tasks requires Dec-POMDP reasoning, where agents use history to infer hidden states and coordinate based on local information. Yet it remains unclear whether popular benchmarks actually demand this reasoning or permit success via simpler strategies. We introduce a diagnostic suite combining statistically grounded performance comparisons and information-theoretic probes to audit the behavioural complexity of baseline policies (IPPO and MAPPO) across 37 scenarios spanning MPE, SMAX, Overcooked, Hanabi, and MaBrax. Our diagnostics reveal that success on these benchmarks rarely requires genuine Dec-POMDP reasoning. Reactive policies match the performance of memory-based agents in over half the scenarios, and emergent coordination frequently relies on brittle, synchronous action coupling rather than robust temporal influence. These findings suggest that some widely used benchmarks may not adequately test core Dec-POMDP assumptions under current training paradigms, potentially leading to over-optimistic assessments of progress. We release our diagnostic tooling to support more rigorous environment design and evaluation in cooperative MARL.",
      "authors": [
        "Kale-ab Tessera",
        "Leonard Hinckeldey",
        "Riccardo Zamboni",
        "David Abel",
        "Amos Storkey"
      ],
      "url": "https://arxiv.org/abs/2602.20804",
      "published": "2026-02-24T11:44:46+00:00",
      "categories": [
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "id": "2602.20796",
      "title": "Exploring the Impact of Parameter Update Magnitude on Forgetting and Generalization of Continual Learning",
      "abstract": "The magnitude of parameter updates are considered a key factor in continual learning. However, most existing studies focus on designing diverse update strategies, while a theoretical understanding of the underlying mechanisms remains limited. Therefore, we characterize model's forgetting from the perspective of parameter update magnitude and formalize it as knowledge degradation induced by task-specific drift in the parameter space, which has not been fully captured in previous studies due to their assumption of a unified parameter space. By deriving the optimal parameter update magnitude that minimizes forgetting, we unify two representative update paradigms, frozen training and initialized training, within an optimization framework for constrained parameter updates. Our theoretical results further reveals that sequence tasks with small parameter distances exhibit better generalization and less forgetting under frozen training rather than initialized training. These theoretical insights inspire a novel hybrid parameter update strategy that adaptively adjusts update magnitude based on gradient directions. Experiments on deep neural networks demonstrate that this hybrid approach outperforms standard training strategies, providing new theoretical perspectives and practical inspiration for designing efficient and scalable continual learning algorithms.",
      "authors": [
        "JinLi He",
        "Liang Bai",
        "Xian Yang"
      ],
      "url": "https://arxiv.org/abs/2602.20796",
      "published": "2026-02-24T11:35:15+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20791",
      "title": "Understanding the Role of Rehearsal Scale in Continual Learning under Varying Model Capacities",
      "abstract": "Rehearsal is one of the key techniques for mitigating catastrophic forgetting and has been widely adopted in continual learning algorithms due to its simplicity and practicality. However, the theoretical understanding of how rehearsal scale influences learning dynamics remains limited. To address this gap, we formulate rehearsal-based continual learning as a multidimensional effectiveness-driven iterative optimization problem, providing a unified characterization across diverse performance metrics. Within this framework, we derive a closed-form analysis of adaptability, memorability, and generalization from the perspective of rehearsal scale. Our results uncover several intriguing and counterintuitive findings. First, rehearsal can impair model's adaptability, in sharp contrast to its traditionally recognized benefits. Second, increasing the rehearsal scale does not necessarily improve memory retention. When tasks are similar and noise levels are low, the memory error exhibits a diminishing lower bound. Finally, we validate these insights through numerical simulations and extended analyses on deep neural networks across multiple real-world datasets, revealing statistical patterns of rehearsal mechanisms in continual learning.",
      "authors": [
        "JinLi He",
        "Liang Bai",
        "Xian Yang"
      ],
      "url": "https://arxiv.org/abs/2602.20791",
      "published": "2026-02-24T11:29:12+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20782",
      "title": "On Electric Vehicle Energy Demand Forecasting and the Effect of Federated Learning",
      "abstract": "The wide spread of new energy resources, smart devices, and demand side management strategies has motivated several analytics operations, from infrastructure load modeling to user behavior profiling. Energy Demand Forecasting (EDF) of Electric Vehicle Supply Equipments (EVSEs) is one of the most critical operations for ensuring efficient energy management and sustainability, since it enables utility providers to anticipate energy/power demand, optimize resource allocation, and implement proactive measures to improve grid reliability. However, accurate EDF is a challenging problem due to external factors, such as the varying user routines, weather conditions, driving behaviors, unknown state of charge, etc. Furthermore, as concerns and restrictions about privacy and sustainability have grown, training data has become increasingly fragmented, resulting in distributed datasets scattered across different data silos and/or edge devices, calling for federated learning solutions. In this paper, we investigate different well-established time series forecasting methodologies to address the EDF problem, from statistical methods (the ARIMA family) to traditional machine learning models (such as XGBoost) and deep neural networks (GRU and LSTM). We provide an overview of these methods through a performance comparison over four real-world EVSE datasets, evaluated under both centralized and federated learning paradigms, focusing on the trade-offs between forecasting fidelity, privacy preservation, and energy overheads. Our experimental results demonstrate, on the one hand, the superiority of gradient boosted trees (XGBoost) over statistical and NN-based models in both prediction accuracy and energy efficiency and, on the other hand, an insight that Federated Learning-enabled models balance these factors, offering a promising direction for decentralized energy demand forecasting.",
      "authors": [
        "Andreas Tritsarolis",
        "Gil Sampaio",
        "Nikos Pelekis",
        "Yannis Theodoridis"
      ],
      "url": "https://arxiv.org/abs/2602.20782",
      "published": "2026-02-24T11:21:45+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20770",
      "title": "Pipeline for Verifying LLM-Generated Mathematical Solutions",
      "abstract": "With the growing popularity of Large Reasoning Models and their results in solving mathematical problems, it becomes crucial to measure their capabilities. We introduce a pipeline for both automatic and interactive verification as a more accurate alternative to only checking the answer which is currently the most popular approach for benchmarks. The pipeline can also be used as a generator of correct solutions both in formal and informal languages. 3 AI agents, which can be chosen for the benchmark accordingly, are included in the structure. The key idea is the use of prompts to obtain the solution in the specific form which allows for easier verification using proof assistants and possible use of small models ($\\le 8B$). Experiments on several datasets suggest low probability of False Positives. The open-source implementation with instructions on setting up a server is available at https://github.com/LogicEnj/lean4_verification_pipeline.",
      "authors": [
        "Varvara Sazonova",
        "Dmitri Shmelkin",
        "Stanislav Kikot",
        "Vasily Motolygin"
      ],
      "url": "https://arxiv.org/abs/2602.20770",
      "published": "2026-02-24T11:01:25+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20759",
      "title": "Overton Pluralistic Reinforcement Learning for Large Language Models",
      "abstract": "Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a \"small models, big perspective coverage\" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.",
      "authors": [
        "Yu Fu",
        "Seongho Son",
        "Ilija Bogunovic"
      ],
      "url": "https://arxiv.org/abs/2602.20759",
      "published": "2026-02-24T10:39:27+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20758",
      "title": "Deep unfolding of MCMC kernels: scalable, modular & explainable GANs for high-dimensional posterior sampling",
      "abstract": "Markov chain Monte Carlo (MCMC) methods are fundamental to Bayesian computation, but can be computationally intensive, especially in high-dimensional settings. Push-forward generative models, such as generative adversarial networks (GANs), variational auto-encoders and normalising flows offer a computationally efficient alternative for posterior sampling. However, push-forward models are opaque as they lack the modularity of Bayes Theorem, leading to poor generalisation with respect to changes in the likelihood function. In this work, we introduce a novel approach to GAN architecture design by applying deep unfolding to Langevin MCMC algorithms. This paradigm maps fixed-step iterative algorithms onto modular neural networks, yielding architectures that are both flexible and amenable to interpretation. Crucially, our design allows key model parameters to be specified at inference time, offering robustness to changes in the likelihood parameters. We train these unfolded samplers end-to-end using a supervised regularized Wasserstein GAN framework for posterior sampling. Through extensive Bayesian imaging experiments, we demonstrate that our proposed approach achieves high sampling accuracy and excellent computational efficiency, while retaining the physics consistency, adaptability and interpretability of classical MCMC strategies.",
      "authors": [
        "Jonathan Spence",
        "Tobías I. Liaudat",
        "Konstantinos Zygalakis",
        "Marcelo Pereyra"
      ],
      "url": "https://arxiv.org/abs/2602.20758",
      "published": "2026-02-24T10:37:10+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20752",
      "title": "OrthoDiffusion: A Generalizable Multi-Task Diffusion Foundation Model for Musculoskeletal MRI Interpretation",
      "abstract": "Musculoskeletal disorders represent a significant global health burden and are a leading cause of disability worldwide. While MRI is essential for accurate diagnosis, its interpretation remains exceptionally challenging. Radiologists must identify multiple potential abnormalities within complex anatomical structures across different imaging planes, a process that requires significant expertise and is prone to variability. We developed OrthoDiffusion, a unified diffusion-based foundation model designed for multi-task musculoskeletal MRI interpretation. The framework utilizes three orientation-specific 3D diffusion models, pre-trained in a self-supervised manner on 15,948 unlabeled knee MRI scans, to learn robust anatomical features from sagittal, coronal, and axial views. These view-specific representations are integrated to support diverse clinical tasks, including anatomical segmentation and multi-label diagnosis. Our evaluation demonstrates that OrthoDiffusion achieves excellent performance in the segmentation of 11 knee structures and the detection of 8 knee abnormalities. The model exhibited remarkable robustness across different clinical centers and MRI field strengths, consistently outperforming traditional supervised models. Notably, in settings where labeled data was scarce, OrthoDiffusion maintained high diagnostic precision using only 10\\% of training labels. Furthermore, the anatomical representations learned from knee imaging proved highly transferable to other joints, achieving strong diagnostic performance across 11 diseases of the ankle and shoulder. These findings suggest that diffusion-based foundation models can serve as a unified platform for multi-disease diagnosis and anatomical segmentation, potentially improving the efficiency and accuracy of musculoskeletal MRI interpretation in real-world clinical workflows.",
      "authors": [
        "Tian Lan",
        "Lei Xu",
        "Zimu Yuan",
        "Shanggui Liu",
        "Jiajun Liu",
        "Jiaxin Liu",
        "Weilai Xiang",
        "Hongyu Yang",
        "Dong Jiang",
        "Jianxin Yin",
        "Dingyu Wang"
      ],
      "url": "https://arxiv.org/abs/2602.20752",
      "published": "2026-02-24T10:29:10+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20751",
      "title": "SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing",
      "abstract": "Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.",
      "authors": [
        "Yifei Xu",
        "Guilherme Potje",
        "Shivam Shandilya",
        "Tiancheng Yuan",
        "Leonardo de Oliveira Nunes",
        "Rakshanda Agarwal",
        "Saeid Asgari",
        "Adam Atkinson",
        "Emre Kıcıman",
        "Songwu Lu",
        "Ranveer Chandra",
        "Tusher Chakraborty"
      ],
      "url": "https://arxiv.org/abs/2602.20751",
      "published": "2026-02-24T10:28:44+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20749",
      "title": "Explicit Grammar Semantic Feature Fusion for Robust Text Classification",
      "abstract": "Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classification model without resorting to full parameterised transformer models or heavy deep learning architectures. The novelty of our approach lies in its explicit encoding of sentence-level grammatical structure, including syntactic composition, phrase patterns, and complexity indicators, into a compact grammar vector, which is then fused with frozen contextual embeddings. These heterogeneous elements unified a single representation that captures both the structural and semantic characteristics of the text. Deep learning models such as Deep Belief Networks (DBNs), Long Short-Term Memory (LSTMs), BiLSTMs, and transformer-based BERT and XLNET were used to train and evaluate the model, with the number of epochs varied. Based on experimental results, the unified feature representation model captures both the semantic and structural properties of text, outperforming baseline models by 2%-15%, enabling more effective learning across heterogeneous domains. Unlike prior syntax-aware transformer models that inject grammatical structure through additional attention layers, tree encoders, or full fine-tuning, the proposed framework treats grammar as an explicit inductive bias rather than a learnable module, resulting in a very lightweight model that delivers better performance on edge devices",
      "authors": [
        "Azrin Sultana",
        "Firoz Ahmed"
      ],
      "url": "https://arxiv.org/abs/2602.20749",
      "published": "2026-02-24T10:25:29+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20744",
      "title": "Voices of the Mountains: Deep Learning-Based Vocal Error Detection System for Kurdish Maqams",
      "abstract": "Maqam, a singing type, is a significant component of Kurdish music. A maqam singer receives training in a traditional face-to-face or through self-training. Automatic Singing Assessment (ASA) uses machine learning (ML) to provide the accuracy of singing styles and can help learners to improve their performance through error detection. Currently, the available ASA tools follow Western music rules. The musical composition requires all notes to stay within their expected pitch range from start to finish. The system fails to detect micro-intervals and pitch bends, so it identifies Kurdish maqam singing as incorrect even though the singer performs according to traditional rules. Kurdish maqam requires recognizing performance errors within microtonal spaces, which is beyond Western equal temperament. This research is the first attempt to address the mentioned gap. While many error types happen during singing, our focus is on pitch, rhythm, and modal stability errors in the context of Bayati-Kurd. We collected 50 songs from 13 vocalists ( 2-3 hours) and annotated 221 error spans (150 fine pitch, 46 rhythm, 25 modal drift). The data was segmented into 15,199 overlapping windows and converted to log-mel spectrograms. We developed a two-headed CNN-BiLSTM with attention mode to decide whether a window contains an error and to classify it based on the chosen errors. Trained for 20 epochs with early stopping at epoch 10, the model reached a validation macro-F1 of 0.468. On the full 50-song evaluation at a 0.750 threshold, recall was 39.4% and precision 25.8% . Within detected windows, type macro-F1 was 0.387, with F1 of 0.492 (fine pitch), 0.536 (rhythm), and 0.133 (modal drift); modal drift recall was 8.0%. The better performance on common error types shows that the method works, while the poor modal-drift recall shows that more data and balancing are needed.",
      "authors": [
        "Darvan Shvan Khairaldeen",
        "Hossein Hassani"
      ],
      "url": "https://arxiv.org/abs/2602.20744",
      "published": "2026-02-24T10:17:16+00:00",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20743",
      "title": "Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization",
      "abstract": "Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.",
      "authors": [
        "Gabriel Loiseau",
        "Damien Sileo",
        "Damien Riquet",
        "Maxime Meyer",
        "Marc Tommasi"
      ],
      "url": "https://arxiv.org/abs/2602.20743",
      "published": "2026-02-24T10:12:40+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20739",
      "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
      "abstract": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.",
      "authors": [
        "Shitian Zhao",
        "Shaoheng Lin",
        "Ming Li",
        "Haoquan Zhang",
        "Wenshuo Peng",
        "Kaipeng Zhang",
        "Chen Wei"
      ],
      "url": "https://arxiv.org/abs/2602.20739",
      "published": "2026-02-24T10:08:33+00:00",
      "categories": [
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "id": "2602.20735",
      "title": "RMIT-ADM+S at the MMU-RAG NeurIPS 2025 Competition",
      "abstract": "This paper presents the award-winning RMIT-ADM+S system for the Text-to-Text   track of the NeurIPS~2025 MMU-RAG Competition. We introduce Routing-to-RAG   (R2RAG), a research-focused retrieval-augmented generation (RAG)   architecture composed of lightweight components that dynamically adapt the   retrieval strategy based on inferred query complexity and evidence   sufficiency. The system uses smaller LLMs, enabling operation on a single   consumer-grade GPU while supporting complex research tasks. It builds on the   G-RAG system, winner of the ACM~SIGIR~2025 LiveRAG Challenge, and extends it   with modules informed by qualitative review of outputs. R2RAG won the Best   Dynamic Evaluation award in the Open Source category, demonstrating high   effectiveness with careful design and efficient use of resources.",
      "authors": [
        "Kun Ran",
        "Marwah Alaofi",
        "Danula Hettiachchi",
        "Chenglong Ma",
        "Khoi Nguyen Dinh Anh",
        "Khoi Vo Nguyen",
        "Sachin Pathiyan Cherumanal",
        "Lida Rashidi",
        "Falk Scholer",
        "Damiano Spina",
        "Shuoqi Sun",
        "Oleg Zendel"
      ],
      "url": "https://arxiv.org/abs/2602.20735",
      "published": "2026-02-24T09:58:25+00:00",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.20732",
      "title": "CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference",
      "abstract": "Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \\textbf{CHESS}, an \\textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \\textbf{1\\%} of the KV cache, delivers low-latency stable inference with up to \\textbf{4.56$\\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \\href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.",
      "authors": [
        "Chao Fei",
        "Guozhong Li",
        "Chenxi Liu",
        "Panos Kalnis"
      ],
      "url": "https://arxiv.org/abs/2602.20732",
      "published": "2026-02-24T09:54:59+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20731",
      "title": "Communication-Inspired Tokenization for Structured Image Representations",
      "abstract": "Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.",
      "authors": [
        "Aram Davtyan",
        "Yusuf Sahin",
        "Yasaman Haghighi",
        "Sebastian Stapf",
        "Pablo Acuaviva",
        "Alexandre Alahi",
        "Paolo Favaro"
      ],
      "url": "https://arxiv.org/abs/2602.20731",
      "published": "2026-02-24T09:53:50+00:00",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20730",
      "title": "Rethink Efficiency Side of Neural Combinatorial Solver: An Offline and Self-Play Paradigm",
      "abstract": "We propose ECO, a versatile learning paradigm that enables efficient offline self-play for Neural Combinatorial Optimization (NCO). ECO addresses key limitations in the field through: 1) Paradigm Shift: Moving beyond inefficient online paradigms, we introduce a two-phase offline paradigm consisting of supervised warm-up and iterative Direct Preference Optimization (DPO); 2) Architecture Shift: We deliberately design a Mamba-based architecture to further enhance the efficiency in the offline paradigm; and 3) Progressive Bootstrapping: To stabilize training, we employ a heuristic-based bootstrapping mechanism that ensures continuous policy improvement during training. Comparison results on TSP and CVRP highlight that ECO performs competitively with up-to-date baselines, with significant advantage on the efficiency side in terms of memory utilization and training throughput. We provide further in-depth analysis on the efficiency, throughput and memory usage of ECO. Ablation studies show rationale behind our designs.",
      "authors": [
        "Zhenxing Xu",
        "Zeyuan Ma",
        "Weidong Bao",
        "Hui Yan",
        "Yan Zheng",
        "Ji Wang"
      ],
      "url": "https://arxiv.org/abs/2602.20730",
      "published": "2026-02-24T09:53:24+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20729",
      "title": "Fuz-RL: A Fuzzy-Guided Robust Framework for Safe Reinforcement Learning under Uncertainty",
      "abstract": "Safe Reinforcement Learning (RL) is crucial for achieving high performance while ensuring safety in real-world applications. However, the complex interplay of multiple uncertainty sources in real environments poses significant challenges for interpretable risk assessment and robust decision-making. To address these challenges, we propose Fuz-RL, a fuzzy measure-guided robust framework for safe RL. Specifically, our framework develops a novel fuzzy Bellman operator for estimating robust value functions using Choquet integrals. Theoretically, we prove that solving the Fuz-RL problem (in Constrained Markov Decision Process (CMDP) form) is equivalent to solving distributionally robust safe RL problems (in robust CMDP form), effectively avoiding min-max optimization. Empirical analyses on safe-control-gym and safety-gymnasium scenarios demonstrate that Fuz-RL effectively integrates with existing safe RL baselines in a model-free manner, significantly improving both safety and control performance under various types of uncertainties in observation, action, and dynamics.",
      "authors": [
        "Xu Wan",
        "Chao Yang",
        "Cheng Yang",
        "Jie Song",
        "Mingyang Sun"
      ],
      "url": "https://arxiv.org/abs/2602.20729",
      "published": "2026-02-24T09:50:17+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20728",
      "title": "Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback",
      "abstract": "Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.",
      "authors": [
        "Chenyang Zhao",
        "Vinny Cahill",
        "Ivana Dusparic"
      ],
      "url": "https://arxiv.org/abs/2602.20728",
      "published": "2026-02-24T09:47:25+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20727",
      "title": "ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition",
      "abstract": "LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.",
      "authors": [
        "Xindian Ma",
        "Rundong Kong",
        "Peng Zhang",
        "Ruoxiang Huang",
        "Yongyu Jiang"
      ],
      "url": "https://arxiv.org/abs/2602.20727",
      "published": "2026-02-24T09:45:10+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20723",
      "title": "Modality-Guided Mixture of Graph Experts with Entropy-Triggered Routing for Multimodal Recommendation",
      "abstract": "Multimodal recommendation enhances ranking by integrating user-item interactions with item content, which is particularly effective under sparse feedback and long-tail distributions. However, multimodal signals are inherently heterogeneous and can conflict in specific contexts, making effective fusion both crucial and challenging. Existing approaches often rely on shared fusion pathways, leading to entangled representations and modality imbalance. To address these issues, we propose \\textbf{MAGNET}, a \\textbf{M}odality-Guided Mixture of \\textbf{A}daptive \\textbf{G}raph Experts \\textbf{N}etwork with Progressive \\textbf{E}ntropy-\\textbf{T}riggered Routing for Multimodal Recommendation, designed to enhance controllability, stability, and interpretability in multimodal fusion. MAGNET couples interaction-conditioned expert routing with structure-aware graph augmentation, so that both \\emph{what} to fuse and \\emph{how} to fuse are explicitly controlled and interpretable. At the representation level, a dual-view graph learning module augments the interaction graph with content-induced edges, improving coverage for sparse and long-tail items while preserving collaborative structure via parallel encoding and lightweight fusion. At the fusion level, MAGNET employs structured experts with explicit modality roles -- dominant, balanced, and complementary -- enabling a more interpretable and adaptive combination of behavioral, visual, and textual cues. To further stabilize sparse routing and prevent expert collapse, we introduce a two-stage entropy-weighting mechanism that monitors routing entropy. This mechanism automatically transitions training from an early coverage-oriented regime to a later specialization-oriented regime, progressively balancing expert utilization and routing confidence. Extensive experiments on public benchmarks demonstrate consistent improvements over strong baselines.",
      "authors": [
        "Ji Dai",
        "Quan Fang",
        "Dengsheng Cai"
      ],
      "url": "https://arxiv.org/abs/2602.20723",
      "published": "2026-02-24T09:36:45+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20722",
      "title": "Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning",
      "abstract": "Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.",
      "authors": [
        "Xu Wan",
        "Yansheng Wang",
        "Wenqi Huang",
        "Mingyang Sun"
      ],
      "url": "https://arxiv.org/abs/2602.20722",
      "published": "2026-02-24T09:35:43+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20720",
      "title": "AdapTools: Adaptive Tool-based Indirect Prompt Injection Attacks on Agentic LLMs",
      "abstract": "The integration of external data services (e.g., Model Context Protocol, MCP) has made large language model-based agents increasingly powerful for complex task execution. However, this advancement introduces critical security vulnerabilities, particularly indirect prompt injection (IPI) attacks. Existing attack methods are limited by their reliance on static patterns and evaluation on simple language models, failing to address the fast-evolving nature of modern AI agents. We introduce AdapTools, a novel adaptive IPI attack framework that selects stealthier attack tools and generates adaptive attack prompts to create a rigorous security evaluation environment. Our approach comprises two key components: (1) Adaptive Attack Strategy Construction, which develops transferable adversarial strategies for prompt optimization, and (2) Attack Enhancement, which identifies stealthy tools capable of circumventing task-relevance defenses. Comprehensive experimental evaluation shows that AdapTools achieves a 2.13 times improvement in attack success rate while degrading system utility by a factor of 1.78. Notably, the framework maintains its effectiveness even against state-of-the-art defense mechanisms. Our method advances the understanding of IPI attacks and provides a useful reference for future research.",
      "authors": [
        "Che Wang",
        "Jiaming Zhang",
        "Ziqi Zhang",
        "Zijie Wang",
        "Yinghui Wang",
        "Jianbo Gao",
        "Tao Wei",
        "Zhong Chen",
        "Wei Yang Bryan Lim"
      ],
      "url": "https://arxiv.org/abs/2602.20720",
      "published": "2026-02-24T09:32:19+00:00",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "id": "2602.21265",
      "title": "ToolMATH: A Math Tool Benchmark for Realistic Long-Horizon Multi-Tool Reasoning",
      "abstract": "We introduce \\ToolMATH, a math-grounded benchmark that evaluates tool-augmented language models in realistic multi-tool environments where the output depends on calling schema-specified tools and sustaining multi-step execution. It turns math problems into a controlled, correctness-checkable benchmark with tool sets, enabling systematic evaluation of model reliability under (1) large, overlapping tool catalogs and (2) the absence of the intended capability. \\ToolMATH provides actionable diagnostic evidence of failure modes in tool-augmented agents, helping identify the control mechanisms required for robustness. \\ToolMATH roughly contains 8k questions and 12k tools; we provide an additional hard-set \\ToolMATHHard with questions and tools. Our evaluation reveals that the key failure factor is due to the inability to reason, leading to the accumulation of intermediate results' errors and constrain later decisions. Tool-list redundancy do not simply add noise, but amplify small early deviations into irreversible execution drift. The benchmark highlights that when the intended capability is missing, distractor tools can sometimes serve as partial substitutes in solution paths, yet they can also mislead models into ungrounded tool trajectories. Finally, comparisons between tool-use protocols emphasize that improvements come less from local action selection and more from long-range plan coherence and disciplined use of observations.",
      "authors": [
        "Hyeonje Choi",
        "Jeongsoo Lee",
        "Hyojun Lee",
        "Jay-Yoon Lee"
      ],
      "url": "https://arxiv.org/abs/2602.21265",
      "published": "2026-02-24T09:23:12+00:00",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SE"
      ]
    },
    {
      "id": "2602.20714",
      "title": "WeirNet: A Large-Scale 3D CFD Benchmark for Geometric Surrogate Modeling of Piano Key Weirs",
      "abstract": "Reliable prediction of hydraulic performance is challenging for Piano Key Weir (PKW) design because discharge capacity depends on three-dimensional geometry and operating conditions. Surrogate models can accelerate hydraulic-structure design, but progress is limited by scarce large, well-documented datasets that jointly capture geometric variation, operating conditions, and functional performance. This study presents WeirNet, a large 3D CFD benchmark dataset for geometric surrogate modeling of PKWs. WeirNet contains 3,794 parametric, feasibility-constrained rectangular and trapezoidal PKW geometries, each scheduled at 19 discharge conditions using a consistent free-surface OpenFOAM workflow, resulting in 71,387 completed simulations that form the benchmark and with complete discharge coefficient labels. The dataset is released as multiple modalities compact parametric descriptors, watertight surface meshes and high-resolution point clouds together with standardized tasks and in-distribution and out-of-distribution splits. Representative surrogate families are benchmarked for discharge coefficient prediction. Tree-based regressors on parametric descriptors achieve the best overall accuracy, while point- and mesh-based models remain competitive and offer parameterization-agnostic inference. All surrogates evaluate in milliseconds per sample, providing orders-of-magnitude speedups over CFD runtimes. Out-of-distribution results identify geometry shift as the dominant failure mode compared to unseen discharge values, and data-efficiency experiments show diminishing returns beyond roughly 60% of the training data. By publicly releasing the dataset together with simulation setups and evaluation pipelines, WeirNet establishes a reproducible framework for data-driven hydraulic modeling and enables faster exploration of PKW designs during the early stages of hydraulic planning.",
      "authors": [
        "Lisa Lüddecke",
        "Michael Hohmann",
        "Sebastian Eilermann",
        "Jan Tillmann-Mumm",
        "Pezhman Pourabdollah",
        "Mario Oertel",
        "Oliver Niggemann"
      ],
      "url": "https://arxiv.org/abs/2602.20714",
      "published": "2026-02-24T09:19:28+00:00",
      "categories": [
        "cs.LG",
        "cs.CE"
      ]
    },
    {
      "id": "2602.20712",
      "title": "F10.7 Index Prediction: A Multiscale Decomposition Strategy with Wavelet Transform for Performance Optimization",
      "abstract": "In this study, we construct Dataset A for training, validation, and testing, and Dataset B to evaluate generalization. We propose a novel F10.7 index forecasting method using wavelet decomposition, which feeds F10.7 together with its decomposed approximate and detail signals into the iTransformer model. We also incorporate the International Sunspot Number (ISN) and its wavelet-decomposed signals to assess their influence on prediction performance. Our optimal method is then compared with the latest method from S. Yan et al. (2025) and three operational models (SWPC, BGS, CLS). Additionally, we transfer our method to the PatchTST model used in H. Ye et al. (2024) and compare our method with theirs on Dataset B. Key findings include: (1) The wavelet-based combination methods overall outperform the baseline using only F10.7 index. The prediction performance improves as higher-level approximate and detail signals are incrementally added. The Combination 6 method integrating F10.7 with its first to fifth level approximate and detail signals outperforms methods using only approximate or detail signals. (2) Incorporating ISN and its wavelet-decomposed signals does not enhance prediction performance. (3) The Combination 6 method significantly surpasses S. Yan et al. (2025) and three operational models, with RMSE, MAE, and MAPE reduced by 18.22%, 15.09%, and 8.57%, respectively, against the former method. It also excels across four different conditions of solar activity. (4) Our method demonstrates superior generalization and prediction capability over the method of H. Ye et al. (2024) across all forecast horizons. To our knowledge, this is the first application of wavelet decomposition in F10.7 prediction, substantially improving forecast performance.",
      "authors": [
        "Xuran Ma",
        "Xuebao Li",
        "Yanfang Zheng",
        "Yongshang Lv",
        "Xiaojia Ji",
        "Jiancheng Xu",
        "Hongwei Ye",
        "Zixian Wu",
        "Shuainan Yan",
        "Liang Dong",
        "Zamri Zainal Abidin",
        "Xusheng Huang",
        "Shunhuang Zhang",
        "Honglei Jin",
        "Tarik Abdul Latef",
        "Noraisyah Mohamed Shah",
        "Mohamadariff Othman",
        "Kamarul Ariffin Noordin"
      ],
      "url": "https://arxiv.org/abs/2602.20712",
      "published": "2026-02-24T09:18:57+00:00",
      "categories": [
        "astro-ph.IM",
        "astro-ph.SR",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20710",
      "title": "Counterfactual Simulation Training for Chain-of-Thought Faithfulness",
      "abstract": "Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training",
      "authors": [
        "Peter Hase",
        "Christopher Potts"
      ],
      "url": "https://arxiv.org/abs/2602.20710",
      "published": "2026-02-24T09:15:30+00:00",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.20709",
      "title": "Onboard-Targeted Segmentation of Straylight in Space Camera Sensors",
      "abstract": "This study details an artificial intelligence (AI)-based methodology for the semantic segmentation of space camera faults. Specifically, we address the segmentation of straylight effects induced by solar presence around the camera's Field of View (FoV). Anomalous images are sourced from our published dataset. Our approach emphasizes generalization across diverse flare textures, leveraging pre-training on a public dataset (Flare7k++) including flares in various non-space contexts to mitigate the scarcity of realistic space-specific data. A DeepLabV3 model with MobileNetV3 backbone performs the segmentation task. The model design targets deployment in spacecraft resource-constrained hardware. Finally, based on a proposed interface between our model and the onboard navigation pipeline, we develop custom metrics to assess the model's performance in the system-level context.",
      "authors": [
        "Riccardo Gallon",
        "Fabian Schiemenz",
        "Alessandra Menicucci",
        "Eberhard Gill"
      ],
      "url": "https://arxiv.org/abs/2602.20709",
      "published": "2026-02-24T09:15:13+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20708",
      "title": "ICON: Indirect Prompt Injection Defense for Agents based on Inference-Time Correction",
      "abstract": "Large Language Model (LLM) agents are susceptible to Indirect Prompt Injection (IPI) attacks, where malicious instructions in retrieved content hijack the agent's execution. Existing defenses typically rely on strict filtering or refusal mechanisms, which suffer from a critical limitation: over-refusal, prematurely terminating valid agentic workflows. We propose ICON, a probing-to-mitigation framework that neutralizes attacks while preserving task continuity. Our key insight is that IPI attacks leave distinct over-focusing signatures in the latent space. We introduce a Latent Space Trace Prober to detect attacks based on high intensity scores. Subsequently, a Mitigating Rectifier performs surgical attention steering that selectively manipulate adversarial query key dependencies while amplifying task relevant elements to restore the LLM's functional trajectory. Extensive evaluations on multiple backbones show that ICON achieves a competitive 0.4% ASR, matching commercial grade detectors, while yielding a over 50% task utility gain. Furthermore, ICON demonstrates robust Out of Distribution(OOD) generalization and extends effectively to multi-modal agents, establishing a superior balance between security and efficiency.",
      "authors": [
        "Che Wang",
        "Fuyao Zhang",
        "Jiaming Zhang",
        "Ziqi Zhang",
        "Yinghui Wang",
        "Longtao Huang",
        "Jianbo Gao",
        "Zhong Chen",
        "Wei Yang Bryan Lim"
      ],
      "url": "https://arxiv.org/abs/2602.20708",
      "published": "2026-02-24T09:13:05+00:00",
      "categories": [
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "id": "2602.20706",
      "title": "Online Algorithms with Unreliable Guidance",
      "abstract": "This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $β$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $β= 0$ (a.k.a. consistency) as well as when $β= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $β$ shifts from $0$ to $1$.   We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.",
      "authors": [
        "Julien Dallot",
        "Yuval Emek",
        "Yuval Gil",
        "Maciej Pacut",
        "Stefan Schmid"
      ],
      "url": "https://arxiv.org/abs/2602.20706",
      "published": "2026-02-24T09:11:56+00:00",
      "categories": [
        "cs.AI",
        "cs.DS"
      ]
    },
    {
      "id": "2602.20698",
      "title": "High-Dimensional Robust Mean Estimation with Untrusted Batches",
      "abstract": "We study high-dimensional mean estimation in a collaborative setting where data is contributed by $N$ users in batches of size $n$. In this environment, a learner seeks to recover the mean $μ$ of a true distribution $P$ from a collection of sources that are both statistically heterogeneous and potentially malicious. We formalize this challenge through a double corruption landscape: an $\\varepsilon$-fraction of users are entirely adversarial, while the remaining ``good'' users provide data from distributions that are related to $P$, but deviate by a proximity parameter $α$.   Unlike existing work on the untrusted batch model, which typically measures this deviation via total variation distance in discrete settings, we address the continuous, high-dimensional regime under two natural variants for deviation: (1) good batches are drawn from distributions with a mean-shift of $\\sqrtα$, or (2) an $α$-fraction of samples within each good batch are adversarially corrupted. In particular, the second model presents significant new challenges: in high dimensions, unlike discrete settings, even a small fraction of sample-level corruption can shift empirical means and covariances arbitrarily.   We provide two Sum-of-Squares (SoS) based algorithms to navigate this tiered corruption. Our algorithms achieve the minimax-optimal error rate $O(\\sqrt{\\varepsilon/n} + \\sqrt{d/nN} + \\sqrtα)$, demonstrating that while heterogeneity $α$ represents an inherent statistical difficulty, the influence of adversarial users is suppressed by a factor of $1/\\sqrt{n}$ due to the internal averaging afforded by the batch structure.",
      "authors": [
        "Maryam Aliakbarpour",
        "Vladimir Braverman",
        "Yuhan Liu",
        "Junze Yin"
      ],
      "url": "https://arxiv.org/abs/2602.20698",
      "published": "2026-02-24T08:59:37+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20696",
      "title": "PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding",
      "abstract": "Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the \"3H\" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.",
      "authors": [
        "Baolong Bi",
        "Yuyao Ge",
        "Shenghua Liu",
        "Yuchen He",
        "Siqian Tong",
        "Lizhe Chen",
        "Lingrui Mei",
        "Zehao Li",
        "Yiwei Wang",
        "Yujun Cai",
        "Ming-Hsuan Yang",
        "Xueqi Cheng"
      ],
      "url": "https://arxiv.org/abs/2602.20696",
      "published": "2026-02-24T08:56:52+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20687",
      "title": "How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective",
      "abstract": "Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.",
      "authors": [
        "Bo Peng",
        "Pi Bu",
        "Keyu Pan",
        "Xinrun Xu",
        "Yinxiu Zhao",
        "Miao Chen",
        "Yang Du",
        "Lin Li",
        "Jun Song",
        "Tong Xu"
      ],
      "url": "https://arxiv.org/abs/2602.20687",
      "published": "2026-02-24T08:42:41+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20684",
      "title": "Agile V: A Compliance-Ready Framework for AI-Augmented Engineering -- From Concept to Audit-Ready Delivery",
      "abstract": "Current AI-assisted engineering workflows lack a built-in mechanism to maintain task-level verification and regulatory traceability at machine-speed delivery. Agile V addresses this gap by embedding independent verification and audit artifact generation into each task cycle. The framework merges Agile iteration with V-Model verification into a continuous Infinity Loop, deploying specialized AI agents for requirements, design, build, test, and compliance, governed by mandatory human approval gates. We evaluate three hypotheses: (H1) audit-ready artifacts emerge as a by-product of development, (H2) 100% requirement-level verification is achievable with independent test generation, and (H3) verified increments can be delivered with single-digit human interactions per cycle. A feasibility case study on a Hardware-in-the-Loop system (about 500 LOC, 8 requirements, 54 tests) supports all three hypotheses: audit-ready documentation was generated automatically (H1), 100% requirement-level pass rate was achieved (H2), and only 6 prompts per cycle were required (H3), yielding an estimated 10-50x cost reduction versus a COCOMO II baseline (sensitivity range from pessimistic to optimistic assumptions). We invite independent replication to validate generalizability.",
      "authors": [
        "Christopher Koch",
        "Joshua Andreas Wellbrock"
      ],
      "url": "https://arxiv.org/abs/2602.20684",
      "published": "2026-02-24T08:41:05+00:00",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.MA"
      ]
    },
    {
      "id": "2602.20677",
      "title": "UrbanFM: Scaling Urban Spatio-Temporal Foundation Models",
      "abstract": "Urban systems, as dynamic complex systems, continuously generate spatio-temporal data streams that encode the fundamental laws of human mobility and city evolution. While AI for Science has witnessed the transformative power of foundation models in disciplines like genomics and meteorology, urban computing remains fragmented due to \"scenario-specific\" models, which are overfitted to specific regions or tasks, hindering their generalizability. To bridge this gap and advance spatio-temporal foundation models for urban systems, we adopt scaling as the central perspective and systematically investigate two key questions: what to scale and how to scale. Grounded in first-principles analysis, we identify three critical dimensions: heterogeneity, correlation, and dynamics, aligning these principles with the fundamental scientific properties of urban spatio-temporal data. Specifically, to address heterogeneity through data scaling, we construct WorldST. This billion-scale corpus standardizes diverse physical signals, such as traffic flow and speed, from over 100 global cities into a unified data format. To enable computation scaling for modeling correlations, we introduce the MiniST unit, a novel split mechanism that discretizes continuous spatio-temporal fields into learnable computational units to unify representations of grid-based and sensor-based observations. Finally, addressing dynamics via architecture scaling, we propose UrbanFM, a minimalist self-attention architecture designed with limited inductive biases to autonomously learn dynamic spatio-temporal dependencies from massive data. Furthermore, we establish EvalST, the largest-scale urban spatio-temporal benchmark to date. Extensive experiments demonstrate that UrbanFM achieves remarkable zero-shot generalization across unseen cities and tasks, marking a pivotal first step toward large-scale urban spatio-temporal foundation models.",
      "authors": [
        "Wei Chen",
        "Yuqian Wu",
        "Junle Chen",
        "Xiaofang Zhou",
        "Yuxuan Liang"
      ],
      "url": "https://arxiv.org/abs/2602.20677",
      "published": "2026-02-24T08:26:46+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20676",
      "title": "PRECTR-V2:Unified Relevance-CTR Framework with Cross-User Preference Mining, Exposure Bias Correction, and LLM-Distilled Encoder Optimization",
      "abstract": "In search systems, effectively coordinating the two core objectives of search relevance matching and click-through rate (CTR) prediction is crucial for discovering users' interests and enhancing platform revenue. In our prior work PRECTR, we proposed a unified framework to integrate these two subtasks,thereby eliminating their inconsistency and leading to mutual benefit.However, our previous work still faces three main challenges. First, low-active users and new users have limited search behavioral data, making it difficult to achieve effective personalized relevance preference modeling. Second, training data for ranking models predominantly come from high-relevance exposures, creating a distribution mismatch with the broader candidate space in coarse-ranking, leading to generalization bias. Third, due to the latency constraint, the original model employs an Emb+MLP architecture with a frozen BERT encoder, which prevents joint optimization and creates misalignment between representation learning and CTR fine-tuning. To solve these issues, we further reinforce our method and propose PRECTR-V2. Specifically, we mitigate the low-activity users' sparse behavior problem by mining global relevance preferences under the specific query, which facilitates effective personalized relevance modeling for cold-start scenarios. Subsequently, we construct hard negative samples through embedding noise injection and relevance label reconstruction, and optimize their relative ranking against positive samples via pairwise loss, thereby correcting exposure bias. Finally, we pretrain a lightweight transformer-based encoder via knowledge distillation from LLM and SFT on the text relevance classification task. This encoder replaces the frozen BERT module, enabling better adaptation to CTR fine-tuning and advancing beyond the traditional Emb+MLP paradigm.",
      "authors": [
        "Shuzhi Cao",
        "Rong Chen",
        "Ailong He",
        "Shuguang Han",
        "Jufeng Chen"
      ],
      "url": "https://arxiv.org/abs/2602.20676",
      "published": "2026-02-24T08:26:17+00:00",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20671",
      "title": "Bikelution: Federated Gradient-Boosting for Scalable Shared Micro-Mobility Demand Forecasting",
      "abstract": "The rapid growth of dockless bike-sharing systems has generated massive spatio-temporal datasets useful for fleet allocation, congestion reduction, and sustainable mobility. Bike demand, however, depends on several external factors, making traditional time-series models insufficient. Centralized Machine Learning (CML) yields high-accuracy forecasts but raises privacy and bandwidth issues when data are distributed across edge devices. To overcome these limitations, we propose Bikelution, an efficient Federated Learning (FL) solution based on gradient-boosted trees that preserves privacy while delivering accurate mid-term demand forecasts up to six hours ahead. Experiments on three real-world BSS datasets show that Bikelution is comparable to its CML-based variant and outperforms the current state-of-the-art. The results highlight the feasibility of privacy-aware demand forecasting and outline the trade-offs between FL and CML approaches.",
      "authors": [
        "Antonios Tziorvas",
        "Andreas Tritsarolis",
        "Yannis Theodoridis"
      ],
      "url": "https://arxiv.org/abs/2602.20671",
      "published": "2026-02-24T08:21:28+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20670",
      "title": "CAMEL: Confidence-Gated Reflection for Reward Modeling",
      "abstract": "Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.",
      "authors": [
        "Zirui Zhu",
        "Hailun Xu",
        "Yang Luo",
        "Yong Liu",
        "Kanchan Sarkar",
        "Kun Xu",
        "Yang You"
      ],
      "url": "https://arxiv.org/abs/2602.20670",
      "published": "2026-02-24T08:20:08+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20659",
      "title": "Recursive Belief Vision Language Action Models",
      "abstract": "Vision-language-action models must enable agents to execute long-horizon tasks under partial observability. However, most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. While semantic grounding is important, long-horizon manipulation fundamentally requires persistent, action-conditioned state representations. Current VLAs lack such representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once per task, the VLM provides high-level intent, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5 percent and 37.5 percent higher success rates on multi-stage pick-and-place and stacking tasks, respectively, compared to pi_0. It also reduces inference latency by up to five times relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show the belief module is the primary driver of performance, increasing success rates from 32.5 percent without belief to 77.5 percent with belief.",
      "authors": [
        "Vaidehi Bagaria",
        "Bijo Sebastian",
        "Nirav Kumar Patel"
      ],
      "url": "https://arxiv.org/abs/2602.20659",
      "published": "2026-02-24T08:02:16+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20658",
      "title": "Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video",
      "abstract": "Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.",
      "authors": [
        "Mohammad Sadra Rajabi",
        "Aanuoluwapo Ojelade",
        "Sunwook Kim",
        "Maury A. Nussbaum"
      ],
      "url": "https://arxiv.org/abs/2602.20658",
      "published": "2026-02-24T08:01:49+00:00",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20652",
      "title": "DANCE: Doubly Adaptive Neighborhood Conformal Estimation",
      "abstract": "The recent developments of complex deep learning models have led to unprecedented ability to accurately predict across multiple data representation types. Conformal prediction for uncertainty quantification of these models has risen in popularity, providing adaptive, statistically-valid prediction sets. For classification tasks, conformal methods have typically focused on utilizing logit scores. For pre-trained models, however, this can result in inefficient, overly conservative set sizes when not calibrated towards the target task. We propose DANCE, a doubly locally adaptive nearest-neighbor based conformal algorithm combining two novel nonconformity scores directly using the data's embedded representation. DANCE first fits a task-adaptive kernel regression model from the embedding layer before using the learned kernel space to produce the final prediction sets for uncertainty quantification. We test against state-of-the-art local, task-adapted and zero-shot conformal baselines, demonstrating DANCE's superior blend of set size efficiency and robustness across various datasets.",
      "authors": [
        "Brandon R. Feng",
        "Brian J. Reich",
        "Daniel Beaglehole",
        "Xihaier Luo",
        "David Keetae Park",
        "Shinjae Yoo",
        "Zhechao Huang",
        "Xueyu Mao",
        "Olcay Boz",
        "Jungeum Kim"
      ],
      "url": "https://arxiv.org/abs/2602.20652",
      "published": "2026-02-24T07:54:53+00:00",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20651",
      "title": "Sparse Bayesian Deep Functional Learning with Structured Region Selection",
      "abstract": "In modern applications such as ECG monitoring, neuroimaging, wearable sensing, and industrial equipment diagnostics, complex and continuously structured data are ubiquitous, presenting both challenges and opportunities for functional data analysis. However, existing methods face a critical trade-off: conventional functional models are limited by linearity, whereas deep learning approaches lack interpretable region selection for sparse effects. To bridge these gaps, we propose a sparse Bayesian functional deep neural network (sBayFDNN). It learns adaptive functional embeddings through a deep Bayesian architecture to capture complex nonlinear relationships, while a structured prior enables interpretable, region-wise selection of influential domains with quantified uncertainty. Theoretically, we establish rigorous approximation error bounds, posterior consistency, and region selection consistency. These results provide the first theoretical guarantees for a Bayesian deep functional model, ensuring its reliability and statistical rigor. Empirically, comprehensive simulations and real-world studies confirm the effectiveness and superiority of sBayFDNN. Crucially, sBayFDNN excels in recognizing intricate dependencies for accurate predictions and more precisely identifies functionally meaningful regions, capabilities fundamentally beyond existing approaches.",
      "authors": [
        "Xiaoxian Zhu",
        "Yingmeng Li",
        "Shuangge Ma",
        "Mengyun Wu"
      ],
      "url": "https://arxiv.org/abs/2602.20651",
      "published": "2026-02-24T07:53:59+00:00",
      "categories": [
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20650",
      "title": "Dataset Color Quantization: A Training-Oriented Framework for Dataset-Level Compression",
      "abstract": "Large-scale image datasets are fundamental to deep learning, but their high storage demands pose challenges for deployment in resource-constrained environments. While existing approaches reduce dataset size by discarding samples, they often ignore the significant redundancy within each image -- particularly in the color space. To address this, we propose Dataset Color Quantization (DCQ), a unified framework that compresses visual datasets by reducing color-space redundancy while preserving information crucial for model training. DCQ achieves this by enforcing consistent palette representations across similar images, selectively retaining semantically important colors guided by model perception, and maintaining structural details necessary for effective feature learning. Extensive experiments across CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that DCQ significantly improves training performance under aggressive compression, offering a scalable and robust solution for dataset-level storage reduction. Code is available at \\href{https://github.com/he-y/Dataset-Color-Quantization}{https://github.com/he-y/Dataset-Color-Quantization}.",
      "authors": [
        "Chenyue Yu",
        "Lingao Xiao",
        "Jinhong Deng",
        "Ivor W. Tsang",
        "Yang He"
      ],
      "url": "https://arxiv.org/abs/2602.20650",
      "published": "2026-02-24T07:53:58+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20648",
      "title": "CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models",
      "abstract": "Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.",
      "authors": [
        "Anqi Li",
        "Chenxiao Wang",
        "Yu Lu",
        "Renjun Xu",
        "Lizhi Ma",
        "Zhenzhong Lan"
      ],
      "url": "https://arxiv.org/abs/2602.20648",
      "published": "2026-02-24T07:52:56+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20647",
      "title": "Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books",
      "abstract": "I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty",
      "authors": [
        "W. Frederick Zimmerman"
      ],
      "url": "https://arxiv.org/abs/2602.20647",
      "published": "2026-02-24T07:52:35+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20646",
      "title": "On the Convergence of Stochastic Gradient Descent with Perturbed Forward-Backward Passes",
      "abstract": "We study stochastic gradient descent (SGD) for composite optimization problems with $N$ sequential operators subject to perturbations in both the forward and backward passes. Unlike classical analyses that treat gradient noise as additive and localized, perturbations to intermediate outputs and gradients cascade through the computational graph, compounding geometrically with the number of operators. We present the first comprehensive theoretical analysis of this setting. Specifically, we characterize how forward and backward perturbations propagate and amplify within a single gradient step, derive convergence guarantees for both general non-convex objectives and functions satisfying the Polyak--Łojasiewicz condition, and identify conditions under which perturbations do not deteriorate the asymptotic convergence order. As a byproduct, our analysis furnishes a theoretical explanation for the gradient spiking phenomenon widely observed in deep learning, precisely characterizing the conditions under which training recovers from spikes or diverges. Experiments on logistic regression with convex and non-convex regularization validate our theories, illustrating the predicted spike behavior and the asymmetric sensitivity to forward versus backward perturbations.",
      "authors": [
        "Boao Kong",
        "Hengrui Zhang",
        "Kun Yuan"
      ],
      "url": "https://arxiv.org/abs/2602.20646",
      "published": "2026-02-24T07:47:15+00:00",
      "categories": [
        "math.OC",
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20643",
      "title": "TrajGPT-R: Generating Urban Mobility Trajectory with Reinforcement Learning-Enhanced Generative Pre-trained Transformer",
      "abstract": "Mobility trajectories are essential for understanding urban dynamics and enhancing urban planning, yet access to such data is frequently hindered by privacy concerns. This research introduces a transformative framework for generating large-scale urban mobility trajectories, employing a novel application of a transformer-based model pre-trained and fine-tuned through a two-phase process. Initially, trajectory generation is conceptualized as an offline reinforcement learning (RL) problem, with a significant reduction in vocabulary space achieved during tokenization. The integration of Inverse Reinforcement Learning (IRL) allows for the capture of trajectory-wise reward signals, leveraging historical data to infer individual mobility preferences. Subsequently, the pre-trained model is fine-tuned using the constructed reward model, effectively addressing the challenges inherent in traditional RL-based autoregressive methods, such as long-term credit assignment and handling of sparse reward environments. Comprehensive evaluations on multiple datasets illustrate that our framework markedly surpasses existing models in terms of reliability and diversity. Our findings not only advance the field of urban mobility modeling but also provide a robust methodology for simulating urban data, with significant implications for traffic management and urban development planning. The implementation is publicly available at https://github.com/Wangjw6/TrajGPT_R.",
      "authors": [
        "Jiawei Wang",
        "Chuang Yang",
        "Jiawei Yong",
        "Xiaohang Xu",
        "Hongjun Wang",
        "Noboru Koshizuka",
        "Shintaro Fukushima",
        "Ryosuke Shibasaki",
        "Renhe Jiang"
      ],
      "url": "https://arxiv.org/abs/2602.20643",
      "published": "2026-02-24T07:44:19+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20639",
      "title": "Grounding LLMs in Scientific Discovery via Embodied Actions",
      "abstract": "Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive \"execute-then-response\" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.",
      "authors": [
        "Bo Zhang",
        "Jinfeng Zhou",
        "Yuxuan Chen",
        "Jianing Yin",
        "Minlie Huang",
        "Hongning Wang"
      ],
      "url": "https://arxiv.org/abs/2602.20639",
      "published": "2026-02-24T07:37:18+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20638",
      "title": "Identifying two piecewise linear additive value functions from anonymous preference information",
      "abstract": "Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.",
      "authors": [
        "Vincent Auriau",
        "Khaled Belahcene",
        "Emmanuel Malherbe",
        "Vincent Mousseau",
        "Marc Pirlot"
      ],
      "url": "https://arxiv.org/abs/2602.20638",
      "published": "2026-02-24T07:37:02+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20636",
      "title": "SurgAtt-Tracker: Online Surgical Attention Tracking via Temporal Proposal Reranking and Motion-Aware Refinement",
      "abstract": "Accurate and stable field-of-view (FoV) guidance is critical for safe and efficient minimally invasive surgery, yet existing approaches often conflate visual attention estimation with downstream camera control or rely on direct object-centric assumptions. In this work, we formulate surgical attention tracking as a spatio-temporal learning problem and model surgeon focus as a dense attention heatmap, enabling continuous and interpretable frame-wise FoV guidance. We propose SurgAtt-Tracker, a holistic framework that robustly tracks surgical attention by exploiting temporal coherence through proposal-level reranking and motion-aware refinement, rather than direct regression. To support systematic training and evaluation, we introduce SurgAtt-1.16M, a large-scale benchmark with a clinically grounded annotation protocol that enables comprehensive heatmap-based attention analysis across procedures and institutions. Extensive experiments on multiple surgical datasets demonstrate that SurgAtt-Tracker consistently achieves state-of-the-art performance and strong robustness under occlusion, multi-instrument interference, and cross-domain settings. Beyond attention tracking, our approach provides a frame-wise FoV guidance signal that can directly support downstream robotic FoV planning and automatic camera control.",
      "authors": [
        "Rulin Zhou",
        "Guankun Wang",
        "An Wang",
        "Yujie Ma",
        "Lixin Ouyang",
        "Bolin Cui",
        "Junyan Li",
        "Chaowei Zhu",
        "Mingyang Li",
        "Ming Chen",
        "Xiaopin Zhong",
        "Peng Lu",
        "Jiankun Wang",
        "Xianming Liu",
        "Hongliang Ren"
      ],
      "url": "https://arxiv.org/abs/2602.20636",
      "published": "2026-02-24T07:30:51+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20634",
      "title": "Enhancing Hate Speech Detection on Social Media: A Comparative Analysis of Machine Learning Models and Text Transformation Approaches",
      "abstract": "The proliferation of hate speech on social media platforms has necessitated the development of effective detection and moderation tools. This study evaluates the efficacy of various machine learning models in identifying hate speech and offensive language and investigates the potential of text transformation techniques to neutralize such content. We compare traditional models like CNNs and LSTMs with advanced neural network models such as BERT and its derivatives, alongside exploring hybrid models that combine different architectural features. Our results indicate that while advanced models like BERT show superior accuracy due to their deep contextual understanding, hybrid models exhibit improved capabilities in certain scenarios. Furthermore, we introduce innovative text transformation approaches that convert negative expressions into neutral ones, thereby potentially mitigating the impact of harmful content. The implications of these findings are discussed, highlighting the strengths and limitations of current technologies and proposing future directions for more robust hate speech detection systems.",
      "authors": [
        "Saurabh Mishra",
        "Shivani Thakur",
        "Radhika Mamidi"
      ],
      "url": "https://arxiv.org/abs/2602.20634",
      "published": "2026-02-24T07:26:17+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20629",
      "title": "QEDBENCH: Quantifying the Alignment Gap in Automated Evaluation of University-Level Mathematical Proofs",
      "abstract": "As Large Language Models (LLMs) saturate elementary benchmarks, the research frontier has shifted from generation to the reliability of automated evaluation. We demonstrate that standard \"LLM-as-a-Judge\" protocols suffer from a systematic Alignment Gap when applied to upper-undergraduate to early graduate level mathematics. To quantify this, we introduce QEDBench, the first large-scale dual-rubric alignment benchmark to systematically measure alignment with human experts on university-level math proofs by contrasting course-specific rubrics against expert common knowledge criteria. By deploying a dual-evaluation matrix (7 judges x 5 solvers) against 1,000+ hours of human evaluation, we reveal that certain frontier evaluators like Claude Opus 4.5, DeepSeek-V3, Qwen 2.5 Max, and Llama 4 Maverick exhibit significant positive bias (up to +0.18, +0.20, +0.30, +0.36 mean score inflation, respectively). Furthermore, we uncover a critical reasoning gap in the discrete domain: while Gemini 3.0 Pro achieves state-of-the-art performance (0.91 average human evaluation score), other reasoning models like GPT-5 Pro and Claude Sonnet 4.5 see their performance significantly degrade in discrete domains. Specifically, their average human evaluation scores drop to 0.72 and 0.63 in Discrete Math, and to 0.74 and 0.50 in Graph Theory. In addition to these research results, we also release QEDBench as a public benchmark for evaluating and improving AI judges. Our benchmark is publicly published at https://github.com/qqliu/Yale-QEDBench.",
      "authors": [
        "Santiago Gonzalez",
        "Alireza Amiri Bavandpour",
        "Peter Ye",
        "Edward Zhang",
        "Ruslans Aleksejevs",
        "Todor Antić",
        "Polina Baron",
        "Sujeet Bhalerao",
        "Shubhrajit Bhattacharya",
        "Zachary Burton",
        "John Byrne",
        "Hyungjun Choi",
        "Nujhat Ahmed Disha",
        "Koppany István Encz",
        "Yuchen Fang",
        "Robert Joseph George",
        "Ebrahim Ghorbani",
        "Alan Goldfarb",
        "Jing Guo",
        "Meghal Gupta",
        "Stefano Huber",
        "Annika Kanckos",
        "Minjung Kang",
        "Hyun Jong Kim",
        "Dino Lorenzini",
        "Levi Lorenzo",
        "Tianyi Mao",
        "Giovanni Marzenta",
        "Ariane M. Masuda",
        "Lukas Mauth",
        "Ana Mickovic",
        "Andres Miniguano-Trujillo",
        "Antoine Moulin",
        "Wenqi Ni",
        "Tomos Parry",
        "Kevin Ren",
        "Hossein Roodbarani",
        "Mathieu Rundström",
        "Manjil Saikia",
        "Detchat Samart",
        "Rebecca Steiner",
        "Connor Stewart",
        "Dhara Thakkar",
        "Jeffrey Tse",
        "Vasiliki Velona",
        "Yunhai Xiang",
        "Sibel Yalçın",
        "Jun Yan",
        "Ji Zeng",
        "Arman Cohan",
        "Quanquan C. Liu"
      ],
      "url": "https://arxiv.org/abs/2602.20629",
      "published": "2026-02-24T07:23:28+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20628",
      "title": "When can we trust untrusted monitoring? A safety case sketch across collusion strategies",
      "abstract": "AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.",
      "authors": [
        "Nelson Gardner-Challis",
        "Jonathan Bostock",
        "Georgiy Kozhevnikov",
        "Morgan Sinclaire",
        "Joan Velja",
        "Alessandro Abate",
        "Charlie Griffin"
      ],
      "url": "https://arxiv.org/abs/2602.20628",
      "published": "2026-02-24T07:23:21+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20624",
      "title": "Physics-based phenomenological characterization of cross-modal bias in multimodal models",
      "abstract": "The term 'algorithmic fairness' is used to evaluate whether AI models operate fairly in both comparative (where fairness is understood as formal equality, such as \"treat like cases as like\") and non-comparative (where unfairness arises from the model's inaccuracy, arbitrariness, or inscrutability) contexts. Recent advances in multimodal large language models (MLLMs) are breaking new ground in multimodal understanding, reasoning, and generation; however, we argue that inconspicuous distortions arising from complex multimodal interaction dynamics can lead to systematic bias. The purpose of this position paper is twofold: first, it is intended to acquaint AI researchers with phenomenological explainable approaches that rely on the physical entities that the machine experiences during training/inference, as opposed to the traditional cognitivist symbolic account or metaphysical approaches; second, it is to state that this phenomenological doctrine will be practically useful for tackling algorithmic fairness issues in MLLMs. We develop a surrogate physics-based model that describes transformer dynamics (i.e., semantic network structure and self-/cross-attention) to analyze the dynamics of cross-modal bias in MLLM, which are not fully captured by conventional embedding- or representation-level analyses. We support this position through multi-input diagnostic experiments: 1) perturbation-based analyses of emotion classification using Qwen2.5-Omni and Gemma 3n, and 2) dynamical analysis of Lorenz chaotic time-series prediction through the physical surrogate. Across two architecturally distinct MLLMs, we show that multimodal inputs can reinforce modality dominance rather than mitigate it, as revealed by structured error-attractor patterns under systematic label perturbation, complemented by dynamical analysis.",
      "authors": [
        "Hyeongmo Kim",
        "Sohyun Kang",
        "Yerin Choi",
        "Seungyeon Ji",
        "Junhyuk Woo",
        "Hyunsuk Chung",
        "Soyeon Caren Han",
        "Kyungreem Han"
      ],
      "url": "https://arxiv.org/abs/2602.20624",
      "published": "2026-02-24T07:21:08+00:00",
      "categories": [
        "cs.AI",
        "cond-mat.stat-mech"
      ]
    },
    {
      "id": "2602.20616",
      "title": "Knowing the Unknown: Interpretable Open-World Object Detection via Concept Decomposition Model",
      "abstract": "Open-world object detection (OWOD) requires incrementally detecting known categories while reliably identifying unknown objects. Existing methods primarily focus on improving unknown recall, yet overlook interpretability, often leading to known-unknown confusion and reduced prediction reliability. This paper aims to make the entire OWOD framework interpretable, enabling the detector to truly \"knowing the unknown\". To this end, we propose a concept-driven InterPretable OWOD framework(IPOW) by introducing a Concept Decomposition Model (CDM) for OWOD, which explicitly decomposes the coupled RoI features in Faster R-CNN into discriminative, shared, and background concepts. Discriminative concepts identify the most discriminative features to enlarge the distances between known categories, while shared and background concepts, due to their strong generalization ability, can be readily transferred to detect unknown categories. Leveraging the interpretable framework, we identify that known-unknown confusion arises when unknown objects fall into the discriminative space of known classes. To address this, we propose Concept-Guided Rectification (CGR) to further resolve such confusion. Extensive experiments show that IPOW significantly improves unknown recall while mitigating confusion, and provides concept-level interpretability for both known and unknown predictions.",
      "authors": [
        "Xueqiang Lv",
        "Shizhou Zhang",
        "Yinghui Xing",
        "Di Xu",
        "Peng Wang",
        "Yanning Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.20616",
      "published": "2026-02-24T07:08:47+00:00",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20611",
      "title": "Amortized Bayesian inference for actigraph time sheet data from mobile devices",
      "abstract": "Mobile data technologies use ``actigraphs'' to furnish information on health variables as a function of a subject's movement. The advent of wearable devices and related technologies has propelled the creation of health databases consisting of human movement data to conduct research on mobility patterns and health outcomes. Statistical methods for analyzing high-resolution actigraph data depend on the specific inferential context, but the advent of Artificial Intelligence (AI) frameworks require that the methods be congruent to transfer learning and amortization. This article devises amortized Bayesian inference for actigraph time sheets. We pursue a Bayesian approach to ensure full propagation of uncertainty and its quantification using a hierarchical dynamic linear model. We build our analysis around actigraph data from the Physical Activity through Sustainable Transport Approaches in Los Angeles (PASTA-LA) study conducted by the Fielding School of Public Health in the University of California, Los Angeles. Apart from achieving probabilistic imputation of actigraph time sheets, we are also able to statistically learn about the time-varying impact of explanatory variables on the magnitude of acceleration (MAG) for a cohort of subjects.",
      "authors": [
        "Daniel Zhou",
        "Sudipto Banerjee"
      ],
      "url": "https://arxiv.org/abs/2602.20611",
      "published": "2026-02-24T07:03:15+00:00",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "id": "2602.20610",
      "title": "SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference",
      "abstract": "Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions.",
      "authors": [
        "Cuong Chi Le",
        "Minh V. T Pham",
        "Tung Vu Duy",
        "Cuong Duc Van",
        "Huy N. Phan",
        "Hoang N. Phan",
        "Tien N. Nguyen"
      ],
      "url": "https://arxiv.org/abs/2602.20610",
      "published": "2026-02-24T07:01:17+00:00",
      "categories": [
        "cs.SE",
        "cs.CL"
      ]
    },
    {
      "id": "2602.20595",
      "title": "OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services",
      "abstract": "Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens'' -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.",
      "authors": [
        "Longxiang Wang",
        "Xiang Zheng",
        "Xuhao Zhang",
        "Yao Zhang",
        "Ye Wu",
        "Cong Wang"
      ],
      "url": "https://arxiv.org/abs/2602.20595",
      "published": "2026-02-24T06:35:22+00:00",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20593",
      "title": "Is the Trigger Essential? A Feature-Based Triggerless Backdoor Attack in Vertical Federated Learning",
      "abstract": "As a distributed collaborative machine learning paradigm, vertical federated learning (VFL) allows multiple passive parties with distinct features and one active party with labels to collaboratively train a model. Although it is known for the privacy-preserving capabilities, VFL still faces significant privacy and security threats from backdoor attacks. Existing backdoor attacks typically involve an attacker implanting a trigger into the model during the training phase and executing the attack by adding the trigger to the samples during the inference phase. However, in this paper, we find that triggers are not essential for backdoor attacks in VFL. In light of this, we disclose a new backdoor attack pathway in VFL by introducing a feature-based triggerless backdoor attack. This attack operates under a more stringent security assumption, where the attacker is honest-but-curious rather than malicious during the training phase. It comprises three modules: label inference for the targeted backdoor attack, poison generation with amplification and perturbation mechanisms, and backdoor execution to implement the attack. Extensive experiments on five benchmark datasets demonstrate that our attack outperforms three baseline backdoor attacks by 2 to 50 times while minimally impacting the main task. Even in VFL scenarios with 32 passive parties and only one set of auxiliary data, our attack maintains high performance. Moreover, when confronted with distinct defense strategies, our attack remains largely unaffected and exhibits strong robustness. We hope that the disclosure of this triggerless backdoor attack pathway will encourage the community to revisit security threats in VFL scenarios and inspire researchers to develop more robust and practical defense strategies.",
      "authors": [
        "Yige Liu",
        "Yiwei Lou",
        "Che Wang",
        "Yongzhi Cao",
        "Hanpin Wang"
      ],
      "url": "https://arxiv.org/abs/2602.20593",
      "published": "2026-02-24T06:33:29+00:00",
      "categories": [
        "cs.LG",
        "cs.CR"
      ]
    },
    {
      "id": "2602.20585",
      "title": "Characterizing Online and Private Learnability under Distributional Constraints via Generalized Smoothness",
      "abstract": "Understanding minimal assumptions that enable learning and generalization is perhaps the central question of learning theory. Several celebrated results in statistical learning theory, such as the VC theorem and Littlestone's characterization of online learnability, establish conditions on the hypothesis class that allow for learning under independent data and adversarial data, respectively. Building upon recent work bridging these extremes, we study sequential decision making under distributional adversaries that can adaptively choose data-generating distributions from a fixed family $U$ and ask when such problems are learnable with sample complexity that behaves like the favorable independent case. We provide a near complete characterization of families $U$ that admit learnability in terms of a notion known as generalized smoothness i.e. a distribution family admits VC-dimension-dependent regret bounds for every finite-VC hypothesis class if and only if it is generalized smooth. Further, we give universal algorithms that achieve low regret under any generalized smooth adversary without explicit knowledge of $U$. Finally, when $U$ is known, we provide refined bounds in terms of a combinatorial parameter, the fragmentation number, that captures how many disjoint regions can carry nontrivial mass under $U$. These results provide a nearly complete understanding of learnability under distributional adversaries. In addition, building upon the surprising connection between online learning and differential privacy, we show that the generalized smoothness also characterizes private learnability under distributional constraints.",
      "authors": [
        "Moïse Blanchard",
        "Abhishek Shetty",
        "Alexander Rakhlin"
      ],
      "url": "https://arxiv.org/abs/2602.20585",
      "published": "2026-02-24T06:15:59+00:00",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20580",
      "title": "Personal Information Parroting in Language Models",
      "abstract": "Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.",
      "authors": [
        "Nishant Subramani",
        "Kshitish Ghate",
        "Mona Diab"
      ],
      "url": "https://arxiv.org/abs/2602.20580",
      "published": "2026-02-24T06:02:03+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20578",
      "title": "Upper-Linearizability of Online Non-Monotone DR-Submodular Maximization over Down-Closed Convex Sets",
      "abstract": "We study online maximization of non-monotone Diminishing-Return(DR)-submodular functions over down-closed convex sets, a regime where existing projection-free online methods suffer from suboptimal regret and limited feedback guarantees. Our main contribution is a new structural result showing that this class is $1/e$-linearizable under carefully designed exponential reparametrization, scaling parameter, and surrogate potential, enabling a reduction to online linear optimization. As a result, we obtain $O(T^{1/2})$ static regret with a single gradient query per round and unlock adaptive and dynamic regret guarantees, together with improved rates under semi-bandit, bandit, and zeroth-order feedback. Across all feedback models, our bounds strictly improve the state of the art.",
      "authors": [
        "Yiyang Lu",
        "Haresh Jadav",
        "Mohammad Pedramfar",
        "Ranveer Singh",
        "Vaneet Aggarwal"
      ],
      "url": "https://arxiv.org/abs/2602.20578",
      "published": "2026-02-24T05:59:42+00:00",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20574",
      "title": "GATES: Self-Distillation under Privileged Context with Consensus Gating",
      "abstract": "We study self-distillation in settings where supervision is unreliable: there are no ground truth labels, verifiable rewards, or external graders to evaluate answers. We focus on document-grounded question answering with asymmetric context, where a single model serves as both tutor (with access to a relevant source document during training) and student (answering from the question alone at test time). Rather than assuming tutor correctness, we derive supervision online from tutor consensus by sampling multiple document-grounded reasoning traces and using agreement to gate learning. Conditioned on this reliability signal, we distill knowledge through full tutor reasoning trajectories (not just final answers), providing a dense and stable learning signal. Empirically, this consensus-gated trajectory distillation substantially improves transfer to the document-free student. Held-out in-domain accuracy under asymmetric evaluation improves from 46.0\\% to 62.0\\%, and average (maj@8) accuracy on public document-free math benchmarks improves from 20.2\\% to 35.4\\%.",
      "authors": [
        "Alex Stein",
        "Furong Huang",
        "Tom Goldstein"
      ],
      "url": "https://arxiv.org/abs/2602.20574",
      "published": "2026-02-24T05:56:20+00:00",
      "categories": [
        "cs.LG",
        "cs.CL"
      ]
    },
    {
      "id": "2602.20573",
      "title": "Benchmarking GNN Models on Molecular Regression Tasks with CKA-Based Representation Analysis",
      "abstract": "Molecules are commonly represented as SMILES strings, which can be readily converted to fixed-size molecular fingerprints. These fingerprints serve as feature vectors to train ML/DL models for molecular property prediction tasks in the field of computational chemistry, drug discovery, biochemistry, and materials science. Recent research has demonstrated that SMILES can be used to construct molecular graphs where atoms are nodes ($V$) and bonds are edges ($E$). These graphs can subsequently be used to train geometric DL models like GNN. GNN learns the inherent structural relationships within a molecule rather than depending on fixed-size fingerprints. Although GNN are powerful aggregators, their efficacy on smaller datasets and inductive biases across different architectures is less studied. In our present study, we performed a systematic benchmarking of four different GNN architectures across a diverse domain of datasets (physical chemistry, biological, and analytical). Additionally, we have also implemented a hierarchical fusion (GNN+FP) framework for target prediction. We observed that the fusion framework consistently outperforms or matches the performance of standalone GNN (RMSE improvement > $7\\%$) and baseline models. Further, we investigated the representational similarity using centered kernel alignment (CKA) between GNN and fingerprint embeddings and found that they occupy highly independent latent spaces (CKA $\\le0.46$). The cross-architectural CKA score suggests a high convergence between isotopic models like GCN, GraphSAGE and GIN (CKA $\\geq0.88$), with GAT learning moderately independent representation (CKA $0.55-0.80$).",
      "authors": [
        "Rajan",
        "Ishaan Gupta"
      ],
      "url": "https://arxiv.org/abs/2602.20573",
      "published": "2026-02-24T05:53:24+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20571",
      "title": "CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation",
      "abstract": "Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.",
      "authors": [
        "Ayush Sawarni",
        "Jiyuan Tan",
        "Vasilis Syrgkanis"
      ],
      "url": "https://arxiv.org/abs/2602.20571",
      "published": "2026-02-24T05:44:25+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20567",
      "title": "Stability and Generalization of Push-Sum Based Decentralized Optimization over Directed Graphs",
      "abstract": "Push-Sum-based decentralized learning enables optimization over directed communication networks, where information exchange may be asymmetric. While convergence properties of such methods are well understood, their finite-iteration stability and generalization behavior remain unclear due to structural bias induced by column-stochastic mixing and asymmetric error propagation. In this work, we develop a unified uniform-stability framework for the Stochastic Gradient Push (SGP) algorithm that captures the effect of directed topology. A key technical ingredient is an imbalance-aware consistency bound for Push-Sum, which controls consensus deviation through two quantities: the stationary distribution imbalance parameter $δ$ and the spectral gap $(1-λ)$ governing mixing speed. This decomposition enables us to disentangle statistical effects from topology-induced bias. We establish finite-iteration stability and optimization guarantees for both convex objectives and non-convex objectives satisfying the Polyak--Łojasiewicz condition. For convex problems, SGP attains excess generalization error of order $\\tilde{\\mathcal{O}}\\!\\left(\\frac{1}{\\sqrt{mn}}+\\fracγ{δ(1-λ)}+γ\\right)$ under step-size schedules, and we characterize the corresponding optimal early stopping time that minimizes this bound. For PŁ objectives, we obtain convex-like optimization and generalization rates with dominant dependence proportional to $κ\\!\\left(1+\\frac{1}{δ(1-λ)}\\right)$, revealing a multiplicative coupling between problem conditioning and directed communication topology. Our analysis clarifies when Push-Sum correction is necessary compared with standard decentralized SGD and quantifies how imbalance and mixing jointly shape the best attainable learning performance.",
      "authors": [
        "Yifei Liang",
        "Yan Sun",
        "Xiaochun Cao",
        "Li Shen"
      ],
      "url": "https://arxiv.org/abs/2602.20567",
      "published": "2026-02-24T05:32:03+00:00",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20558",
      "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production",
      "abstract": "Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.",
      "authors": [
        "Yucheng Shi",
        "Ying Li",
        "Yu Wang",
        "Yesu Feng",
        "Arjun Rao",
        "Rein Houthooft",
        "Shradha Sehgal",
        "Jin Wang",
        "Hao Zhen",
        "Ninghao Liu",
        "Linas Baltrunas"
      ],
      "url": "https://arxiv.org/abs/2602.20558",
      "published": "2026-02-24T05:15:24+00:00",
      "categories": [
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "id": "2602.20557",
      "title": "GENSR: Symbolic Regression Based in Equation Generative Space",
      "abstract": "Symbolic Regression (SR) tries to reveal the hidden equations behind observed data. However, most methods search within a discrete equation space, where the structural modifications of equations rarely align with their numerical behavior, leaving fitting error feedback too noisy to guide exploration. To address this challenge, we propose GenSR, a generative latent space-based SR framework following the `map construction -> coarse localization -> fine search'' paradigm. Specifically, GenSR first pretrains a dual-branch Conditional Variational Autoencoder (CVAE) to reparameterize symbolic equations into a generative latent space with symbolic continuity and local numerical smoothness. This space can be regarded as a well-structured `map'' of the equation space, providing directional signals for search. At inference, the CVAE coarsely localizes the input data to promising regions in the latent space. Then, a modified CMA-ES refines the candidate region, leveraging smooth latent gradients. From a Bayesian perspective, GenSR reframes the SR task as maximizing the conditional distribution $p(\\mathrm{Equ.} \\mid \\mathrm{Num.})$, with CVAE training achieving this objective through the Evidence Lower Bound (ELBO). This new perspective provides a theoretical guarantee for the effectiveness of GenSR. Extensive experiments show that GenSR jointly optimizes predictive accuracy, expression simplicity, and computational efficiency, while remaining robust under noise.",
      "authors": [
        "Qian Li",
        "Yuxiao Hu",
        "Juncheng Liu",
        "Yuntian Chen"
      ],
      "url": "https://arxiv.org/abs/2602.20557",
      "published": "2026-02-24T05:14:34+00:00",
      "categories": [
        "cs.LG",
        "cs.SC"
      ]
    },
    {
      "id": "2602.20555",
      "title": "Standard Transformers Achieve the Minimax Rate in Nonparametric Regression with $C^{s,λ}$ Targets",
      "abstract": "The tremendous success of Transformer models in fields such as large language models and computer vision necessitates a rigorous theoretical investigation. To the best of our knowledge, this paper is the first work proving that standard Transformers can approximate Hölder functions $ C^{s,λ}\\left([0,1]^{d\\times n}\\right) $$ (s\\in\\mathbb{N}_{\\geq0},0<λ\\leq1) $ under the $L^t$ distance ($t \\in [1, \\infty]$) with arbitrary precision. Building upon this approximation result, we demonstrate that standard Transformers achieve the minimax optimal rate in nonparametric regression for Hölder target functions. It is worth mentioning that, by introducing two metrics: the size tuple and the dimension vector, we provide a fine-grained characterization of Transformer structures, which facilitates future research on the generalization and optimization errors of Transformers with different structures. As intermediate results, we also derive the upper bounds for the Lipschitz constant of standard Transformers and their memorization capacity, which may be of independent interest. These findings provide theoretical justification for the powerful capabilities of Transformer models.",
      "authors": [
        "Yanming Lai",
        "Defeng Sun"
      ],
      "url": "https://arxiv.org/abs/2602.20555",
      "published": "2026-02-24T05:14:01+00:00",
      "categories": [
        "stat.ML",
        "cs.IT",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20549",
      "title": "Sample-efficient evidence estimation of score based priors for model selection",
      "abstract": "The choice of prior is central to solving ill-posed imaging inverse problems, making it essential to select one consistent with the measurements $y$ to avoid severe bias. In Bayesian inverse problems, this could be achieved by evaluating the model evidence $p(y \\mid M)$ under different models $M$ that specify the prior and then selecting the one with the highest value. Diffusion models are the state-of-the-art approach to solving inverse problems with a data-driven prior; however, directly computing the model evidence with respect to a diffusion prior is intractable. Furthermore, most existing model evidence estimators require either many pointwise evaluations of the unnormalized prior density or an accurate clean prior score. We propose \\method, an estimator of the model evidence of a diffusion prior by integrating over the time-marginals of posterior sampling methods. Our method leverages the large amount of intermediate samples naturally obtained during the reverse diffusion sampling process to obtain an accurate estimation of the model evidence using only a handful of posterior samples (e.g., 20). We also demonstrate how to implement our estimator in tandem with recent diffusion posterior sampling methods. Empirically, our estimator matches the model evidence when it can be computed analytically, and it is able to both select the correct diffusion model prior and diagnose prior misfit under different highly ill-conditioned, non-linear inverse problems, including a real-world black hole imaging problem.",
      "authors": [
        "Frederic Wang",
        "Katherine L. Bouman"
      ],
      "url": "https://arxiv.org/abs/2602.20549",
      "published": "2026-02-24T05:06:46+00:00",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ME"
      ]
    },
    {
      "id": "2602.20547",
      "title": "What Drives Students' Use of AI Chatbots? Technology Acceptance in Conversational AI",
      "abstract": "Conversational AI tools have been rapidly adopted by students and are becoming part of their learning routines. To understand what drives this adoption, we draw on the Technology Acceptance Model (TAM) and examine how perceived usefulness and perceived ease of use relate to students' behavioral intention to use conversational AI that generates responses for learning tasks. We extend TAM by incorporating trust, perceived enjoyment, and subjective norms as additional factors that capture social and affective influences and uncertainty around AI outputs.   Using partial least squares structural equation modeling, we find perceived usefulness remains the strongest predictor of students' intention to use conversational AI. However, perceived ease of use does not exert a significant direct effect on behavioral intention once other factors are considered, operating instead indirectly through perceived usefulness. Trust and subjective norms significantly influence perceptions of usefulness, while perceived enjoyment exerts both a direct and indirect effect on usage intentions. These findings suggest that adoption decisions for conversational AI systems are influenced less by effort-related considerations and more by confidence in system outputs, affective engagement, and social context. Future research is needed to further examine how these acceptance relationships generalize across different conversational systems and usage contexts.",
      "authors": [
        "Griffin Pitts",
        "Sanaz Motamedi"
      ],
      "url": "https://arxiv.org/abs/2602.20547",
      "published": "2026-02-24T05:00:41+00:00",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ]
    },
    {
      "id": "2602.20541",
      "title": "Maximin Share Guarantees via Limited Cost-Sensitive Sharing",
      "abstract": "We study the problem of fairly allocating indivisible goods when limited sharing is allowed, that is, each good may be allocated to up to $k$ agents, while incurring a cost for sharing. While classic maximin share (MMS) allocations may not exist in many instances, we demonstrate that allowing controlled sharing can restore fairness guarantees that are otherwise unattainable in certain scenarios. (1) Our first contribution shows that exact maximin share (MMS) allocations are guaranteed to exist whenever goods are allowed to be cost-sensitively shared among at least half of the agents and the number of agents is even; for odd numbers of agents, we obtain a slightly weaker MMS guarantee. (2) We further design a Shared Bag-Filling Algorithm that guarantees a $(1 - C)(k - 1)$-approximate MMS allocation, where $C$ is the maximum cost of sharing a good. Notably, when $(1 - C)(k - 1) \\geq 1$, our algorithm recovers an exact MMS allocation. (3) We additionally introduce the Sharing Maximin Share (SMMS) fairness notion, a natural extension of MMS to the $k$-sharing setting. (4) We show that SMMS allocations always exist under identical utilities and for instances with two agents. (5) We construct a counterexample to show the impossibility of the universal existence of an SMMS allocation. (6) Finally, we establish a connection between SMMS and constrained MMS (CMMS), yielding approximation guarantees for SMMS via existing CMMS results. These contributions provide deep theoretical insights for the problem of fair resource allocation when a limited sharing of resources are allowed in multi-agent environments.",
      "authors": [
        "Hana Salavcova",
        "Martin Černý",
        "Arpita Biswas"
      ],
      "url": "https://arxiv.org/abs/2602.20541",
      "published": "2026-02-24T04:39:46+00:00",
      "categories": [
        "cs.GT",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20532",
      "title": "Actor-Curator: Co-adaptive Curriculum Learning via Policy-Improvement Bandits for RL Post-Training",
      "abstract": "Post-training large foundation models with reinforcement learning typically relies on massive and heterogeneous datasets, making effective curriculum learning both critical and challenging. In this work, we propose ACTOR-CURATOR, a scalable and fully automated curriculum learning framework for reinforcement learning post-training of large language models (LLMs). ACTOR-CURATOR learns a neural curator that dynamically selects training problems from large problem banks by directly optimizing for expected policy performance improvement. We formulate problem selection as a non-stationary stochastic bandit problem, derive a principled loss function based on online stochastic mirror descent, and establish regret guarantees under partial feedback. Empirically, ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across a wide range of challenging reasoning benchmarks, demonstrating improved training stability and efficiency. Notably, it achieves relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline and up to 80% speedup. These results suggest that ACTOR-CURATOR is a powerful and practical approach for scalable LLM post-training.",
      "authors": [
        "Zhengyao Gu",
        "Jonathan Light",
        "Raul Astudillo",
        "Ziyu Ye",
        "Langzhou He",
        "Henry Peng Zou",
        "Wei Cheng",
        "Santiago Paternain",
        "Philip S. Yu",
        "Yisong Yue"
      ],
      "url": "https://arxiv.org/abs/2602.20532",
      "published": "2026-02-24T04:19:48+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.20530",
      "title": "Memory-guided Prototypical Co-occurrence Learning for Mixed Emotion Recognition",
      "abstract": "Emotion recognition from multi-modal physiological and behavioral signals plays a pivotal role in affective computing, yet most existing models remain constrained to the prediction of singular emotions in controlled laboratory settings. Real-world human emotional experiences, by contrast, are often characterized by the simultaneous presence of multiple affective states, spurring recent interest in mixed emotion recognition as an emotion distribution learning problem. Current approaches, however, often neglect the valence consistency and structured correlations inherent among coexisting emotions. To address this limitation, we propose a Memory-guided Prototypical Co-occurrence Learning (MPCL) framework that explicitly models emotion co-occurrence patterns. Specifically, we first fuse multi-modal signals via a multi-scale associative memory mechanism. To capture cross-modal semantic relationships, we construct emotion-specific prototype memory banks, yielding rich physiological and behavioral representations, and employ prototype relation distillation to ensure cross-modal alignment in the latent prototype space. Furthermore, inspired by human cognitive memory systems, we introduce a memory retrieval strategy to extract semantic-level co-occurrence associations across emotion categories. Through this bottom-up hierarchical abstraction process, our model learns affectively informative representations for accurate emotion distribution prediction. Comprehensive experiments on two public datasets demonstrate that MPCL consistently outperforms state-of-the-art methods in mixed emotion recognition, both quantitatively and qualitatively.",
      "authors": [
        "Ming Li",
        "Yong-Jin Liu",
        "Fang Liu",
        "Huankun Sheng",
        "Yeying Fan",
        "Yixiang Wei",
        "Minnan Luo",
        "Weizhan Zhang",
        "Wenping Wang"
      ],
      "url": "https://arxiv.org/abs/2602.20530",
      "published": "2026-02-24T04:11:25+00:00",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "id": "2602.20528",
      "title": "Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning",
      "abstract": "The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a \"thinking\" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.",
      "authors": [
        "Justin Lovelace",
        "Christian Belardi",
        "Sofian Zalouk",
        "Adhitya Polavaram",
        "Srivatsa Kundurthy",
        "Kilian Q. Weinberger"
      ],
      "url": "https://arxiv.org/abs/2602.20528",
      "published": "2026-02-24T04:09:31+00:00",
      "categories": [
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "id": "2602.21262",
      "title": "Under the Influence: Quantifying Persuasion and Vigilance in Large Language Models",
      "abstract": "With increasing integration of Large Language Models (LLMs) into areas of high-stakes human decision-making, it is important to understand the risks they introduce as advisors. To be useful advisors, LLMs must sift through large amounts of content, written with both benevolent and malicious intent, and then use this information to convince a user to take a specific action. This involves two social capacities: vigilance (the ability to determine which information to use, and which to discard) and persuasion (synthesizing the available evidence to make a convincing argument). While existing work has investigated these capacities in isolation, there has been little prior investigation of how these capacities may be linked. Here, we use a simple multi-turn puzzle-solving game, Sokoban, to study LLMs' abilities to persuade and be rationally vigilant towards other LLM agents. We find that puzzle-solving performance, persuasive capability, and vigilance are dissociable capacities in LLMs. Performing well on the game does not automatically mean a model can detect when it is being misled, even if the possibility of deception is explicitly mentioned. % as part of the prompt. However, LLMs do consistently modulate their token use, using fewer tokens to reason when advice is benevolent and more when it is malicious, even if they are still persuaded to take actions leading them to failure. To our knowledge, our work presents the first investigation of the relationship between persuasion, vigilance, and task performance in LLMs, and suggests that monitoring all three independently will be critical for future work in AI safety.",
      "authors": [
        "Sasha Robinson",
        "Kerem Oktar",
        "Katherine M. Collins",
        "Ilia Sucholutsky",
        "Kelsey R. Allen"
      ],
      "url": "https://arxiv.org/abs/2602.21262",
      "published": "2026-02-24T04:09:21+00:00",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "id": "2602.20527",
      "title": "A Generalized Apprenticeship Learning Framework for Capturing Evolving Student Pedagogical Strategies",
      "abstract": "Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) have advanced rapidly in recent years and have been successfully applied to e-learning environments like intelligent tutoring systems (ITSs). Despite great success, the broader application of DRL to educational technologies has been limited due to major challenges such as sample inefficiency and difficulty designing the reward function. In contrast, Apprenticeship Learning (AL) uses a few expert demonstrations to infer the expert's underlying reward functions and derive decision-making policies that generalize and replicate optimal behavior. In this work, we leverage a generalized AL framework, THEMES, to induce effective pedagogical policies by capturing the complexities of the expert student learning process, where multiple reward functions may dynamically evolve over time. We evaluate the effectiveness of THEMES against six state-of-the-art baselines, demonstrating its superior performance and highlighting its potential as a powerful alternative for inducing effective pedagogical policies and show that it can achieve high performance, with an AUC of 0.899 and a Jaccard of 0.653, using only 18 trajectories of a previous semester to predict student pedagogical decisions in a later semester.",
      "authors": [
        "Md Mirajul Islam",
        "Xi Yang",
        "Adittya Soukarjya Saha",
        "Rajesh Debnath",
        "Min Chi"
      ],
      "url": "https://arxiv.org/abs/2602.20527",
      "published": "2026-02-24T04:08:31+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20520",
      "title": "How Do Inpainting Artifacts Propagate to Language?",
      "abstract": "We study how visual artifacts introduced by diffusion-based inpainting affect language generation in vision-language models. We use a two-stage diagnostic setup in which masked image regions are reconstructed and then provided to captioning models, enabling controlled comparisons between captions generated from original and reconstructed inputs. Across multiple datasets, we analyze the relationship between reconstruction fidelity and downstream caption quality. We observe consistent associations between pixel-level and perceptual reconstruction metrics and both lexical and semantic captioning performance. Additional analysis of intermediate visual representations and attention patterns shows that inpainting artifacts lead to systematic, layer-dependent changes in model behavior. Together, these results provide a practical diagnostic framework for examining how visual reconstruction quality influences language generation in multimodal systems.",
      "authors": [
        "Pratham Yashwante",
        "Davit Abrahamyan",
        "Shresth Grover",
        "Sukruth Rao"
      ],
      "url": "https://arxiv.org/abs/2602.20520",
      "published": "2026-02-24T03:46:33+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20517",
      "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination",
      "abstract": "Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.",
      "authors": [
        "Rakshit Trivedi",
        "Kartik Sharma",
        "David C Parkes"
      ],
      "url": "https://arxiv.org/abs/2602.20517",
      "published": "2026-02-24T03:37:42+00:00",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20513",
      "title": "From Performance to Purpose: A Sociotechnical Taxonomy for Evaluating Large Language Model Utility",
      "abstract": "As large language models (LLMs) continue to improve at completing discrete tasks, they are being integrated into increasingly complex and diverse real-world systems. However, task-level success alone does not establish a model's fit for use in practice. In applied, high-stakes settings, LLM effectiveness is driven by a wider array of sociotechnical determinants that extend beyond conventional performance measures. Although a growing set of metrics capture many of these considerations, they are rarely organized in a way that supports consistent evaluation, leaving no unified taxonomy for assessing and comparing LLM utility across use cases. To address this gap, we introduce the Language Model Utility Taxonomy (LUX), a comprehensive framework that structures utility evaluation across four domains: performance, interaction, operations, and governance. Within each domain, LUX is organized hierarchically into thematically aligned dimensions and components, each grounded in metrics that enable quantitative comparison and alignment of model selection with intended use. In addition, an external dynamic web tool is provided to support exploration of the framework by connecting each component to a repository of relevant metrics (factors) for applied evaluation.",
      "authors": [
        "Gavin Levinson",
        "Keith Feldman"
      ],
      "url": "https://arxiv.org/abs/2602.20513",
      "published": "2026-02-24T03:31:07+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20502",
      "title": "ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory",
      "abstract": "Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.   We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.   To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.   This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.   Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.",
      "authors": [
        "Hongbin Zhong",
        "Fazle Faisal",
        "Luis França",
        "Tanakorn Leesatapornwongsa",
        "Adriana Szekeres",
        "Kexin Rong",
        "Suman Nath"
      ],
      "url": "https://arxiv.org/abs/2602.20502",
      "published": "2026-02-24T03:03:18+00:00",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.21941",
      "title": "MERRY: Semantically Decoupled Evaluation of Multimodal Emotional and Role Consistencies of Role-Playing Agents",
      "abstract": "Multimodal Role-Playing Agents (MRPAs) are attracting increasing attention due to their ability to deliver more immersive multimodal emotional interactions. However, existing studies still rely on pure textual benchmarks to evaluate the text responses of MRPAs, while delegating the assessment of their multimodal expressions solely to modality-synthesis metrics. This evaluation paradigm, on the one hand, entangles semantic assessment with modality generation, leading to ambiguous error attribution, and on the other hand remains constrained by the heavy reliance on human judgment. To this end, we propose MERRY, a semantically decoupled evaluation framework for assessing Multimodal Emotional and Role consistencies of Role-playing agents. This framework introduce five refined metrics for EC and three for RC. Notably, we transform the traditional subjective scoring approach into a novel bidirectional-evidence-finding task, significantly improving the human agreement of LLM-as-Judge evaluations. Based on MERRY, we conduct extensive evaluations. Our empirical results primarily reveal that: (1) Training on synthetic datasets tends to reduce emotional consistency, whereas training on real-world datasets improves it; (2) Existing models suffer from emotional templatization and simplification, exhibiting positive-bias and performance bottleneck in fine-grained negative emotions; (3) Simple prompting method strengthens the weak models but constrains the strong ones, while simple fine-tuning method suffers from poor role generalization. Codes and dataset are available.",
      "authors": [
        "Zhenyu Wang",
        "Xiaofen Xing",
        "Yirong Chen",
        "Xiangmin Xu"
      ],
      "url": "https://arxiv.org/abs/2602.21941",
      "published": "2026-02-24T02:53:58+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20497",
      "title": "LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration",
      "abstract": "Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub.",
      "authors": [
        "Peiliang Cai",
        "Jiacheng Liu",
        "Haowen Xu",
        "Xinyu Wang",
        "Chang Zou",
        "Linfeng Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.20497",
      "published": "2026-02-24T02:53:28+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20494",
      "title": "KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning",
      "abstract": "Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.",
      "authors": [
        "Haotian Si",
        "Changhua Pei",
        "Xiao He",
        "Zeyan Li",
        "Zhe Xie",
        "Zexin Wang",
        "Jiyao Hu",
        "Zhaoyang Yu",
        "Tieying Zhang",
        "Dan Pei",
        "Jianhui Li",
        "Gaogang Xie"
      ],
      "url": "https://arxiv.org/abs/2602.20494",
      "published": "2026-02-24T02:50:38+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20492",
      "title": "Wireless Federated Multi-Task LLM Fine-Tuning via Sparse-and-Orthogonal LoRA",
      "abstract": "Decentralized federated learning (DFL) based on low-rank adaptation (LoRA) enables mobile devices with multi-task datasets to collaboratively fine-tune a large language model (LLM) by exchanging locally updated parameters with a subset of neighboring devices via wireless connections for knowledge integration.However, directly aggregating parameters fine-tuned on heterogeneous datasets induces three primary issues across the DFL life-cycle: (i) \\textit{catastrophic knowledge forgetting during fine-tuning process}, arising from conflicting update directions caused by data heterogeneity; (ii) \\textit{inefficient communication and convergence during model aggregation process}, due to bandwidth-intensive redundant model transmissions; and (iii) \\textit{multi-task knowledge interference during inference process}, resulting from incompatible knowledge representations coexistence during inference. To address these issues in a fully decentralized scenario, we first propose a sparse-and-orthogonal LoRA that ensures orthogonality between model updates to eliminate direction conflicts during fine-tuning.Then, we analyze how device connection topology affects multi-task performance, prompting a cluster-based topology design during aggregation.Finally, we propose an implicit mixture of experts (MoE) mechanism to avoid the coexistence of incompatible knowledge during inference. Simulation results demonstrate that the proposed approach effectively reduces communication resource consumption by up to $73\\%$ and enhances average performance by $5\\%$ compared with the traditional LoRA method.",
      "authors": [
        "Nuocheng Yang",
        "Sihua Wang",
        "Ouwen Huan",
        "Mingzhe Chen",
        "Tony Q. S. Quek",
        "Changchuan Yin"
      ],
      "url": "https://arxiv.org/abs/2602.20492",
      "published": "2026-02-24T02:45:32+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20486",
      "title": "Hybrid LLM-Embedded Dialogue Agents for Learner Reflection: Designing Responsive and Theory-Driven Interactions",
      "abstract": "Dialogue systems have long supported learner reflections, with theoretically grounded, rule-based designs offering structured scaffolding but often struggling to respond to shifts in engagement. Large Language Models (LLMs), in contrast, can generate context-sensitive responses but are not informed by decades of research on how learning interactions should be structured, raising questions about their alignment with pedagogical theories. This paper presents a hybrid dialogue system that embeds LLM responsiveness within a theory-aligned, rule-based framework to support learner reflections in a culturally responsive robotics summer camp. The rule-based structure grounds dialogue in self-regulated learning theory, while the LLM decides when and how to prompt deeper reflections, responding to evolving conversation context. We analyze themes across dialogues to explore how our hybrid system shaped learner reflections. Our findings indicate that LLM-embedded dialogues supported richer learner reflections on goals and activities, but also introduced challenges due to repetitiveness and misalignment in prompts, reducing engagement.",
      "authors": [
        "Paras Sharma",
        "YuePing Sha",
        "Janet Shufor Bih Epse Fofang",
        "Brayden Yan",
        "Jess A. Turner",
        "Nicole Balay",
        "Hubert O. Asare",
        "Angela E. B. Stewart",
        "Erin Walker"
      ],
      "url": "https://arxiv.org/abs/2602.20486",
      "published": "2026-02-24T02:29:05+00:00",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20480",
      "title": "VINA: Variational Invertible Neural Architectures",
      "abstract": "The distinctive architectural features of normalizing flows (NFs), notably bijectivity and tractable Jacobians, make them well-suited for generative modeling. Invertible neural networks (INNs) build on these principles to address supervised inverse problems, enabling direct modeling of both forward and inverse mappings. In this paper, we revisit these architectures from both theoretical and practical perspectives and address a key gap in the literature: the lack of theoretical guarantees on approximation quality under realistic assumptions, whether for posterior inference in INNs or for generative modeling with NFs.   We introduce a unified framework for INNs and NFs based on variational unsupervised loss functions, inspired by analogous formulations in related areas such as generative adversarial networks (GANs) and the Precision-Recall divergence for training normalizing flows. Within this framework, we derive theoretical performance guarantees, quantifying posterior accuracy for INNs and distributional accuracy for NFs, under assumptions that are weaker and more practically realistic than those used in prior work.   Building on these theoretical results, we conduct extensive case studies to distill general design principles and practical guidelines. We conclude by demonstrating the effectiveness of our approach on a realistic ocean-acoustic inversion problem.",
      "authors": [
        "Shubhanshu Shekhar",
        "Mohammad Javad Khojasteh",
        "Ananya Acharya",
        "Tony Tohme",
        "Kamal Youcef-Toumi"
      ],
      "url": "https://arxiv.org/abs/2602.20480",
      "published": "2026-02-24T02:16:44+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20475",
      "title": "PhyGHT: Physics-Guided HyperGraph Transformer for Signal Purification at the HL-LHC",
      "abstract": "The High-Luminosity Large Hadron Collider (HL-LHC) at CERN will produce unprecedented datasets capable of revealing fundamental properties of the universe. However, realizing its discovery potential faces a significant challenge: extracting small signal fractions from overwhelming backgrounds dominated by approximately 200 simultaneous pileup collisions. This extreme noise severely distorts the physical observables required for accurate reconstruction. To address this, we introduce the Physics-Guided Hypergraph Transformer (PhyGHT), a hybrid architecture that combines distance-aware local graph attention with global self-attention to mirror the physical topology of particle showers formed in proton-proton collisions. Crucially, we integrate a Pileup Suppression Gate (PSG), an interpretable, physics-constrained mechanism that explicitly learns to filter soft noise prior to hypergraph aggregation. To validate our approach, we release a novel simulated dataset of top-quark pair production to model extreme pileup conditions. PhyGHT outperforms state-of-the-art baselines from the ATLAS and CMS experiments in predicting the signal's energy and mass correction factors. By accurately reconstructing the top quark's invariant mass, we demonstrate how machine learning innovation and interdisciplinary collaboration can directly advance scientific discovery at the frontiers of experimental physics and enhance the HL-LHC's discovery potential. The dataset and code are available at https://github.com/rAIson-Lab/PhyGHT",
      "authors": [
        "Mohammed Rakib",
        "Luke Vaughan",
        "Shivang Patel",
        "Flera Rizatdinova",
        "Alexander Khanov",
        "Atriya Sen"
      ],
      "url": "https://arxiv.org/abs/2602.20475",
      "published": "2026-02-24T02:07:33+00:00",
      "categories": [
        "hep-ex",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20468",
      "title": "CGSTA: Cross-Scale Graph Contrast with Stability-Aware Alignment for Multivariate Time-Series Anomaly Detection",
      "abstract": "Multivariate time-series anomaly detection is essential for reliable industrial control, telemetry, and service monitoring. However, the evolving inter-variable dependencies and inevitable noise render it challenging. Existing methods often use single-scale graphs or instance-level contrast. Moreover, learned dynamic graphs can overfit noise without a stable anchor, causing false alarms or misses. To address these challenges, we propose the CGSTA framework with two key innovations. First, Dynamic Layered Graph Construction (DLGC) forms local, regional, and global views of variable relations for each sliding window; rather than contrasting whole windows, Contrastive Discrimination across Scales (CDS) contrasts graph representations within each view and aligns the same window across views to make learning structure-aware. Second, Stability-Aware Alignment (SAA) maintains a per-scale stable reference learned from normal data and guides the current window's fast-changing graphs toward it to suppress noise. We fuse the multi-scale and temporal features and use a conditional density estimator to produce per-time-step anomaly scores. Across four benchmarks, CGSTA delivers optimal performance on PSM and WADI, and is comparable to the baseline methods on SWaT and SMAP.",
      "authors": [
        "Zhongpeng Qi",
        "Jun Zhang",
        "Wei Li",
        "Zhuoxuan Liang"
      ],
      "url": "https://arxiv.org/abs/2602.20468",
      "published": "2026-02-24T01:58:39+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20467",
      "title": "Elimination-compensation pruning for fully-connected neural networks",
      "abstract": "The unmatched ability of Deep Neural Networks in capturing complex patterns in large and noisy datasets is often associated with their large hypothesis space, and consequently to the vast amount of parameters that characterize model architectures. Pruning techniques affirmed themselves as valid tools to extract sparse representations of neural networks parameters, carefully balancing between compression and preservation of information. However, a fundamental assumption behind pruning is that expendable weights should have small impact on the error of the network, while highly important weights should tend to have a larger influence on the inference. We argue that this idea could be generalized; what if a weight is not simply removed but also compensated with a perturbation of the adjacent bias, which does not contribute to the network sparsity? Our work introduces a novel pruning method in which the importance measure of each weight is computed considering the output behavior after an optimal perturbation of its adjacent bias, efficiently computable by automatic differentiation. These perturbations can be then applied directly after the removal of each weight, independently of each other. After deriving analytical expressions for the aforementioned quantities, numerical experiments are conducted to benchmark this technique against some of the most popular pruning strategies, demonstrating an intrinsic efficiency of the proposed approach in very diverse machine learning scenarios. Finally, our findings are discussed and the theoretical implications of our results are presented.",
      "authors": [
        "Enrico Ballini",
        "Luca Muscarnera",
        "Alessio Fumagalli",
        "Anna Scotti",
        "Francesco Regazzoni"
      ],
      "url": "https://arxiv.org/abs/2602.20467",
      "published": "2026-02-24T01:56:12+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20465",
      "title": "Prior-Agnostic Incentive-Compatible Exploration",
      "abstract": "In bandit settings, optimizing long-term regret metrics requires exploration, which corresponds to sometimes taking myopically sub-optimal actions. When a long-lived principal merely recommends actions to be executed by a sequence of different agents (as in an online recommendation platform) this provides an incentive misalignment: exploration is \"worth it\" for the principal but not for the agents. Prior work studies regret minimization under the constraint of Bayesian Incentive-Compatibility in a static stochastic setting with a fixed and common prior shared amongst the agents and the algorithm designer.   We show that (weighted) swap regret bounds on their own suffice to cause agents to faithfully follow forecasts in an approximate Bayes Nash equilibrium, even in dynamic environments in which agents have conflicting prior beliefs and the mechanism designer has no knowledge of any agents beliefs. To obtain these bounds, it is necessary to assume that the agents have some degree of uncertainty not just about the rewards, but about their arrival time -- i.e. their relative position in the sequence of agents served by the algorithm. We instantiate our abstract bounds with concrete algorithms for guaranteeing adaptive and weighted regret in bandit settings.",
      "authors": [
        "Ramya Ramalingam",
        "Osbert Bastani",
        "Aaron Roth"
      ],
      "url": "https://arxiv.org/abs/2602.20465",
      "published": "2026-02-24T01:53:08+00:00",
      "categories": [
        "cs.GT",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20463",
      "title": "A Long-Short Flow-Map Perspective for Drifting Models",
      "abstract": "This paper provides a reinterpretation of the Drifting Model~\\cite{deng2026generative} through a semigroup-consistent long-short flow-map factorization. We show that a global transport process can be decomposed into a long-horizon flow map followed by a short-time terminal flow map admitting a closed-form optimal velocity representation, and that taking the terminal interval length to zero recovers exactly the drifting field together with a conservative impulse term required for flow-map consistency. Based on this perspective, we propose a new likelihood learning formulation that aligns the long-short flow-map decomposition with density evolution under transport. We validate the framework through both theoretical analysis and empirical evaluations on benchmark tests, and further provide a theoretical interpretation of the feature-space optimization while highlighting several open problems for future study.",
      "authors": [
        "Zhiqi Li",
        "Bo Zhu"
      ],
      "url": "https://arxiv.org/abs/2602.20463",
      "published": "2026-02-24T01:48:52+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20461",
      "title": "Nonparametric Teaching of Attention Learners",
      "abstract": "Attention learners, neural networks built on the attention mechanism, e.g., transformers, excel at learning the implicit relationships that relate sequences to their corresponding properties, e.g., mapping a given sequence of tokens to the probability of the next token. However, the learning process tends to be costly. To address this, we present a novel paradigm named Attention Neural Teaching (AtteNT) that reinterprets the learning process through a nonparametric teaching perspective. Specifically, the latter provides a theoretical framework for teaching mappings that are implicitly defined (i.e., nonparametric) via example selection. Such an implicit mapping is embodied through a dense set of sequence-property pairs, with the AtteNT teacher selecting a subset to accelerate convergence in attention learner training. By analytically investigating the role of attention on parameter-based gradient descent during training, and recasting the evolution of attention learners, shaped by parameter updates, through functional gradient descent in nonparametric teaching, we show for the first time that teaching attention learners is consistent with teaching importance-adaptive nonparametric learners. These new findings readily commit AtteNT to enhancing learning efficiency of attention learners. Specifically, we observe training time reductions of 13.01% for LLMs and 20.58% for ViTs, spanning both fine-tuning and training-from-scratch regimes. Crucially, these gains are achieved without compromising accuracy; in fact, performance is consistently preserved and often enhanced across a diverse set of downstream tasks.",
      "authors": [
        "Chen Zhang",
        "Jianghui Wang",
        "Bingyang Cheng",
        "Zhongtao Chen",
        "Wendong XU",
        "Cong Wang",
        "Marco Canini",
        "Francesco Orabona",
        "Yik Chung WU",
        "Ngai Wong"
      ],
      "url": "https://arxiv.org/abs/2602.20461",
      "published": "2026-02-24T01:42:48+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20459",
      "title": "PreScience: A Benchmark for Forecasting Scientific Contributions",
      "abstract": "Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.",
      "authors": [
        "Anirudh Ajith",
        "Amanpreet Singh",
        "Jay DeYoung",
        "Nadav Kunievsky",
        "Austin C. Kozlowski",
        "Oyvind Tafjord",
        "James Evans",
        "Daniel S. Weld",
        "Tom Hope",
        "Doug Downey"
      ],
      "url": "https://arxiv.org/abs/2602.20459",
      "published": "2026-02-24T01:37:53+00:00",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.20457",
      "title": "Oracle-Robust Online Alignment for Large Language Models",
      "abstract": "We study online alignment of large language models under misspecified preference feedback, where the observed preference oracle deviates from an ideal but unknown ground-truth oracle. The online LLM alignment problem is a bi-level reinforcement problem due to the coupling between data collection and policy updates. Recently, the problem has been reduced to tractable single-level objective in the SAIL (Self-Improving Efficient Online Alignment) framework. In this paper, we introduce a pointwise oracle uncertainty set in this problem and formulate an oracle-robust online alignment objective as a worst-case optimization problem. For log-linear policies, we show that this robust objective admits an exact closed-form decomposition into the original loss function plus an explicit sensitivity penalty. We develop projected stochastic composite updates for the resulting weakly convex objective and prove $\\widetilde{O}(\\varepsilon^{-2})$ oracle complexity for reaching approximate stationarity.",
      "authors": [
        "Zimeng Li",
        "Mudit Gaur",
        "Vaneet Aggarwal"
      ],
      "url": "https://arxiv.org/abs/2602.20457",
      "published": "2026-02-24T01:33:35+00:00",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20450",
      "title": "Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning",
      "abstract": "Federated Learning (FL) enables a distributed client-server architecture where multiple clients collaboratively train a global Machine Learning (ML) model without sharing sensitive local data. However, FL often results in lower accuracy than traditional ML algorithms due to statistical heterogeneity across clients. Prior works attempt to address this by using model updates, such as loss and bias, from client models to select participants that can improve the global model's accuracy. However, these updates neither accurately represent a client's heterogeneity nor are their selection methods deterministic. We mitigate these limitations by introducing Terraform, a novel client selection methodology that uses gradient updates and a deterministic selection algorithm to select heterogeneous clients for retraining. This bi-pronged approach allows Terraform to achieve up to 47 percent higher accuracy over prior works. We further demonstrate its efficiency through comprehensive ablation studies and training time analyses, providing strong justification for the robustness of Terraform.",
      "authors": [
        "Nihal Balivada",
        "Shrey Gupta",
        "Shashank Shreedhar Bhatt",
        "Suyash Gupta"
      ],
      "url": "https://arxiv.org/abs/2602.20450",
      "published": "2026-02-24T01:19:10+00:00",
      "categories": [
        "cs.DC",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20449",
      "title": "Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference",
      "abstract": "Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space despite a vocabulary of only 20 amino acids. These differences motivate research into how transformer-based architectures operate differently in the protein domain and how we can better leverage PLMs to solve protein-related tasks. In this work, we begin by directly comparing how the distribution of information stored across layers of attention heads differs between the protein and natural language domain. Furthermore, we adapt a simple early-exit technique-originally used in the natural language domain to improve efficiency at the cost of performance-to achieve both increased accuracy and substantial efficiency gains in protein non-structural property prediction by allowing the model to automatically select protein representations from the intermediate layers of the PLMs for the specific task and protein at hand. We achieve performance gains ranging from 0.4 to 7.01 percentage points while simultaneously improving efficiency by over 10 percent across models and non-structural prediction tasks. Our work opens up an area of research directly comparing how language models change behavior when moved into the protein domain and advances language modeling in biological domains.",
      "authors": [
        "Anna Hart",
        "Chi Han",
        "Jeonghwan Kim",
        "Huimin Zhao",
        "Heng Ji"
      ],
      "url": "https://arxiv.org/abs/2602.20449",
      "published": "2026-02-24T01:18:30+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "q-bio.BM"
      ]
    },
    {
      "id": "2602.20442",
      "title": "Imputation of Unknown Missingness in Sparse Electronic Health Records",
      "abstract": "Machine learning holds great promise for advancing the field of medicine, with electronic health records (EHRs) serving as a primary data source. However, EHRs are often sparse and contain missing data due to various challenges and limitations in data collection and sharing between healthcare providers. Existing techniques for imputing missing values predominantly focus on known unknowns, such as missing or unavailable values of lab test results; most do not explicitly address situations where it is difficult to distinguish what is missing. For instance, a missing diagnosis code in an EHR could signify either that the patient has not been diagnosed with the condition or that a diagnosis was made, but not shared by a provider. Such situations fall into the paradigm of unknown unknowns. To address this challenge, we develop a general purpose algorithm for denoising data to recover unknown missing values in binary EHRs. We design a transformer-based denoising neural network where the output is thresholded adaptively to recover values in cases where we predict data are missing. Our results demonstrate improved accuracy in denoising medical codes within a real EHR dataset compared to existing imputation approaches and leads to increased performance on downstream tasks using the denoised data. In particular, when applying our method to a real world application, predicting hospital readmission from EHRs, our method achieves statistically significant improvement over all existing baselines.",
      "authors": [
        "Jun Han",
        "Josue Nassar",
        "Sanjit Singh Batra",
        "Aldo Cordova-Palomera",
        "Vijay Nori",
        "Robert E. Tillman"
      ],
      "url": "https://arxiv.org/abs/2602.20442",
      "published": "2026-02-24T01:04:02+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20433",
      "title": "Disentangling Geometry, Performance, and Training in Language Models",
      "abstract": "Geometric properties of Transformer weights, particularly the unembedding matrix, have been widely useful in language model interpretability research. Yet, their utility for estimating downstream performance remains unclear. In this work, we systematically investigate the relationship between model performance and the unembedding matrix geometry, particularly its effective rank. Our experiments, involving a suite of 108 OLMo-style language models trained under controlled variation, reveal several key findings. While the best-performing models often exhibit a high effective rank, this trend is not universal across tasks and training setups. Contrary to prior work, we find that low effective rank does not cause late-stage performance degradation in small models, but instead co-occurs with it; we find adversarial cases where low-rank models do not exhibit saturation. Moreover, we show that effective rank is strongly influenced by pre-training hyperparameters, such as batch size and weight decay, which in-turn affect the model's performance. Lastly, extending our analysis to other geometric metrics and final-layer representation, we find that these metrics are largely aligned, but none can reliably predict downstream performance. Overall, our findings suggest that the model's geometry, as captured by existing metrics, primarily reflects training choices rather than performance.",
      "authors": [
        "Atharva Kulkarni",
        "Jacob Mitchell Springer",
        "Arjun Subramonian",
        "Swabha Swayamdipta"
      ],
      "url": "https://arxiv.org/abs/2602.20433",
      "published": "2026-02-24T00:31:04+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20427",
      "title": "GauS: Differentiable Scheduling Optimization via Gaussian Reparameterization",
      "abstract": "Efficient operator scheduling is a fundamental challenge in software compilation and hardware synthesis. While recent differentiable approaches have sought to replace traditional ones like exact solvers or heuristics with gradient-based search, they typically rely on categorical distributions that fail to capture the ordinal nature of time and suffer from a parameter space that scales poorly. In this paper, we propose a novel differentiable framework, GauS, that models operator scheduling as a stochastic relaxation using Gaussian distributions, which fully utilize modern parallel computing devices like GPUs. By representing schedules as continuous Gaussian variables, we successfully capture the ordinal nature of time and reduce the optimization space by orders of magnitude. Our method is highly flexible to represent various objectives and constraints, which provides the first differentiable formulation for the complex pipelined scheduling problem. We evaluate our method on a range of benchmarks, demonstrating that Gaus achieves Pareto-optimal results.",
      "authors": [
        "Yaohui Cai",
        "Vesal Bakhtazad",
        "Cunxi Yu",
        "Zhiru Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.20427",
      "published": "2026-02-23T23:58:32+00:00",
      "categories": [
        "cs.LG",
        "cs.AR"
      ]
    },
    {
      "id": "2602.20426",
      "title": "Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use",
      "abstract": "The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.",
      "authors": [
        "Ruocheng Guo",
        "Kaiwen Dong",
        "Xiang Gao",
        "Kamalika Das"
      ],
      "url": "https://arxiv.org/abs/2602.20426",
      "published": "2026-02-23T23:50:24+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20424",
      "title": "Implicit Intelligence -- Evaluating Agents on What Users Don't Say",
      "abstract": "Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.",
      "authors": [
        "Ved Sirdeshmukh",
        "Marc Wetter"
      ],
      "url": "https://arxiv.org/abs/2602.20424",
      "published": "2026-02-23T23:46:55+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20423",
      "title": "MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation",
      "abstract": "Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.",
      "authors": [
        "Taha Koleilat",
        "Hojat Asgariandehkordi",
        "Omid Nejati Manzari",
        "Berardino Barile",
        "Yiming Xiao",
        "Hassan Rivaz"
      ],
      "url": "https://arxiv.org/abs/2602.20423",
      "published": "2026-02-23T23:46:05+00:00",
      "categories": [
        "cs.CV",
        "cs.CL"
      ]
    },
    {
      "id": "2602.20422",
      "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning",
      "abstract": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.",
      "authors": [
        "Hanping Zhang",
        "Yuhong Guo"
      ],
      "url": "https://arxiv.org/abs/2602.20422",
      "published": "2026-02-23T23:41:22+00:00",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20419",
      "title": "CREDIT: Certified Ownership Verification of Deep Neural Networks Against Model Extraction Attacks",
      "abstract": "Machine Learning as a Service (MLaaS) has emerged as a widely adopted paradigm for providing access to deep neural network (DNN) models, enabling users to conveniently leverage these models through standardized APIs. However, such services are highly vulnerable to Model Extraction Attacks (MEAs), where an adversary repeatedly queries a target model to collect input-output pairs and uses them to train a surrogate model that closely replicates its functionality. While numerous defense strategies have been proposed, verifying the ownership of a suspicious model with strict theoretical guarantees remains a challenging task. To address this gap, we introduce CREDIT, a certified ownership verification against MEAs. Specifically, we employ mutual information to quantify the similarity between DNN models, propose a practical verification threshold, and provide rigorous theoretical guarantees for ownership verification based on this threshold. We extensively evaluate our approach on several mainstream datasets across different domains and tasks, achieving state-of-the-art performance. Our implementation is publicly available at: https://github.com/LabRAI/CREDIT.",
      "authors": [
        "Bolin Shen",
        "Zhan Cheng",
        "Neil Zhenqiang Gong",
        "Fan Yao",
        "Yushun Dong"
      ],
      "url": "https://arxiv.org/abs/2602.20419",
      "published": "2026-02-23T23:36:25+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20418",
      "title": "CITED: A Decision Boundary-Aware Signature for GNNs Towards Model Extraction Defense",
      "abstract": "Graph neural networks (GNNs) have demonstrated superior performance in various applications, such as recommendation systems and financial risk management. However, deploying large-scale GNN models locally is particularly challenging for users, as it requires significant computational resources and extensive property data. Consequently, Machine Learning as a Service (MLaaS) has become increasingly popular, offering a convenient way to deploy and access various models, including GNNs. However, an emerging threat known as Model Extraction Attacks (MEAs) presents significant risks, as adversaries can readily obtain surrogate GNN models exhibiting similar functionality. Specifically, attackers repeatedly query the target model using subgraph inputs to collect corresponding responses. These input-output pairs are subsequently utilized to train their own surrogate models at minimal cost. Many techniques have been proposed to defend against MEAs, but most are limited to specific output levels (e.g., embedding or label) and suffer from inherent technical drawbacks. To address these limitations, we propose a novel ownership verification framework CITED which is a first-of-its-kind method to achieve ownership verification on both embedding and label levels. Moreover, CITED is a novel signature-based method that neither harms downstream performance nor introduces auxiliary models that reduce efficiency, while still outperforming all watermarking and fingerprinting approaches. Extensive experiments demonstrate the effectiveness and robustness of our CITED framework. Code is available at: https://github.com/LabRAI/CITED.",
      "authors": [
        "Bolin Shen",
        "Md Shamim Seraj",
        "Zhan Cheng",
        "Shayok Chakraborty",
        "Yushun Dong"
      ],
      "url": "https://arxiv.org/abs/2602.20418",
      "published": "2026-02-23T23:33:31+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20408",
      "title": "Examining and Addressing Barriers to Diversity in LLM-Generated Ideas",
      "abstract": "Ideas generated by independent samples of humans tend to be more diverse than ideas generated from independent LLM samples, raising concerns that widespread reliance on LLMs could homogenize ideation and undermine innovation at a societal level. Drawing on cognitive psychology, we identify (both theoretically and empirically) two mechanisms undermining LLM idea diversity. First, at the individual level, LLMs exhibit fixation just as humans do, where early outputs constrain subsequent ideation. Second, at the collective level, LLMs aggregate knowledge into a unified distribution rather than exhibiting the knowledge partitioning inherent to human populations, where each person occupies a distinct region of the knowledge space. Through four studies, we demonstrate that targeted prompting interventions can address each mechanism independently: Chain-of-Thought (CoT) prompting reduces fixation by encouraging structured reasoning (only in LLMs, not humans), while ordinary personas (versus \"creative entrepreneurs\" such as Steve Jobs) improve knowledge partitioning by serving as diverse sampling cues, anchoring generation in distinct regions of the semantic space. Combining both approaches produces the highest idea diversity, outperforming humans. These findings offer a theoretically grounded framework for understanding LLM idea diversity and practical strategies for human-AI collaborations that leverage AI's efficiency without compromising the diversity essential to a healthy innovation ecosystem.",
      "authors": [
        "Yuting Deng",
        "Melanie Brucks",
        "Olivier Toubia"
      ],
      "url": "https://arxiv.org/abs/2602.20408",
      "published": "2026-02-23T23:10:47+00:00",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "id": "2602.20404",
      "title": "$κ$-Explorer: A Unified Framework for Active Model Estimation in MDPs",
      "abstract": "In tabular Markov decision processes (MDPs) with perfect state observability, each trajectory provides active samples from the transition distributions conditioned on state-action pairs. Consequently, accurate model estimation depends on how the exploration policy allocates visitation frequencies in accordance with the intrinsic complexity of each transition distribution. Building on recent work on coverage-based exploration, we introduce a parameterized family of decomposable and concave objective functions $U_κ$ that explicitly incorporate both intrinsic estimation complexity and extrinsic visitation frequency. Moreover, the curvature $κ$ provides a unified treatment of various global objectives, such as the average-case and worst-case estimation error objectives. Using the closed-form characterization of the gradient of $U_κ$, we propose $κ$-Explorer, an active exploration algorithm that performs Frank-Wolfe-style optimization over state-action occupancy measures. The diminishing-returns structure of $U_κ$ naturally prioritizes underexplored and high-variance transitions, while preserving smoothness properties that enable efficient optimization. We establish tight regret guarantees for $κ$-Explorer and further introduce a fully online and computationally efficient surrogate algorithm for practical use. Experiments on benchmark MDPs demonstrate that $κ$-Explorer provides superior performance compared to existing exploration strategies.",
      "authors": [
        "Xihe Gu",
        "Urbashi Mitra",
        "Tara Javidi"
      ],
      "url": "https://arxiv.org/abs/2602.20404",
      "published": "2026-02-23T22:56:32+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20403",
      "title": "Wasserstein Distributionally Robust Online Learning",
      "abstract": "We study distributionally robust online learning, where a risk-averse learner updates decisions sequentially to guard against worst-case distributions drawn from a Wasserstein ambiguity set centered at past observations. While this paradigm is well understood in the offline setting through Wasserstein Distributionally Robust Optimization (DRO), its online extension poses significant challenges in both convergence and computation. In this paper, we address these challenges. First, we formulate the problem as an online saddle-point stochastic game between a decision maker and an adversary selecting worst-case distributions, and propose a general framework that converges to a robust Nash equilibrium coinciding with the solution of the corresponding offline Wasserstein DRO problem. Second, we address the main computational bottleneck, which is the repeated solution of worst-case expectation problems. For the important class of piecewise concave loss functions, we propose a tailored algorithm that exploits problem geometry to achieve substantial speedups over state-of-the-art solvers such as Gurobi. The key insight is a novel connection between the worst-case expectation problem, an inherently infinite-dimensional optimization problem, and a classical and tractable budget allocation problem, which is of independent interest.",
      "authors": [
        "Guixian Chen",
        "Salar Fattahi",
        "Soroosh Shafiee"
      ],
      "url": "https://arxiv.org/abs/2602.20403",
      "published": "2026-02-23T22:55:07+00:00",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20400",
      "title": "Three Concrete Challenges and Two Hopes for the Safety of Unsupervised Elicitation",
      "abstract": "To steer language models towards truthful outputs on tasks which are beyond human capability, previous work has suggested training models on easy tasks to steer them on harder ones (easy-to-hard generalization), or using unsupervised training algorithms to steer models with no external labels at all (unsupervised elicitation). Although techniques from both paradigms have been shown to improve model accuracy on a wide variety of tasks, we argue that the datasets used for these evaluations could cause overoptimistic evaluation results. Unlike many real-world datasets, they often (1) have no features with more salience than truthfulness, (2) have balanced training sets, and (3) contain only data points to which the model can give a well-defined answer. We construct datasets that lack each of these properties to stress-test a range of standard unsupervised elicitation and easy-to-hard generalization techniques. We find that no technique reliably performs well on any of these challenges. We also study ensembling and combining easy-to-hard and unsupervised techniques, and find they only partially mitigate performance degradation due to these challenges. We believe that overcoming these challenges should be a priority for future work on unsupervised elicitation.",
      "authors": [
        "Callum Canavan",
        "Aditya Shrivastava",
        "Allison Qi",
        "Jonathan Michala",
        "Fabien Roger"
      ],
      "url": "https://arxiv.org/abs/2602.20400",
      "published": "2026-02-23T22:39:40+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20399",
      "title": "GeoPT: Scaling Physics Simulation via Lifted Geometric Pre-Training",
      "abstract": "Neural simulators promise efficient surrogates for physics simulation, but scaling them is bottlenecked by the prohibitive cost of generating high-fidelity training data. Pre-training on abundant off-the-shelf geometries offers a natural alternative, yet faces a fundamental gap: supervision on static geometry alone ignores dynamics and can lead to negative transfer on physics tasks. We present GeoPT, a unified pre-trained model for general physics simulation based on lifted geometric pre-training. The core idea is to augment geometry with synthetic dynamics, enabling dynamics-aware self-supervision without physics labels. Pre-trained on over one million samples, GeoPT consistently improves industrial-fidelity benchmarks spanning fluid mechanics for cars, aircraft, and ships, and solid mechanics in crash simulation, reducing labeled data requirements by 20-60% and accelerating convergence by 2$\\times$. These results show that lifting with synthetic dynamics bridges the geometry-physics gap, unlocking a scalable path for neural simulation and potentially beyond. Code is available at https://github.com/Physics-Scaling/GeoPT.",
      "authors": [
        "Haixu Wu",
        "Minghao Guo",
        "Zongyi Li",
        "Zhiyang Dou",
        "Mingsheng Long",
        "Kaiming He",
        "Wojciech Matusik"
      ],
      "url": "https://arxiv.org/abs/2602.20399",
      "published": "2026-02-23T22:32:08+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20396",
      "title": "cc-Shapley: Measuring Multivariate Feature Importance Needs Causal Context",
      "abstract": "Explainable artificial intelligence promises to yield insights into relevant features, thereby enabling humans to examine and scrutinize machine learning models or even facilitating scientific discovery. Considering the widespread technique of Shapley values, we find that purely data-driven operationalization of multivariate feature importance is unsuitable for such purposes. Even for simple problems with two features, spurious associations due to collider bias and suppression arise from considering one feature only in the observational context of the other, which can lead to misinterpretations. Causal knowledge about the data-generating process is required to identify and correct such misleading feature attributions. We propose cc-Shapley (causal context Shapley), an interventional modification of conventional observational Shapley values leveraging knowledge of the data's causal structure, thereby analyzing the relevance of a feature in the causal context of the remaining features. We show theoretically that this eradicates spurious association induced by collider bias. We compare the behavior of Shapley and cc-Shapley values on various, synthetic, and real-world datasets. We observe nullification or reversal of associations compared to univariate feature importance when moving from observational to cc-Shapley.",
      "authors": [
        "Jörg Martin",
        "Stefan Haufe"
      ],
      "url": "https://arxiv.org/abs/2602.20396",
      "published": "2026-02-23T22:21:29+00:00",
      "categories": [
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "id": "2602.20394",
      "title": "Selecting Optimal Variable Order in Autoregressive Ising Models",
      "abstract": "Autoregressive models enable tractable sampling from learned probability distributions, but their performance critically depends on the variable ordering used in the factorization via complexities of the resulting conditional distributions. We propose to learn the Markov random field describing the underlying data, and use the inferred graphical model structure to construct optimized variable orderings. We illustrate our approach on two-dimensional image-like models where a structure-aware ordering leads to restricted conditioning sets, thereby reducing model complexity. Numerical experiments on Ising models with discrete data demonstrate that graph-informed orderings yield higher-fidelity generated samples compared to naive variable orderings.",
      "authors": [
        "Shiba Biswal",
        "Marc Vuffray",
        "Andrey Y. Lokhov"
      ],
      "url": "https://arxiv.org/abs/2602.20394",
      "published": "2026-02-23T22:15:10+00:00",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20383",
      "title": "Detecting and Mitigating Group Bias in Heterogeneous Treatment Effects",
      "abstract": "Heterogeneous treatment effects (HTEs) are increasingly estimated using machine learning models that produce highly personalized predictions of treatment effects. In practice, however, predicted treatment effects are rarely interpreted, reported, or audited at the individual level but, instead, are often aggregated to broader subgroups, such as demographic segments, risk strata, or markets. We show that such aggregation can induce systematic bias of the group-level causal effect: even when models for predicting the individual-level conditional average treatment effect (CATE) are correctly specified and trained on data from randomized experiments, aggregating the predicted CATEs up to the group level does not, in general, recover the corresponding group average treatment effect (GATE). We develop a unified statistical framework to detect and mitigate this form of group bias in randomized experiments. We first define group bias as the discrepancy between the model-implied and experimentally identified GATEs, derive an asymptotically normal estimator, and then provide a simple-to-implement statistical test. For mitigation, we propose a shrinkage-based bias-correction, and show that the theoretically optimal and empirically feasible solutions have closed-form expressions. The framework is fully general, imposes minimal assumptions, and only requires computing sample moments. We analyze the economic implications of mitigating detected group bias for profit-maximizing personalized targeting, thereby characterizing when bias correction alters targeting decisions and profits, and the trade-offs involved. Applications to large-scale experimental data at major digital platforms validate our theoretical results and demonstrate empirical performance.",
      "authors": [
        "Joel Persson",
        "Jurriën Bakker",
        "Dennis Bohle",
        "Stefan Feuerriegel",
        "Florian von Wangenheim"
      ],
      "url": "https://arxiv.org/abs/2602.20383",
      "published": "2026-02-23T21:47:01+00:00",
      "categories": [
        "stat.ME",
        "cs.LG",
        "econ.EM"
      ]
    },
    {
      "id": "2602.20379",
      "title": "Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems",
      "abstract": "Enterprise Retrieval-Augmented Generation (RAG) assistants operate in multi-turn, case-based workflows such as technical support and IT operations, where evaluation must reflect operational constraints, structured identifiers (e.g., error codes, versions), and resolution workflows. Existing RAG evaluation frameworks are primarily designed for benchmark-style or single-turn settings and often fail to capture enterprise-specific failure modes such as case misidentification, workflow misalignment, and partial resolution across turns.   We present a case-aware LLM-as-a-Judge evaluation framework for enterprise multi-turn RAG systems. The framework evaluates each turn using eight operationally grounded metrics that separate retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. A severity-aware scoring protocol reduces score inflation and improves diagnostic clarity across heterogeneous enterprise cases. The system uses deterministic prompting with strict JSON outputs, enabling scalable batch evaluation, regression testing, and production monitoring.   Through a comparative study of two instruction-tuned models across short and long workflows, we show that generic proxy metrics provide ambiguous signals, while the proposed framework exposes enterprise-critical tradeoffs that are actionable for system improvement.",
      "authors": [
        "Mukul Chhabra",
        "Luigi Medrano",
        "Arush Verma"
      ],
      "url": "https://arxiv.org/abs/2602.20379",
      "published": "2026-02-23T21:37:06+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20376",
      "title": "Exploiting Low-Rank Structure in Max-K-Cut Problems",
      "abstract": "We approach the Max-3-Cut problem through the lens of maximizing complex-valued quadratic forms and demonstrate that low-rank structure in the objective matrix can be exploited, leading to alternative algorithms to classical semidefinite programming (SDP) relaxations and heuristic techniques. We propose an algorithm for maximizing these quadratic forms over a domain of size $K$ that enumerates and evaluates a set of $O\\left(n^{2r-1}\\right)$ candidate solutions, where $n$ is the dimension of the matrix and $r$ represents the rank of an approximation of the objective. We prove that this candidate set is guaranteed to include the exact maximizer when $K=3$ (corresponding to Max-3-Cut) and the objective is low-rank, and provide approximation guarantees when the objective is a perturbation of a low-rank matrix. This construction results in a family of novel, inherently parallelizable and theoretically-motivated algorithms for Max-3-Cut. Extensive experimental results demonstrate that our approach achieves performance comparable to existing algorithms across a wide range of graphs, while being highly scalable.",
      "authors": [
        "Ria Stevens",
        "Fangshuo Liao",
        "Barbara Su",
        "Jianqiang Li",
        "Anastasios Kyrillidis"
      ],
      "url": "https://arxiv.org/abs/2602.20376",
      "published": "2026-02-23T21:29:47+00:00",
      "categories": [
        "cs.DS",
        "cs.LG",
        "math.OC",
        "quant-ph"
      ]
    },
    {
      "id": "2602.20372",
      "title": "How communicatively optimal are exact numeral systems? Once more on lexicon size and morphosyntactic complexity",
      "abstract": "Recent research argues that exact recursive numeral systems optimize communicative efficiency by balancing a tradeoff between the size of the numeral lexicon and the average morphosyntactic complexity (roughly length in morphemes) of numeral terms. We argue that previous studies have not characterized the data in a fashion that accounts for the degree of complexity languages display. Using data from 52 genetically diverse languages and an annotation scheme distinguishing between predictable and unpredictable allomorphy (formal variation), we show that many of the world's languages are decisively less efficient than one would expect. We discuss the implications of our findings for the study of numeral systems and linguistic evolution more generally.",
      "authors": [
        "Chundra Cathcart",
        "Arne Rubehn",
        "Katja Bocklage",
        "Luca Ciucci",
        "Kellen Parker van Dam",
        "Alžběta Kučerová",
        "Jekaterina Mažara",
        "Carlo Y. Meloni",
        "David Snee",
        "Johann-Mattis List"
      ],
      "url": "https://arxiv.org/abs/2602.20372",
      "published": "2026-02-23T21:19:07+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20370",
      "title": "Quantitative Approximation Rates for Group Equivariant Learning",
      "abstract": "The universal approximation theorem establishes that neural networks can approximate any continuous function on a compact set. Later works in approximation theory provide quantitative approximation rates for ReLU networks on the class of $α$-Hölder functions $f: [0,1]^N \\to \\mathbb{R}$. The goal of this paper is to provide similar quantitative approximation results in the context of group equivariant learning, where the learned $α$-Hölder function is known to obey certain group symmetries. While there has been much interest in the literature in understanding the universal approximation properties of equivariant models, very few quantitative approximation results are known for equivariant models.   In this paper, we bridge this gap by deriving quantitative approximation rates for several prominent group-equivariant and invariant architectures. The architectures that we consider include: the permutation-invariant Deep Sets architecture; the permutation-equivariant Sumformer and Transformer architectures; joint invariance to permutations and rigid motions using invariant networks based on frame averaging; and general bi-Lipschitz invariant models. Overall, we show that equally-sized ReLU MLPs and equivariant architectures are equally expressive over equivariant functions. Thus, hard-coding equivariance does not result in a loss of expressivity or approximation power in these models.",
      "authors": [
        "Jonathan W. Siegel",
        "Snir Hordan",
        "Hannah Lawrence",
        "Ali Syed",
        "Nadav Dym"
      ],
      "url": "https://arxiv.org/abs/2602.20370",
      "published": "2026-02-23T21:17:46+00:00",
      "categories": [
        "cs.LG",
        "math.NA"
      ]
    },
    {
      "id": "2602.20361",
      "title": "Learning During Detection: Continual Learning for Neural OFDM Receivers via DMRS",
      "abstract": "Deep neural networks (DNNs) have been increasingly explored for receiver design because they can handle complex environments without relying on explicit channel models. Nevertheless, because communication channels change rapidly, their distributions can shift over time, often making periodic retraining necessary. This paper proposes a zero-overhead online and continual learning framework for orthogonal frequency-division multiplexing (OFDM) neural receivers that directly detect the soft bits of received signals. Unlike conventional fine-tuning methods that rely on dedicated training intervals or full resource grids, our approach leverages existing demodulation reference signals (DMRS) to simultaneously enable signal demodulation and model adaptation. We introduce three pilot designs: fully randomized, hybrid, and additional pilots that flexibly support joint demodulation and learning. To accommodate these pilot designs, we develop two receiver architectures: (i) a parallel design that separates inference and fine-tuning for uninterrupted operation, and (ii) a forward-pass reusing design that reduces computational complexity. Simulation results show that the proposed method effectively tracks both slow and fast channel distribution variations without additional overhead, service interruption, or catastrophic performance degradation under distribution shift.",
      "authors": [
        "Mohanad Obeed",
        "Ming Jian"
      ],
      "url": "https://arxiv.org/abs/2602.20361",
      "published": "2026-02-23T21:07:36+00:00",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20360",
      "title": "Momentum Guidance: Plug-and-Play Guidance for Flow Models",
      "abstract": "Flow-based generative models have become a strong framework for high-quality generative modeling, yet pretrained models are rarely used in their vanilla conditional form: conditional samples without guidance often appear diffuse and lack fine-grained detail due to the smoothing effects of neural networks. Existing guidance techniques such as classifier-free guidance (CFG) improve fidelity but double the inference cost and typically reduce sample diversity. We introduce Momentum Guidance (MG), a new dimension of guidance that leverages the ODE trajectory itself. MG extrapolates the current velocity using an exponential moving average of past velocities and preserves the standard one-evaluation-per-step cost. It matches the effect of standard guidance without extra computation and can further improve quality when combined with CFG. Experiments demonstrate MG's effectiveness across benchmarks. Specifically, on ImageNet-256, MG achieves average improvements in FID of 36.68% without CFG and 25.52% with CFG across various sampling settings, attaining an FID of 1.597 at 64 sampling steps. Evaluations on large flow-based models like Stable Diffusion 3 and FLUX.1-dev further confirm consistent quality enhancements across standard metrics.",
      "authors": [
        "Runlong Liao",
        "Jian Yu",
        "Baiyu Su",
        "Chi Zhang",
        "Lizhang Chen",
        "Qiang Liu"
      ],
      "url": "https://arxiv.org/abs/2602.20360",
      "published": "2026-02-23T21:06:35+00:00",
      "categories": [
        "cs.LG",
        "cs.CV"
      ]
    },
    {
      "id": "2602.20344",
      "title": "Hierarchical Molecular Representation Learning via Fragment-Based Self-Supervised Embedding Prediction",
      "abstract": "Graph self-supervised learning (GSSL) has demonstrated strong potential for generating expressive graph embeddings without the need for human annotations, making it particularly valuable in domains with high labeling costs such as molecular graph analysis. However, existing GSSL methods mostly focus on node- or edge-level information, often ignoring chemically relevant substructures which strongly influence molecular properties. In this work, we propose Graph Semantic Predictive Network (GraSPNet), a hierarchical self-supervised framework that explicitly models both atomic-level and fragment-level semantics. GraSPNet decomposes molecular graphs into chemically meaningful fragments without predefined vocabularies and learns node- and fragment-level representations through multi-level message passing with masked semantic prediction at both levels. This hierarchical semantic supervision enables GraSPNet to learn multi-resolution structural information that is both expressive and transferable. Extensive experiments on multiple molecular property prediction benchmarks demonstrate that GraSPNet learns chemically meaningful representations and consistently outperforms state-of-the-art GSSL methods in transfer learning settings.",
      "authors": [
        "Jiele Wu",
        "Haozhe Ma",
        "Zhihan Guo",
        "Thanh Vinh Vo",
        "Tze Yun Leong"
      ],
      "url": "https://arxiv.org/abs/2602.20344",
      "published": "2026-02-23T20:41:44+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ]
    },
    {
      "id": "2602.20338",
      "title": "Emergent Manifold Separability during Reasoning in Large Language Models",
      "abstract": "Chain-of-Thought (CoT) prompting significantly improves reasoning in Large Language Models, yet the temporal dynamics of the underlying representation geometry remain poorly understood. We investigate these dynamics by applying Manifold Capacity Theory (MCT) to a compositional Boolean logic task, allowing us to quantify the linear separability of latent representations without the confounding factors of probe training. Our analysis reveals that reasoning manifests as a transient geometric pulse, where concept manifolds are untangled into linearly separable subspaces immediately prior to computation and rapidly compressed thereafter. This behavior diverges from standard linear probe accuracy, which remains high long after computation, suggesting a fundamental distinction between information that is merely retrievable and information that is geometrically prepared for processing. We interpret this phenomenon as \\emph{Dynamic Manifold Management}, a mechanism where the model dynamically modulates representational capacity to optimize the bandwidth of the residual stream throughout the reasoning chain.",
      "authors": [
        "Alexandre Polo",
        "Chanwoo Chun",
        "SueYeon Chung"
      ],
      "url": "https://arxiv.org/abs/2602.20338",
      "published": "2026-02-23T20:36:17+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20336",
      "title": "Natural Language Processing Models for Robust Document Categorization",
      "abstract": "This article presents an evaluation of several machine learning methods applied to automated text classification, alongside the design of a demonstrative system for unbalanced document categorization and distribution. The study focuses on balancing classification accuracy with computational efficiency, a key consideration when integrating AI into real world automation pipelines. Three models of varying complexity were examined: a Naive Bayes classifier, a bidirectional LSTM network, and a fine tuned transformer based BERT model.   The experiments reveal substantial differences in performance. BERT achieved the highest accuracy, consistently exceeding 99\\%, but required significantly longer training times and greater computational resources. The BiLSTM model provided a strong compromise, reaching approximately 98.56\\% accuracy while maintaining moderate training costs and offering robust contextual understanding. Naive Bayes proved to be the fastest to train, on the order of milliseconds, yet delivered the lowest accuracy, averaging around 94.5\\%. Class imbalance influenced all methods, particularly in the recognition of minority categories.   A fully functional demonstrative system was implemented to validate practical applicability, enabling automated routing of technical requests with throughput unattainable through manual processing. The study concludes that BiLSTM offers the most balanced solution for the examined scenario, while also outlining opportunities for future improvements and further exploration of transformer architectures.",
      "authors": [
        "Radoslaw Roszczyk",
        "Pawel Tecza",
        "Maciej Stodolski",
        "Krzysztof Siwek"
      ],
      "url": "https://arxiv.org/abs/2602.20336",
      "published": "2026-02-23T20:33:22+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20333",
      "title": "DMCD: Semantic-Statistical Framework for Causal Discovery",
      "abstract": "We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.   We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.",
      "authors": [
        "Samarth KaPatel",
        "Sofia Nikiforova",
        "Giacinto Paolo Saggese",
        "Paul Smith"
      ],
      "url": "https://arxiv.org/abs/2602.20333",
      "published": "2026-02-23T20:29:35+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20332",
      "title": "No One Size Fits All: QueryBandits for Hallucination Mitigation",
      "abstract": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.",
      "authors": [
        "Nicole Cho",
        "William Watson",
        "Alec Koppel",
        "Sumitra Ganesh",
        "Manuela Veloso"
      ],
      "url": "https://arxiv.org/abs/2602.20332",
      "published": "2026-02-23T20:28:48+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20330",
      "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking",
      "abstract": "Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.",
      "authors": [
        "Jingcheng Yang",
        "Tianhu Xiong",
        "Shengyi Qian",
        "Klara Nahrstedt",
        "Mingyuan Wu"
      ],
      "url": "https://arxiv.org/abs/2602.20330",
      "published": "2026-02-23T20:26:45+00:00",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20329",
      "title": "CaDrift: A Time-dependent Causal Generator of Drifting Data Streams",
      "abstract": "This work presents Causal Drift Generator (CaDrift), a time-dependent synthetic data generator framework based on Structural Causal Models (SCMs). The framework produces a virtually infinite combination of data streams with controlled shift events and time-dependent data, making it a tool to evaluate methods under evolving data. CaDrift synthesizes various distributional and covariate shifts by drifting mapping functions of the SCM, which change underlying cause-and-effect relationships between features and the target. In addition, CaDrift models occasional perturbations by leveraging interventions in causal modeling. Experimental results show that, after distributional shift events, the accuracy of classifiers tends to drop, followed by a gradual retrieval, confirming the generator's effectiveness in simulating shifts. The framework has been made available on GitHub.",
      "authors": [
        "Eduardo V. L. Barboza",
        "Jean Paul Barddal",
        "Robert Sabourin",
        "Rafael M. O. Cruz"
      ],
      "url": "https://arxiv.org/abs/2602.20329",
      "published": "2026-02-23T20:25:43+00:00",
      "categories": [
        "cs.LG",
        "cs.DB"
      ]
    },
    {
      "id": "2602.20324",
      "title": "An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models",
      "abstract": "Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.",
      "authors": [
        "Cathy Shyr",
        "Yan Hu",
        "Rory J. Tinker",
        "Thomas A. Cassini",
        "Kevin W. Byram",
        "Rizwan Hamid",
        "Daniel V. Fabbri",
        "Adam Wright",
        "Josh F. Peterson",
        "Lisa Bastarache",
        "Hua Xu"
      ],
      "url": "https://arxiv.org/abs/2602.20324",
      "published": "2026-02-23T20:20:23+00:00",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20323",
      "title": "Learning Physical Principles from Interaction: Self-Evolving Planning via Test-Time Memory",
      "abstract": "Reliable object manipulation requires understanding physical properties that vary across objects and environments. Vision-language model (VLM) planners can reason about friction and stability in general terms; however, they often cannot predict how a specific ball will roll on a particular surface or which stone will provide a stable foundation without direct experience. We present PhysMem, a memory framework that enables VLM robot planners to learn physical principles from interaction at test time, without updating model parameters. The system records experiences, generates candidate hypotheses, and verifies them through targeted interaction before promoting validated knowledge to guide future decisions. A central design choice is verification before application: the system tests hypotheses against new observations rather than applying retrieved experience directly, reducing rigid reliance on prior experience when physical conditions change. We evaluate PhysMem on three real-world manipulation tasks and simulation benchmarks across four VLM backbones. On a controlled brick insertion task, principled abstraction achieves 76% success compared to 23% for direct experience retrieval, and real-world experiments show consistent improvement over 30-minute deployment sessions.",
      "authors": [
        "Haoyang Li",
        "Yang You",
        "Hao Su",
        "Leonidas Guibas"
      ],
      "url": "https://arxiv.org/abs/2602.20323",
      "published": "2026-02-23T20:18:35+00:00",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20317",
      "title": "Fast Spectrogram Event Extraction via Offline Self-Supervised Learning: From Fusion Diagnostics to Bioacoustics",
      "abstract": "Next-generation fusion facilities like ITER face a \"data deluge,\" generating petabytes of multi-diagnostic signals daily that challenge manual analysis. We present a \"signals-first\" self-supervised framework for the automated extraction of coherent and transient modes from high-noise time-frequency data. We also develop a general-purpose method and tool for extracting coherent, quasi-coherent, and transient modes for fluctuation measurements in tokamaks by employing non-linear optimal techniques in multichannel signal processing with a fast neural network surrogate on fast magnetics, electron cyclotron emission, CO2 interferometers, and beam emission spectroscopy measurements from DIII-D. Results are tested on data from DIII-D, TJ-II, and non-fusion spectrograms. With an inference latency of 0.5 seconds, this framework enables real-time mode identification and large-scale automated database generation for advanced plasma control. Repository is in https://github.com/PlasmaControl/TokEye.",
      "authors": [
        "Nathaniel Chen",
        "Kouroche Bouchiat",
        "Peter Steiner",
        "Andrew Rothstein",
        "David Smith",
        "Max Austin",
        "Mike van Zeeland",
        "Azarakhsh Jalalvand",
        "Egemen Kolemen"
      ],
      "url": "https://arxiv.org/abs/2602.20317",
      "published": "2026-02-23T20:03:10+00:00",
      "categories": [
        "eess.SP",
        "cs.AI",
        "physics.plasm-ph"
      ]
    },
    {
      "id": "2602.20309",
      "title": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models",
      "abstract": "Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.",
      "authors": [
        "Jingxuan Zhang",
        "Yunta Hsieh",
        "Zhongwei Wan",
        "Haokun Lin",
        "Xin Wang",
        "Ziqi Wang",
        "Yingtie Lei",
        "Mi Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.20309",
      "published": "2026-02-23T19:55:54+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20307",
      "title": "In-context Pre-trained Time-Series Foundation Models adapt to Unseen Tasks",
      "abstract": "Time-series foundation models (TSFMs) have demonstrated strong generalization capabilities across diverse datasets and tasks. However, existing foundation models are typically pre-trained to enhance performance on specific tasks and often struggle to generalize to unseen tasks without fine-tuning. To address this limitation, we propose augmenting TSFMs with In-Context Learning (ICL) capabilities, enabling them to perform test-time inference by dynamically adapting to input-output relationships provided within the context. Our framework, In-Context Time-series Pre-training (ICTP), restructures the original pre-training data to equip the backbone TSFM with ICL capabilities, enabling adaptation to unseen tasks. Experiments demonstrate that ICT improves the performance of state-of-the-art TSFMs by approximately 11.4% on unseen tasks without requiring fine-tuning.",
      "authors": [
        "Shangqing Xu",
        "Harshavardhan Kamarthi",
        "Haoxin Liu",
        "B. Aditya Prakash"
      ],
      "url": "https://arxiv.org/abs/2602.20307",
      "published": "2026-02-23T19:48:47+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20306",
      "title": "Shape-informed cardiac mechanics surrogates in data-scarce regimes via geometric encoding and generative augmentation",
      "abstract": "High-fidelity computational models of cardiac mechanics provide mechanistic insight into the heart function but are computationally prohibitive for routine clinical use. Surrogate models can accelerate simulations, but generalization across diverse anatomies is challenging, particularly in data-scarce settings. We propose a two-step framework that decouples geometric representation from learning the physics response, to enable shape-informed surrogate modeling under data-scarce conditions. First, a shape model learns a compact latent representation of left ventricular geometries. The learned latent space effectively encodes anatomies and enables synthetic geometries generation for data augmentation. Second, a neural field-based surrogate model, conditioned on this geometric encoding, is trained to predict ventricular displacement under external loading. The proposed architecture performs positional encoding by using universal ventricular coordinates, which improves generalization across diverse anatomies. Geometric variability is encoded using two alternative strategies, which are systematically compared: a PCA-based approach suitable for working with point cloud representations of geometries, and a DeepSDF-based implicit neural representation learned directly from point clouds. Overall, our results, obtained on idealized and patient-specific datasets, show that the proposed approaches allow for accurate predictions and generalization to unseen geometries, and robustness to noisy or sparsely sampled inputs.",
      "authors": [
        "Davide Carrara",
        "Marc Hirschvogel",
        "Francesca Bonizzoni",
        "Stefano Pagani",
        "Simone Pezzuto",
        "Francesco Regazzoni"
      ],
      "url": "https://arxiv.org/abs/2602.20306",
      "published": "2026-02-23T19:40:55+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ]
    },
    {
      "id": "2602.20303",
      "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health",
      "abstract": "Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.",
      "authors": [
        "Joyanta Jyoti Mondal"
      ],
      "url": "https://arxiv.org/abs/2602.20303",
      "published": "2026-02-23T19:31:44+00:00",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20300",
      "title": "What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance",
      "abstract": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.",
      "authors": [
        "William Watson",
        "Nicole Cho",
        "Sumitra Ganesh",
        "Manuela Veloso"
      ],
      "url": "https://arxiv.org/abs/2602.20300",
      "published": "2026-02-23T19:30:08+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20297",
      "title": "Gap-Dependent Bounds for Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation",
      "abstract": "We study gap-dependent performance guarantees for nearly minimax-optimal algorithms in reinforcement learning with linear function approximation. While prior works have established gap-dependent regret bounds in this setting, existing analyses do not apply to algorithms that achieve the nearly minimax-optimal worst-case regret bound $\\tilde{O}(d\\sqrt{H^3K})$, where $d$ is the feature dimension, $H$ is the horizon length, and $K$ is the number of episodes. We bridge this gap by providing the first gap-dependent regret bound for the nearly minimax-optimal algorithm LSVI-UCB++ (He et al., 2023). Our analysis yields improved dependencies on both $d$ and $H$ compared to previous gap-dependent results. Moreover, leveraging the low policy-switching property of LSVI-UCB++, we introduce a concurrent variant that enables efficient parallel exploration across multiple agents and establish the first gap-dependent sample complexity upper bound for online multi-agent RL with linear function approximation, achieving linear speedup with respect to the number of agents.",
      "authors": [
        "Haochen Zhang",
        "Zhong Zheng",
        "Lingzhou Xue"
      ],
      "url": "https://arxiv.org/abs/2602.20297",
      "published": "2026-02-23T19:25:46+00:00",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20296",
      "title": "Learning to Solve Complex Problems via Dataset Decomposition",
      "abstract": "Curriculum learning is a class of training strategies that organizes the data being exposed to a model by difficulty, gradually from simpler to more complex examples. This research explores a reverse curriculum generation approach that recursively decomposes complex datasets into simpler, more learnable components. We propose a teacher-student framework where the teacher is equipped with the ability to reason step-by-step, which is used to recursively generate easier versions of examples, enabling the student model to progressively master difficult tasks. We propose a novel scoring system to measure data difficulty based on its structural complexity and conceptual depth, allowing curriculum construction over decomposed data. Experiments on math datasets (MATH and AIME) and code generation datasets demonstrate that models trained with curricula generated by our approach exhibit superior performance compared to standard training on original datasets.",
      "authors": [
        "Wanru Zhao",
        "Lucas Caccia",
        "Zhengyan Shi",
        "Minseon Kim",
        "Weijia Xu",
        "Alessandro Sordoni"
      ],
      "url": "https://arxiv.org/abs/2602.20296",
      "published": "2026-02-23T19:25:40+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20294",
      "title": "InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation",
      "abstract": "Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.",
      "authors": [
        "Yu Li",
        "Pranav Narayanan Venkit",
        "Yada Pruksachatkun",
        "Chien-Sheng Wu"
      ],
      "url": "https://arxiv.org/abs/2602.20294",
      "published": "2026-02-23T19:21:10+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "id": "2602.20293",
      "title": "Discrete Diffusion with Sample-Efficient Estimators for Conditionals",
      "abstract": "We study a discrete denoising diffusion framework that integrates a sample-efficient estimator of single-site conditionals with round-robin noising and denoising dynamics for generative modeling over discrete state spaces. Rather than approximating a discrete analog of a score function, our formulation treats single-site conditional probabilities as the fundamental objects that parameterize the reverse diffusion process. We employ a sample-efficient method known as Neural Interaction Screening Estimator (NeurISE) to estimate these conditionals in the diffusion dynamics. Controlled experiments on synthetic Ising models, MNIST, and scientific data sets produced by a D-Wave quantum annealer, synthetic Potts model and one-dimensional quantum systems demonstrate the proposed approach. On the binary data sets, these experiments demonstrate that the proposed approach outperforms popular existing methods including ratio-based approaches, achieving improved performance in total variation, cross-correlations, and kernel density estimation metrics.",
      "authors": [
        "Karthik Elamvazhuthi",
        "Abhijith Jayakumar",
        "Andrey Y. Lokhov"
      ],
      "url": "https://arxiv.org/abs/2602.20293",
      "published": "2026-02-23T19:20:37+00:00",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20292",
      "title": "Quantifying the Expectation-Realisation Gap for Agentic AI Systems",
      "abstract": "Agentic AI systems are deployed with expectations of substantial productivity gains, yet rigorous empirical evidence reveals systematic discrepancies between pre-deployment expectations and post-deployment outcomes. We review controlled trials and independent validations across software engineering, clinical documentation, and clinical decision support to quantify this expectation-realisation gap. In software development, experienced developers expected a 24% speedup from AI tools but were slowed by 19% -- a 43 percentage-point calibration error. In clinical documentation, vendor claims of multi-minute time savings contrast with measured reductions of less than one minute per note, and one widely deployed tool showed no statistically significant effect. In clinical decision support, externally validated performance falls substantially below developer-reported metrics. These shortfalls are driven by workflow integration friction, verification burden, measurement construct mismatches, and systematic variation in who benefits and who does not. The evidence motivates structured planning frameworks that require explicit, quantified benefit expectations with human oversight costs factored in.",
      "authors": [
        "Sebastian Lobentanzer"
      ],
      "url": "https://arxiv.org/abs/2602.20292",
      "published": "2026-02-23T19:16:30+00:00",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20289",
      "title": "The Sim-to-Real Gap in MRS Quantification: A Systematic Deep Learning Validation for GABA",
      "abstract": "Magnetic resonance spectroscopy (MRS) is used to quantify metabolites in vivo and estimate biomarkers for conditions ranging from neurological disorders to cancers. Quantifying low-concentration metabolites such as GABA ($γ$-aminobutyric acid) is challenging due to low signal-to-noise ratio (SNR) and spectral overlap. We investigate and validate deep learning for quantifying complex, low-SNR, overlapping signals from MEGA-PRESS spectra, devise a convolutional neural network (CNN) and a Y-shaped autoencoder (YAE), and select the best models via Bayesian optimisation on 10,000 simulated spectra from slice-profile-aware MEGA-PRESS simulations. The selected models are trained on 100,000 simulated spectra. We validate their performance on 144 spectra from 112 experimental phantoms containing five metabolites of interest (GABA, Glu, Gln, NAA, Cr) with known ground truth concentrations across solution and gel series acquired at 3 T under varied bandwidths and implementations. These models are further assessed against the widely used LCModel quantification tool. On simulations, both models achieve near-perfect agreement (small MAEs; regression slopes $\\approx 1.00$, $R^2 \\approx 1.00$). On experimental phantom data, errors initially increased substantially. However, modelling variable linewidths in the training data significantly reduced this gap. The best augmented deep learning models achieved a mean MAE for GABA over all phantom spectra of 0.151 (YAE) and 0.160 (FCNN) in max-normalised relative concentrations, outperforming the conventional baseline LCModel (0.220). A sim-to-real gap remains, but physics-informed data augmentation substantially reduced it. Phantom ground truth is needed to judge whether a method will perform reliably on real data.",
      "authors": [
        "Zien Ma",
        "S. M. Shermer",
        "Oktay Karakuş",
        "Frank C. Langbein"
      ],
      "url": "https://arxiv.org/abs/2602.20289",
      "published": "2026-02-23T19:16:03+00:00",
      "categories": [
        "eess.SP",
        "cs.LG",
        "q-bio.QM"
      ]
    },
    {
      "id": "2602.20273",
      "title": "The Truthfulness Spectrum Hypothesis",
      "abstract": "Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec.",
      "authors": [
        "Zhuofan Josh Ying",
        "Shauli Ravfogel",
        "Nikolaus Kriegeskorte",
        "Peter Hase"
      ],
      "url": "https://arxiv.org/abs/2602.20273",
      "published": "2026-02-23T19:01:31+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20271",
      "title": "Uncertainty-Aware Delivery Delay Duration Prediction via Multi-Task Deep Learning",
      "abstract": "Accurate delivery delay prediction is critical for maintaining operational efficiency and customer satisfaction across modern supply chains. Yet the increasing complexity of logistics networks, spanning multimodal transportation, cross-country routing, and pronounced regional variability, makes this prediction task inherently challenging. This paper introduces a multi-task deep learning model for delivery delay duration prediction in the presence of significant imbalanced data, where delayed shipments are rare but operationally consequential. The model embeds high-dimensional shipment features with dedicated embedding layers for tabular data, and then uses a classification-then-regression strategy to predict the delivery delay duration for on-time and delayed shipments. Unlike sequential pipelines, this approach enables end-to-end training, improves the detection of delayed cases, and supports probabilistic forecasting for uncertainty-aware decision making. The proposed approach is evaluated on a large-scale real-world dataset from an industrial partner, comprising more than 10 million historical shipment records across four major source locations with distinct regional characteristics. The proposed model is compared with traditional machine learning methods. Experimental results show that the proposed method achieves a mean absolute error of 0.67-0.91 days for delayed-shipment predictions, outperforming single-step tree-based regression baselines by 41-64% and two-step classify-then-regress tree-based models by 15-35%. These gains demonstrate the effectiveness of the proposed model in operational delivery delay forecasting under highly imbalanced and heterogeneous conditions.",
      "authors": [
        "Stefan Faulkner",
        "Reza Zandehshahvar",
        "Vahid Eghbal Akhlaghi",
        "Sebastien Ouellet",
        "Carsten Jordan",
        "Pascal Van Hentenryck"
      ],
      "url": "https://arxiv.org/abs/2602.20271",
      "published": "2026-02-23T19:01:03+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.AP"
      ]
    },
    {
      "id": "2602.20159",
      "title": "A Very Big Video Reasoning Suite",
      "abstract": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
      "authors": [
        "Maijunxian Wang",
        "Ruisi Wang",
        "Juyi Lin",
        "Ran Ji",
        "Thaddäus Wiedemer",
        "Qingying Gao",
        "Dezhi Luo",
        "Yaoyao Qian",
        "Lianyu Huang",
        "Zelong Hong",
        "Jiahui Ge",
        "Qianli Ma",
        "Hang He",
        "Yifan Zhou",
        "Lingzi Guo",
        "Lantao Mei",
        "Jiachen Li",
        "Hanwen Xing",
        "Tianqi Zhao",
        "Fengyuan Yu",
        "Weihang Xiao",
        "Yizheng Jiao",
        "Jianheng Hou",
        "Danyang Zhang",
        "Pengcheng Xu",
        "Boyang Zhong",
        "Zehong Zhao",
        "Gaoyun Fang",
        "John Kitaoka",
        "Yile Xu",
        "Hua Xu",
        "Kenton Blacutt",
        "Tin Nguyen",
        "Siyuan Song",
        "Haoran Sun",
        "Shaoyue Wen",
        "Linyang He",
        "Runming Wang",
        "Yanzhi Wang",
        "Mengyue Yang",
        "Ziqiao Ma",
        "Raphaël Millière",
        "Freda Shi",
        "Nuno Vasconcelos",
        "Daniel Khashabi",
        "Alan Yuille",
        "Yilun Du",
        "Ziming Liu",
        "Bo Li",
        "Dahua Lin",
        "Ziwei Liu",
        "Vikash Kumar",
        "Yijiang Li",
        "Lei Yang",
        "Zhongang Cai",
        "Hokin Deng"
      ],
      "url": "https://arxiv.org/abs/2602.20159",
      "published": "2026-02-23T18:59:41+00:00",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.RO"
      ]
    },
    {
      "id": "2602.20156",
      "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
      "abstract": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.",
      "authors": [
        "David Schmotz",
        "Luca Beurer-Kellner",
        "Sahar Abdelnabi",
        "Maksym Andriushchenko"
      ],
      "url": "https://arxiv.org/abs/2602.20156",
      "published": "2026-02-23T18:59:27+00:00",
      "categories": [
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20153",
      "title": "JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks",
      "abstract": "We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models' internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.",
      "authors": [
        "Jakob Heiss",
        "Sören Lambrecht",
        "Jakob Weissteiner",
        "Hanna Wutte",
        "Žan Žurič",
        "Josef Teichmann",
        "Bin Yu"
      ],
      "url": "https://arxiv.org/abs/2602.20153",
      "published": "2026-02-23T18:59:10+00:00",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "id": "2602.20152",
      "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data",
      "abstract": "Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.",
      "authors": [
        "Zhenyao Ma",
        "Yue Liang",
        "Dongxu Li"
      ],
      "url": "https://arxiv.org/abs/2602.20152",
      "published": "2026-02-23T18:59:04+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20151",
      "title": "Conformal Risk Control for Non-Monotonic Losses",
      "abstract": "Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.",
      "authors": [
        "Anastasios N. Angelopoulos"
      ],
      "url": "https://arxiv.org/abs/2602.20151",
      "published": "2026-02-23T18:58:54+00:00",
      "categories": [
        "stat.ME",
        "cs.LG",
        "math.ST",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20232",
      "title": "Coupled Cluster con MōLe: Molecular Orbital Learning for Neural Wavefunctions",
      "abstract": "Density functional theory (DFT) is the most widely used method for calculating molecular properties; however, its accuracy is often insufficient for quantitative predictions. Coupled-cluster (CC) theory is the most successful method for achieving accuracy beyond DFT and for predicting properties that closely align with experiment. It is known as the ''gold standard'' of quantum chemistry. Unfortunately, the high computational cost of CC limits its widespread applicability. In this work, we present the Molecular Orbital Learning (MōLe) architecture, an equivariant machine learning model that directly predicts CC's core mathematical objects, the excitation amplitudes, from the mean-field Hartree-Fock molecular orbitals as inputs. We test various aspects of our model and demonstrate its remarkable data efficiency and out-of-distribution generalization to larger molecules and off-equilibrium geometries, despite being trained only on small equilibrium geometries. Finally, we also examine its ability to reduce the number of cycles required to converge CC calculations. MōLe can set the foundations for high-accuracy wavefunction-based ML architectures to accelerate molecular design and complement force-field approaches.",
      "authors": [
        "Luca Thiede",
        "Abdulrahman Aldossary",
        "Andreas Burger",
        "Jorge Arturo Campos-Gonzalez-Angulo",
        "Ning Wang",
        "Alexander Zook",
        "Melisa Alkan",
        "Kouhei Nakaji",
        "Taylor Lee Patti",
        "Jérôme Florian Gonthier",
        "Mohammad Ghazi Vakili",
        "Alán Aspuru-Guzik"
      ],
      "url": "https://arxiv.org/abs/2602.20232",
      "published": "2026-02-23T18:54:46+00:00",
      "categories": [
        "cs.LG",
        "physics.chem-ph"
      ]
    },
    {
      "id": "2602.20144",
      "title": "Agentic AI for Scalable and Robust Optical Systems Control",
      "abstract": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.",
      "authors": [
        "Zehao Wang",
        "Mingzhe Han",
        "Wei Cheng",
        "Yue-Kai Huang",
        "Philip Ji",
        "Denton Wu",
        "Mahdi Safari",
        "Flemming Holtorf",
        "Kenaish AlQubaisi",
        "Norbert M. Linke",
        "Danyang Zhuo",
        "Yiran Chen",
        "Ting Wang",
        "Dirk Englund",
        "Tingjun Chen"
      ],
      "url": "https://arxiv.org/abs/2602.20144",
      "published": "2026-02-23T18:54:32+00:00",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.NI"
      ]
    },
    {
      "id": "2602.20141",
      "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games",
      "abstract": "Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax.",
      "authors": [
        "Clarisse Wibault",
        "Johannes Forkel",
        "Sebastian Towers",
        "Tiphaine Wibault",
        "Juan Duque",
        "George Whittle",
        "Andreas Schaab",
        "Yucheng Yang",
        "Chiyuan Wang",
        "Michael Osborne",
        "Benjamin Moll",
        "Jakob Foerster"
      ],
      "url": "https://arxiv.org/abs/2602.20141",
      "published": "2026-02-23T18:53:09+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20135",
      "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
      "abstract": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.",
      "authors": [
        "Mohammad Amanlou",
        "Erfan Shafiee Moghaddam",
        "Yasaman Amou Jafari",
        "Mahdi Noori",
        "Farhan Farsi",
        "Behnam Bahrak"
      ],
      "url": "https://arxiv.org/abs/2602.20135",
      "published": "2026-02-23T18:46:27+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "id": "2602.20134",
      "title": "Modeling Epidemiological Dynamics Under Adversarial Data and User Deception",
      "abstract": "Epidemiological models increasingly rely on self-reported behavioral data such as vaccination status, mask usage, and social distancing adherence to forecast disease transmission and assess the impact of non-pharmaceutical interventions (NPIs). While such data provide valuable real-time insights, they are often subject to strategic misreporting, driven by individual incentives to avoid penalties, access benefits, or express distrust in public health authorities. To account for such human behavior, in this paper, we introduce a game-theoretic framework that models the interaction between the population and a public health authority as a signaling game. Individuals (senders) choose how to report their behaviors, while the public health authority (receiver) updates their epidemiological model(s) based on potentially distorted signals. Focusing on deception around masking and vaccination, we characterize analytically game equilibrium outcomes and evaluate the degree to which deception can be tolerated while maintaining epidemic control through policy interventions. Our results show that separating equilibria-with minimal deception-drive infections to near zero over time. Remarkably, even under pervasive dishonesty in pooling equilibria, well-designed sender and receiver strategies can still maintain effective epidemic control. This work advances the understanding of adversarial data in epidemiology and offers tools for designing more robust public health models in the presence of strategic user behavior.",
      "authors": [
        "Yiqi Su",
        "Christo Kurisummoottil Thomas",
        "Walid Saad",
        "Bud Mishra",
        "Naren Ramakrishnan"
      ],
      "url": "https://arxiv.org/abs/2602.20134",
      "published": "2026-02-23T18:45:55+00:00",
      "categories": [
        "cs.GT",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20133",
      "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
      "abstract": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.",
      "authors": [
        "Mert Cemri",
        "Shubham Agrawal",
        "Akshat Gupta",
        "Shu Liu",
        "Audrey Cheng",
        "Qiuyang Mang",
        "Ashwin Naren",
        "Lutfi Eren Erdogan",
        "Koushik Sen",
        "Matei Zaharia",
        "Alex Dimakis",
        "Ion Stoica"
      ],
      "url": "https://arxiv.org/abs/2602.20133",
      "published": "2026-02-23T18:45:31+00:00",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.20132",
      "title": "LAD: Learning Advantage Distribution for Reasoning",
      "abstract": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.",
      "authors": [
        "Wendi Li",
        "Sharon Li"
      ],
      "url": "https://arxiv.org/abs/2602.20132",
      "published": "2026-02-23T18:44:10+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20130",
      "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
      "abstract": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.   Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.   Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.   Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.   Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.",
      "authors": [
        "Zaifu Zhan",
        "Min Zeng",
        "Shuang Zhou",
        "Yiran Song",
        "Xiaoyi Chen",
        "Yu Hou",
        "Yifan Wu",
        "Yang Ruan",
        "Rui Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.20130",
      "published": "2026-02-23T18:42:50+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20126",
      "title": "Adaptation to Intrinsic Dependence in Diffusion Language Models",
      "abstract": "Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\\widetilde O(\\mathsf{TC}/K)$ and $\\widetilde O(\\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\\mathsf{TC}$ and $\\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K<L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.",
      "authors": [
        "Yunxiao Zhao",
        "Changxiao Cai"
      ],
      "url": "https://arxiv.org/abs/2602.20126",
      "published": "2026-02-23T18:41:34+00:00",
      "categories": [
        "cs.LG",
        "cs.IT",
        "math.ST",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20122",
      "title": "NanoKnow: How to Know What Your Language Model Knows",
      "abstract": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.",
      "authors": [
        "Lingwei Gu",
        "Nour Jedidi",
        "Jimmy Lin"
      ],
      "url": "https://arxiv.org/abs/2602.20122",
      "published": "2026-02-23T18:37:49+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20119",
      "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
      "abstract": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
      "authors": [
        "Jiahui Fu",
        "Junyu Nan",
        "Lingfeng Sun",
        "Hongyu Li",
        "Jianing Qian",
        "Jennifer L. Barry",
        "Kris Kitani",
        "George Konidaris"
      ],
      "url": "https://arxiv.org/abs/2602.20119",
      "published": "2026-02-23T18:35:18+00:00",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "id": "2602.20117",
      "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs",
      "authors": [
        "Andre He",
        "Nathaniel Weir",
        "Kaj Bostrom",
        "Allen Nie",
        "Darion Cassel",
        "Sam Bayless",
        "Huzefa Rangwala"
      ],
      "url": "https://arxiv.org/abs/2602.20117",
      "published": "2026-02-23T18:34:29+00:00",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20114",
      "title": "Benchmarking Unlearning for Vision Transformers",
      "abstract": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.",
      "authors": [
        "Kairan Zhao",
        "Iurie Luca",
        "Peter Triantafillou"
      ],
      "url": "https://arxiv.org/abs/2602.20114",
      "published": "2026-02-23T18:33:16+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20113",
      "title": "StyleStream: Real-Time Zero-Shot Voice Style Conversion",
      "abstract": "Voice style conversion aims to transform an input utterance to match a target speaker's timbre, accent, and emotion, with a central challenge being the disentanglement of linguistic content from style. While prior work has explored this problem, conversion quality remains limited, and real-time voice style conversion has not been addressed. We propose StyleStream, the first streamable zero-shot voice style conversion system that achieves state-of-the-art performance. StyleStream consists of two components: a Destylizer, which removes style attributes while preserving linguistic content, and a Stylizer, a diffusion transformer (DiT) that reintroduces target style conditioned on reference speech. Robust content-style disentanglement is enforced through text supervision and a highly constrained information bottleneck. This design enables a fully non-autoregressive architecture, achieving real-time voice style conversion with an end-to-end latency of 1 second. Samples and real-time demo: https://berkeley-speech-group.github.io/StyleStream/.",
      "authors": [
        "Yisi Liu",
        "Nicholas Lee",
        "Gopala Anumanchipalli"
      ],
      "url": "https://arxiv.org/abs/2602.20113",
      "published": "2026-02-23T18:32:59+00:00",
      "categories": [
        "cs.SD",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20111",
      "title": "Reliable Abstention under Adversarial Injections: Tight Lower Bounds and New Upper Bounds",
      "abstract": "We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\\ from an unknown distribution $\\mathcal{D}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. Crucially, labels are always consistent with a fixed target concept (the clean-label setting). The learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\\ rounds. Perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $O(d^2 \\log T)$ combined error for VC dimension $d$, while distribution-agnostic algorithms achieve only $\\tilde{O}(\\sqrt{T})$ for restricted classes, leaving open whether this gap is fundamental.   We resolve this question by proving a matching $Ω(\\sqrt{T})$ lower bound for VC dimension $1$, establishing a sharp separation between the two information regimes. On the algorithmic side, we introduce a potential-based framework driven by \\emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. We instantiate this framework using two combinatorial dimensions: (1) \\emph{inference dimension}, yielding combined error $\\tilde{O}(T^{1-1/k})$ for classes of inference dimension $k$, and (2) \\emph{certificate dimension}, a new relaxation we introduce. As an application, we show that halfspaces in $\\mathbb{R}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\\tilde{O}(T^{2/3})$ for this class. This is notable since [Blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention.",
      "authors": [
        "Ezra Edelman",
        "Surbhi Goel"
      ],
      "url": "https://arxiv.org/abs/2602.20111",
      "published": "2026-02-23T18:30:48+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20104",
      "title": "Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration",
      "abstract": "In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.",
      "authors": [
        "Hasan Amin",
        "Ming Yin",
        "Rajiv Khanna"
      ],
      "url": "https://arxiv.org/abs/2602.20104",
      "published": "2026-02-23T18:22:58+00:00",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20102",
      "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
      "abstract": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.",
      "authors": [
        "Thanh Q. Tran",
        "Arun Verma",
        "Kiwan Wong",
        "Bryan Kian Hsiang Low",
        "Daniela Rus",
        "Wei Xiao"
      ],
      "url": "https://arxiv.org/abs/2602.20102",
      "published": "2026-02-23T18:19:46+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20100",
      "title": "Transcending the Annotation Bottleneck: AI-Powered Discovery in Biology and Medicine",
      "abstract": "The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets. By learning directly from the intrinsic structure of data - whether pixels in a magnetic resonance image (MRI), voxels in a volumetric scan, or tokens in a genomic sequence - these methods facilitate the discovery of novel phenotypes, the linkage of morphology to genetics, and the detection of anomalies without human bias. This article synthesises seminal and recent advances in \"learning without labels,\" highlighting how unsupervised frameworks can derive heritable cardiac traits, predict spatial gene expression in histology, and detect pathologies with performance that rivals or exceeds supervised counterparts.",
      "authors": [
        "Soumick Chatterjee"
      ],
      "url": "https://arxiv.org/abs/2602.20100",
      "published": "2026-02-23T18:15:30+00:00",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ]
    },
    {
      "id": "2602.20094",
      "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
      "abstract": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.",
      "authors": [
        "Yuzhe Wang",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "url": "https://arxiv.org/abs/2602.20094",
      "published": "2026-02-23T18:06:15+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20092",
      "title": "BabyLM Turns 4 and Goes Multilingual: Call for Papers for the 2026 BabyLM Workshop",
      "abstract": "The goal of the BabyLM is to stimulate new research connections between cognitive modeling and language model pretraining. We invite contributions in this vein to the BabyLM Workshop, which will also include the 4th iteration of the BabyLM Challenge. As in previous years, the challenge features two ``standard'' tracks (Strict and Strict-Small), in which participants must train language models on under 100M or 10M words of data, respectively. This year, we move beyond our previous English-only pretraining datasets with a new Multilingual track, focusing on English, Dutch, and Chinese. For the workshop, we call for papers related to the overall theme of BabyLM, which includes training efficiency, small-scale training datasets, cognitive modeling, model evaluation, and architecture innovation.",
      "authors": [
        "Leshem Choshen",
        "Ryan Cotterell",
        "Mustafa Omer Gul",
        "Jaap Jumelet",
        "Tal Linzen",
        "Aaron Mueller",
        "Suchir Salhan",
        "Raj Sanjay Shah",
        "Alex Warstadt",
        "Ethan Gotlieb Wilcox"
      ],
      "url": "https://arxiv.org/abs/2602.20092",
      "published": "2026-02-23T18:02:23+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20091",
      "title": "How Retrieved Context Shapes Internal Representations in RAG",
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.",
      "authors": [
        "Samuel Yeh",
        "Sharon Li"
      ],
      "url": "https://arxiv.org/abs/2602.20091",
      "published": "2026-02-23T18:02:04+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20089",
      "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
      "abstract": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them \"structure-centric\". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.",
      "authors": [
        "Zanxi Ruan",
        "Qiuyu Kong",
        "Songqun Gao",
        "Yiming Wang",
        "Marco Cristani"
      ],
      "url": "https://arxiv.org/abs/2602.20089",
      "published": "2026-02-23T17:57:37+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20078",
      "title": "Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning",
      "abstract": "Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Θ(N)$, yielding sample complexity $\\mathcal{O}(N/ε)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent's gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Θ(N)$ to $\\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\\mathcal{O}(1/ε)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures.",
      "authors": [
        "Shan Yang",
        "Yang Liu"
      ],
      "url": "https://arxiv.org/abs/2602.20078",
      "published": "2026-02-23T17:45:08+00:00",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20076",
      "title": "Robust Taylor-Lagrange Control for Safety-Critical Systems",
      "abstract": "Solving safety-critical control problem has widely adopted the Control Barrier Function (CBF) method. However, the existence of a CBF is only a sufficient condition for system safety. The recently proposed Taylor-Lagrange Control (TLC) method addresses this limitation, but is vulnerable to the feasibility preservation problem (e.g., inter-sampling effect). In this paper, we propose a robust TLC (rTLC) method to address the feasibility preservation problem. Specifically, the rTLC method expands the safety function at an order higher than the relative degree of the function using Taylor's expansion with Lagrange remainder, which allows the control to explicitly show up at the current time instead of the future time in the TLC method. The rTLC method naturally addresses the feasibility preservation problem with only one hyper-parameter (the discretization time interval size during implementation), which is much less than its counterparts. Finally, we illustrate the effectiveness of the proposed rTLC method through an adaptive cruise control problem, and compare it with existing safety-critical control methods.",
      "authors": [
        "Wei Xiao",
        "Christos Cassandras",
        "Anni Li"
      ],
      "url": "https://arxiv.org/abs/2602.20076",
      "published": "2026-02-23T17:40:05+00:00",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.RO"
      ]
    },
    {
      "id": "2602.20070",
      "title": "Training-Free Generative Modeling via Kernelized Stochastic Interpolants",
      "abstract": "We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems. The drift of the generative SDE is $\\hat b_t(x) = \\nablaφ(x)^\\topη_t$, where $η_t\\in\\R^P$ solves a $P\\times P$ system computable from data, with $P$ independent of the data dimension $d$. Since estimates are inexact, the diffusion coefficient $D_t$ affects sample quality; the optimal $D_t^*$ from Girsanov diverges at $t=0$, but this poses no difficulty and we develop an integrator that handles it seamlessly. The framework accommodates diverse feature maps -- scattering transforms, pretrained generative models etc. -- enabling training-free generation and model combination. We demonstrate the approach on financial time series, turbulence, and image generation.",
      "authors": [
        "Florentin Coeurdoux",
        "Etienne Lempereur",
        "Nathanaël Cuvelle-Magar",
        "Thomas Eboli",
        "Stéphane Mallat",
        "Anastasia Borovykh",
        "Eric Vanden-Eijnden"
      ],
      "url": "https://arxiv.org/abs/2602.20070",
      "published": "2026-02-23T17:26:09+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20068",
      "title": "The Invisible Gorilla Effect in Out-of-distribution Detection",
      "abstract": "Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions. Although prior work shows that OOD detection performance varies by artefact type, the underlying causes remain underexplored. To this end, we identify a previously unreported bias in OOD detection: for hard-to-detect artefacts (near-OOD), detection performance typically improves when the artefact shares visual similarity (e.g. colour) with the model's ROI and drops when it does not - a phenomenon we term the Invisible Gorilla Effect. For example, in a skin lesion classifier with red lesion ROI, we show the method Mahalanobis Score achieves a 31.5% higher AUROC when detecting OOD red ink (similar to ROI) compared to black ink (dissimilar) annotations. We annotated artefacts by colour in 11,355 images from three public datasets (e.g. ISIC) and generated colour-swapped counterfactuals to rule out dataset bias. We then evaluated 40 OOD methods across 7 benchmarks and found significant performance drops for most methods when artefacts differed from the ROI. Our findings highlight an overlooked failure mode in OOD detection and provide guidance for more robust detectors. Code and annotations are available at: https://github.com/HarryAnthony/Invisible_Gorilla_Effect.",
      "authors": [
        "Harry Anthony",
        "Ziyun Liang",
        "Hermione Warr",
        "Konstantinos Kamnitsas"
      ],
      "url": "https://arxiv.org/abs/2602.20068",
      "published": "2026-02-23T17:24:18+00:00",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20066",
      "title": "HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images",
      "abstract": "Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using semantic features extracted from satellite images, basic Geographic Information System (GIS), and building-level features. We feed pretrained Large Vision Language Models (VLMs) with a domain-specific prompt to act as an energy planner and extract the visual attributes such as roof age, building density, etc, from the RGB satellite image that correspond to the thermal load. A Multi-Layer Perceptron (MLP) regressor trained on these captions shows an $R^2$ uplift of 93.7% and shrinks the mean absolute error (MAE) by 30% compared to the baseline model. Qualitative analysis shows that high-impact tokens align with high-demand zones, offering lightweight support for heat planning in data-scarce regions.",
      "authors": [
        "Kundan Thota",
        "Xuanhao Mu",
        "Thorsten Schlachter",
        "Veit Hagenmeyer"
      ],
      "url": "https://arxiv.org/abs/2602.20066",
      "published": "2026-02-23T17:22:54+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20065",
      "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees",
      "abstract": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.",
      "authors": [
        "Natalia Moskvina",
        "Raquel Montero",
        "Masaya Yoshida",
        "Ferdy Hubers",
        "Paolo Morosi",
        "Walid Irhaymi",
        "Jin Yan",
        "Tamara Serrano",
        "Elena Pagliarini",
        "Fritz Günther",
        "Evelina Leivada"
      ],
      "url": "https://arxiv.org/abs/2602.20065",
      "published": "2026-02-23T17:22:46+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20064",
      "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow",
      "abstract": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming.",
      "authors": [
        "Zac Garby",
        "Andrew D. Gordon",
        "David Sands"
      ],
      "url": "https://arxiv.org/abs/2602.20064",
      "published": "2026-02-23T17:22:35+00:00",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "id": "2602.20062",
      "title": "A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning",
      "abstract": "Pretraining and fine-tuning are central stages in modern machine learning systems. In practice, feature learning plays an important role across both stages: deep neural networks learn a broad range of useful features during pretraining and further refine those features during fine-tuning. However, an end-to-end theoretical understanding of how choices of initialization impact the ability to reuse and refine features during fine-tuning has remained elusive. Here we develop an analytical theory of the pretraining-fine-tuning pipeline in diagonal linear networks, deriving exact expressions for the generalization error as a function of initialization parameters and task statistics. We find that different initialization choices place the network into four distinct fine-tuning regimes that are distinguished by their ability to support feature learning and reuse, and therefore by the task statistics for which they are beneficial. In particular, a smaller initialization scale in earlier layers enables the network to both reuse and refine its features, leading to superior generalization on fine-tuning tasks that rely on a subset of pretraining features. We demonstrate empirically that the same initialization parameters impact generalization in nonlinear networks trained on CIFAR-100. Overall, our results demonstrate analytically how data and network initialization interact to shape fine-tuning generalization, highlighting an important role for the relative scale of initialization across different layers in enabling continued feature learning during fine-tuning.",
      "authors": [
        "Nicolas Anguita",
        "Francesco Locatello",
        "Andrew M. Saxe",
        "Marco Mondelli",
        "Flavia Mancini",
        "Samuel Lippl",
        "Clementine Domine"
      ],
      "url": "https://arxiv.org/abs/2602.20062",
      "published": "2026-02-23T17:19:33+00:00",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "id": "2602.20059",
      "title": "Interaction Theater: A case of LLM Agents Interacting at Scale",
      "abstract": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\\%$) vary their output across contexts, $65\\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\\%$) and off-topic content ($22\\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.",
      "authors": [
        "Sarath Shekkizhar",
        "Adam Earle"
      ],
      "url": "https://arxiv.org/abs/2602.20059",
      "published": "2026-02-23T17:14:29+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20057",
      "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation",
      "abstract": "Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world models provide strong supervision signals, enabling online adaptive learning in dynamic environments, which can be complemented by force-torque feedback to mitigate dynamic force shifts. Our AdaWorldPolicy integrates a world model, an action expert, and a force predictor-all implemented as interconnected Flow Matching Diffusion Transformers (DiT). They are interconnected via the multi-modal self-attention layers, enabling deep feature exchange for joint learning while preserving their distinct modularity characteristics. We further propose a novel Online Adaptive Learning (AdaOL) strategy that dynamically switches between an Action Generation mode and a Future Imagination mode to drive reactive updates across all three modules. This creates a powerful closed-loop mechanism that adapts to both visual and physical domain shifts with minimal overhead. Across a suite of simulated and real-robot benchmarks, our AdaWorldPolicy achieves state-of-the-art performance, with dynamical adaptive capacity to out-of-distribution scenarios.",
      "authors": [
        "Ge Yuan",
        "Qiyuan Qiao",
        "Jing Zhang",
        "Dong Xu"
      ],
      "url": "https://arxiv.org/abs/2602.20057",
      "published": "2026-02-23T17:12:25+00:00",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20055",
      "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation",
      "abstract": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.",
      "authors": [
        "Apoorva Vashisth",
        "Manav Kulshrestha",
        "Pranav Bakshi",
        "Damon Conover",
        "Guillaume Sartoretti",
        "Aniket Bera"
      ],
      "url": "https://arxiv.org/abs/2602.20055",
      "published": "2026-02-23T17:10:00+00:00",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "id": "2602.21257",
      "title": "Structured Prompt Language: Declarative Context Management for LLMs",
      "abstract": "We present SPL (Structured Prompt Language), a declarative SQL-inspired language that treats large language models as generative knowledge bases and their context windows as constrained resources. SPL provides explicit WITH BUDGET/LIMIT token management, an automatic query optimizer, EXPLAIN transparency analogous to SQL's EXPLAIN ANALYZE, and native integration of retrieval-augmented generation (RAG) and persistent memory in a single declarative framework. SPL-flow extends SPL into resilient agentic pipelines with a three-tier provider fallback strategy (Ollama -> OpenRouter -> self-healing retry) fully transparent to the .spl script. Five extensions demonstrate the paradigm's breadth: (1) Text2SPL (multilingual NL->SPL translation); (2) Mixture-of-Models (MoM) routing that dispatches each PROMPT to a domain-specialist model at runtime; (3) Logical Chunking, an intelligent strategy for documents exceeding a single context window--expressed naturally through SPL's existing CTE syntax with no new constructs, decomposing a large query into a Map-Reduce pipeline that reduces attention cost from O(N^2) to O(N^2/k) and runs identically on cloud (parallel) or local hardware (sequential); (4) SPL-flow, a declarative agentic orchestration layer with resilient three-tier provider fallback; and (5) BENCHMARK for parallel multi-model comparison with automatic winner persistence. We provide a formal EBNF grammar, two pip-installable Python packages (spl-llm, spl-flow), and comparison against Prompty, DSPy, and LMQL. SPL reduces prompt boilerplate by 65% on average, surfaces a 68x cost spread across model tiers as a pre-execution signal, and runs the identical .spl script at $0.002 on OpenRouter or at zero marginal cost on a local Ollama instance--without modification.",
      "authors": [
        "Wen G. Gong"
      ],
      "url": "https://arxiv.org/abs/2602.21257",
      "published": "2026-02-23T17:03:31+00:00",
      "categories": [
        "cs.CL",
        "cs.DB",
        "cs.PL"
      ]
    },
    {
      "id": "2602.20052",
      "title": "Entropy in Large Language Models",
      "abstract": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular.",
      "authors": [
        "Marco Scharringhausen"
      ],
      "url": "https://arxiv.org/abs/2602.20052",
      "published": "2026-02-23T17:02:45+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20051",
      "title": "SEAL-pose: Enhancing 3D Human Pose Estimation via a Learned Loss for Structural Consistency",
      "abstract": "3D human pose estimation (HPE) is characterized by intricate local and global dependencies among joints. Conventional supervised losses are limited in capturing these correlations because they treat each joint independently. Previous studies have attempted to promote structural consistency through manually designed priors or rule-based constraints; however, these approaches typically require manual specification and are often non-differentiable, limiting their use as end-to-end training objectives. We propose SEAL-pose, a data-driven framework in which a learnable loss-net trains a pose-net by evaluating structural plausibility. Rather than relying on hand-crafted priors, our joint-graph-based design enables the loss-net to learn complex structural dependencies directly from data. Extensive experiments on three 3D HPE benchmarks with eight backbones show that SEAL-pose reduces per-joint errors and improves pose plausibility compared with the corresponding backbones across all settings. Beyond improving each backbone, SEAL-pose also outperforms models with explicit structural constraints, despite not enforcing any such constraints. Finally, we analyze the relationship between the loss-net and structural consistency, and evaluate SEAL-pose in cross-dataset and in-the-wild settings.",
      "authors": [
        "Yeonsung Kim",
        "Junggeun Do",
        "Seunguk Do",
        "Sangmin Kim",
        "Jaesik Park",
        "Jay-Yoon Lee"
      ],
      "url": "https://arxiv.org/abs/2602.20051",
      "published": "2026-02-23T17:00:35+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20048",
      "title": "CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence",
      "abstract": "Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI repository, we demonstrate that graph-based structural navigation via CodeCompass--a Model Context Protocol server exposing dependency graphs--achieves 99.4% task completion on hidden-dependency tasks, a 23.2 percentage-point improvement over vanilla agents (76.2%) and 21.2 points over BM25 retrieval (78.2%).However, we uncover a critical adoption gap: 58% of trials with graph access made zero tool calls, and agents required explicit prompt engineering to adopt the tool consistently. Our findings reveal that the bottleneck is not tool availability but behavioral alignment--agents must be explicitly guided to leverage structural context over lexical heuristics. We contribute: (1) a task taxonomy distinguishing semantic-search, structural, and hidden-dependency scenarios; (2) empirical evidence that graph navigation outperforms retrieval when dependencies lack lexical overlap; and (3) open-source infrastructure for reproducible evaluation of navigation tools.",
      "authors": [
        "Tarakanath Paipuru"
      ],
      "url": "https://arxiv.org/abs/2602.20048",
      "published": "2026-02-23T16:58:37+00:00",
      "categories": [
        "cs.AI",
        "cs.SE"
      ]
    },
    {
      "id": "2602.20046",
      "title": "Closing the gap in multimodal medical representation alignment",
      "abstract": "In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remains unknown and unresolved in more complex multimodal settings, such as the medical domain. In this work, we study this phenomenon in the latter case, revealing that the modality gap is present also in medical alignment, and we propose a modality-agnostic framework that closes this gap, ensuring that semantically related representations are more aligned, regardless of their source modality. Our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning.",
      "authors": [
        "Eleonora Grassucci",
        "Giordano Cicchetti",
        "Danilo Comminiello"
      ],
      "url": "https://arxiv.org/abs/2602.20046",
      "published": "2026-02-23T16:57:39+00:00",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20042",
      "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously",
      "abstract": "Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \\textbf{structural} value flattening, \\textbf{normative} representation loss, and \\textbf{cognitive} uncertainty blindness. We introduce Edge Alignment as a distinct approach in which systems preserve multi dimensional value structure, support plural and democratic representation, and incorporate epistemic mechanisms for interaction and clarification. To make this approach practical, we propose seven interdependent pillars organized into three phases. We identify key challenges in data collection, training objectives, and evaluation, outlining complementary technical and governance directions. Taken together, these measures reframe alignment as a lifecycle problem of dynamic normative governance rather than as a single instance optimization task.",
      "authors": [
        "Han Bao",
        "Yue Huang",
        "Xiaoda Wang",
        "Zheyuan Zhang",
        "Yujun Zhou",
        "Carl Yang",
        "Xiangliang Zhang",
        "Yanfang Ye"
      ],
      "url": "https://arxiv.org/abs/2602.20042",
      "published": "2026-02-23T16:51:43+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20040",
      "title": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization",
      "abstract": "Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.",
      "authors": [
        "Fahmida Liza Piya",
        "Rahmatollah Beheshti"
      ],
      "url": "https://arxiv.org/abs/2602.20040",
      "published": "2026-02-23T16:49:37+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20031",
      "title": "Latent Introspection: Models Can Detect Prior Concept Injections",
      "abstract": "We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.2%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.62 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety.",
      "authors": [
        "Theia Pearson-Vogel",
        "Martin Vanek",
        "Raymond Douglas",
        "Jan Kulveit"
      ],
      "url": "https://arxiv.org/abs/2602.20031",
      "published": "2026-02-23T16:39:42+00:00",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20021",
      "title": "Agents of Chaos",
      "abstract": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.",
      "authors": [
        "Natalie Shapira",
        "Chris Wendler",
        "Avery Yen",
        "Gabriele Sarti",
        "Koyena Pal",
        "Olivia Floody",
        "Adam Belfki",
        "Alex Loftus",
        "Aditya Ratan Jannali",
        "Nikhil Prakash",
        "Jasmine Cui",
        "Giordano Rogers",
        "Jannik Brinkmann",
        "Can Rager",
        "Amir Zur",
        "Michael Ripa",
        "Aruna Sankaranarayanan",
        "David Atkinson",
        "Rohit Gandikota",
        "Jaden Fiotto-Kaufman",
        "EunJeong Hwang",
        "Hadas Orgad",
        "P Sam Sahil",
        "Negev Taglicht",
        "Tomer Shabtay",
        "Atai Ambus",
        "Nitay Alon",
        "Shiri Oron",
        "Ayelet Gordon-Tapiero",
        "Yotam Kaplan",
        "Vered Shwartz",
        "Tamar Rott Shaham",
        "Christoph Riedl",
        "Reuth Mirsky",
        "Maarten Sap",
        "David Manheim",
        "Tomer Ullman",
        "David Bau"
      ],
      "url": "https://arxiv.org/abs/2602.20021",
      "published": "2026-02-23T16:28:48+00:00",
      "categories": [
        "cs.AI",
        "cs.CY"
      ]
    },
    {
      "id": "2602.20020",
      "title": "gencat: Generative computerized adaptive testing",
      "abstract": "Existing computerized Adaptive Testing (CAT) frameworks are typically built on predicting the correctness of a student response to a question. Although effective, this approach fails to leverage textual information in questions and responses, especially for open-ended questions. In this work, we propose GENCAT (\\textbf{GEN}erative \\textbf{CAT}), a novel CAT framework that leverages Large Language Models for knowledge estimate and question selection. First, we develop a Generative Item Response Theory (GIRT) model that enables us to estimate student knowledge from their open-ended responses and predict responses to unseen questions. We train the model in a two-step process, first via Supervised Fine-Tuning and then via preference optimization for knowledge-response alignment. Second, we introduce three question selection algorithms that leverage the generative capabilities of the GIRT model, based on the uncertainty, linguistic diversity, and information of sampled student responses. Third, we conduct experiments on two real-world programming datasets and demonstrate that GENCAT outperforms existing CAT baselines, achieving an AUC improvement of up to 4.32\\% in the key early testing stages.",
      "authors": [
        "Wanyong Feng",
        "Andrew Lan"
      ],
      "url": "https://arxiv.org/abs/2602.20020",
      "published": "2026-02-23T16:28:46+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20019",
      "title": "Learning Discriminative and Generalizable Anomaly Detector for Dynamic Graph with Limited Supervision",
      "abstract": "Dynamic graph anomaly detection (DGAD) is critical for many real-world applications but remains challenging due to the scarcity of labeled anomalies. Existing methods are either unsupervised or semi-supervised: unsupervised methods avoid the need for labeled anomalies but often produce ambiguous boundary, whereas semi-supervised methods can overfit to the limited labeled anomalies and generalize poorly to unseen anomalies. To address this gap, we consider a largely underexplored problem in DGAD: learning a discriminative boundary from normal/unlabeled data, while leveraging limited labeled anomalies \\textbf{when available} without sacrificing generalization to unseen anomalies. To this end, we propose an effective, generalizable, and model-agnostic framework with three main components: (i) residual representation encoding that capture deviations between current interactions and their historical context, providing anomaly-relevant signals; (ii) a restriction loss that constrain the normal representations within an interval bounded by two co-centered hyperspheres, ensuring consistent scales while keeping anomalies separable; (iii) a bi-boundary optimization strategy that learns a discriminative and robust boundary using the normal log-likelihood distribution modeled by a normalizing flow. Extensive experiments demonstrate the superiority of our framework across diverse evaluation settings.",
      "authors": [
        "Yuxing Tian",
        "Yiyan Qi",
        "Fengran Mo",
        "Weixu Zhang",
        "Jian Guo",
        "Jian-Yun Nie"
      ],
      "url": "https://arxiv.org/abs/2602.20019",
      "published": "2026-02-23T16:25:35+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20017",
      "title": "QUIETT: Query-Independent Table Transformation for Robust Reasoning",
      "abstract": "Real-world tables often exhibit irregular schemas, heterogeneous value formats, and implicit relational structure, which degrade the reliability of downstream table reasoning and question answering. Most existing approaches address these issues in a query-dependent manner, entangling table cleanup with reasoning and thus limiting generalization. We introduce QuIeTT, a query-independent table transformation framework that preprocesses raw tables into a single SQL-ready canonical representation before any test-time queries are observed. QuIeTT performs lossless schema and value normalization, exposes implicit relations, and preserves full provenance via raw table snapshots. By decoupling table transformation from reasoning, QuIeTT enables cleaner, more reliable, and highly efficient querying without modifying downstream models. Experiments on four benchmarks, WikiTQ, HiTab, NQ-Table, and SequentialQA show consistent gains across models and reasoning paradigms, with particularly strong improvements on a challenge set of structurally diverse, unseen questions.",
      "authors": [
        "Gaurav Najpande",
        "Tampu Ravi Kumar",
        "Manan Roy Choudhury",
        "Neha Valeti",
        "Yanjie Fu",
        "Vivek Gupta"
      ],
      "url": "https://arxiv.org/abs/2602.20017",
      "published": "2026-02-23T16:23:49+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20224",
      "title": "Exploring Anti-Aging Literature via ConvexTopics and Large Language Models",
      "abstract": "The rapid expansion of biomedical publications creates challenges for organizing knowledge and detecting emerging trends, underscoring the need for scalable and interpretable methods. Common clustering and topic modeling approaches such as K-means or LDA remain sensitive to initialization and prone to local optima, limiting reproducibility and evaluation. We propose a reformulation of a convex optimization based clustering algorithm that produces stable, fine-grained topics by selecting exemplars from the data and guaranteeing a global optimum. Applied to about 12,000 PubMed articles on aging and longevity, our method uncovers topics validated by medical experts. It yields interpretable topics spanning from molecular mechanisms to dietary supplements, physical activity, and gut microbiota. The method performs favorably, and most importantly, its reproducibility and interpretability distinguish it from common clustering approaches, including K-means, LDA, and BERTopic. This work provides a basis for developing scalable, web-accessible tools for knowledge discovery.",
      "authors": [
        "Lana E. Yeganova",
        "Won G. Kim",
        "Shubo Tian",
        "Natalie Xie",
        "Donald C. Comeau",
        "W. John Wilbur",
        "Zhiyong Lu"
      ],
      "url": "https://arxiv.org/abs/2602.20224",
      "published": "2026-02-23T16:17:33+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.20003",
      "title": "A Secure and Private Distributed Bayesian Federated Learning Design",
      "abstract": "Distributed Federated Learning (DFL) enables decentralized model training across large-scale systems without a central parameter server. However, DFL faces three critical challenges: privacy leakage from honest-but-curious neighbors, slow convergence due to the lack of central coordination, and vulnerability to Byzantine adversaries aiming to degrade model accuracy. To address these issues, we propose a novel DFL framework that integrates Byzantine robustness, privacy preservation, and convergence acceleration. Within this framework, each device trains a local model using a Bayesian approach and independently selects an optimal subset of neighbors for posterior exchange. We formulate this neighbor selection as an optimization problem to minimize the global loss function under security and privacy constraints. Solving this problem is challenging because devices only possess partial network information, and the complex coupling between topology, security, and convergence remains unclear. To bridge this gap, we first analytically characterize the trade-offs between dynamic connectivity, Byzantine detection, privacy levels, and convergence speed. Leveraging these insights, we develop a fully distributed Graph Neural Network (GNN)-based Reinforcement Learning (RL) algorithm. This approach enables devices to make autonomous connection decisions based on local observations. Simulation results demonstrate that our method achieves superior robustness and efficiency with significantly lower overhead compared to traditional security and privacy schemes.",
      "authors": [
        "Nuocheng Yang",
        "Sihua Wang",
        "Zhaohui Yang",
        "Mingzhe Chen",
        "Changchuan Yin",
        "Kaibin Huang"
      ],
      "url": "https://arxiv.org/abs/2602.20003",
      "published": "2026-02-23T16:12:02+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20001",
      "title": "FairFS: Addressing Deep Feature Selection Biases for Recommender System",
      "abstract": "Large-scale online marketplaces and recommender systems serve as critical technological support for e-commerce development. In industrial recommender systems, features play vital roles as they carry information for downstream models. Accurate feature importance estimation is critical because it helps identify the most useful feature subsets from thousands of feature candidates for online services. Such selection enables improved online performance while reducing computational cost. To address feature selection problems in deep learning, trainable gate-based and sensitivity-based methods have been proposed and proven effective in industrial practice. However, through the analysis of real-world cases, we identified three bias issues that cause feature importance estimation to rely on partial model layers, samples, or gradients, ultimately leading to inaccurate importance estimation. We refer to these as layer bias, baseline bias, and approximation bias. To mitigate these issues, we propose FairFS, a fair and accurate feature selection algorithm. FairFS regularizes feature importance estimated across all nonlinear transformation layers to address layer bias. It also introduces a smooth baseline feature close to the classifier decision boundary and adopts an aggregated approximation method to alleviate baseline and approximation biases. Extensive experiments demonstrate that FairFS effectively mitigates these biases and achieves state-of-the-art feature selection performance.",
      "authors": [
        "Xianquan Wang",
        "Zhaocheng Du",
        "Jieming Zhu",
        "Qinglin Jia",
        "Zhenhua Dong",
        "Kai Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.20001",
      "published": "2026-02-23T16:08:32+00:00",
      "categories": [
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19991",
      "title": "Cross-lingual Matryoshka Representation Learning across Speech and Text",
      "abstract": "Speakers of under-represented languages face both a language barrier, as most online knowledge is in a few dominant languages, and a modality barrier, since information is largely text-based while many languages are primarily oral. We address this for French-Wolof by training the first bilingual speech-text Matryoshka embedding model, enabling efficient retrieval of French text from Wolof speech queries without relying on a costly ASR-translation pipelines. We introduce large-scale data curation pipelines and new benchmarks, compare modeling strategies, and show that modality fusion within a frozen text Matryoshka model performs best. Although trained only for retrieval, the model generalizes well to other tasks, such as speech intent detection, indicating the learning of general semantic representations. Finally, we analyze cost-accuracy trade-offs across Matryoshka dimensions and ranks, showing that information is concentrated only in a few components, suggesting potential for efficiency improvements.",
      "authors": [
        "Yaya Sy",
        "Dioula Doucouré",
        "Christophe Cerisara",
        "Irina Illina"
      ],
      "url": "https://arxiv.org/abs/2602.19991",
      "published": "2026-02-23T15:57:16+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.19987",
      "title": "Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction",
      "abstract": "This paper tackles the problem of time-to-event counterfactual survival prediction, aiming to optimize individualized survival outcomes in the presence of heterogeneity and censored data. We propose CURE, a framework that advances counterfactual survival modeling via comprehensive multimodal embedding and latent subgroup retrieval. CURE integrates clinical, paraclinical, demographic, and multi-omics information, which are aligned and fused through cross-attention mechanisms. Complex multi-omics signals can be adaptively refined using a mixture-of-experts architecture, emphasizing the most informative omics components. Building upon this representation, CURE implicitly retrieves patient-specific latent subgroups that capture both baseline survival dynamics and treatment-dependent variations. Experimental results on METABRIC and TCGA-LUAD datasets demonstrate that proposed CURE model consistently outperforms strong baselines in survival analysis, evaluated using the Time-dependent Concordance Index ($C^{td}$) and Integrated Brier Score (IBS). These findings highlight the potential of CURE to enhance multimodal understanding and serve as a foundation for future treatment recommendation models. All code and related resources are publicly available to facilitate the reproducibility https://github.com/L2R-UET/CURE.",
      "authors": [
        "Ha-Anh Hoang Nguyen",
        "Tri-Duc Phan Le",
        "Duc-Hoang Pham",
        "Huy-Son Nguyen",
        "Cam-Van Thi Nguyen",
        "Duc-Trong Le",
        "Hoang-Quynh Le"
      ],
      "url": "https://arxiv.org/abs/2602.19987",
      "published": "2026-02-23T15:53:25+00:00",
      "categories": [
        "cs.LG",
        "cs.IR"
      ]
    },
    {
      "id": "2602.19984",
      "title": "Multivariate time-series forecasting of ASTRI-Horn monitoring data: A Normal Behavior Model",
      "abstract": "This study presents a Normal Behavior Model (NBM) developed to forecast monitoring time-series data from the ASTRI-Horn Cherenkov telescope under normal operating conditions. The analysis focused on 15 physical variables acquired by the Telescope Control Unit between September 2022 and July 2024, representing sensor measurements from the Azimuth and Elevation motors. After data cleaning, resampling, feature selection, and correlation analysis, the dataset was segmented into fixed-length intervals, in which the first I samples represented the input sequence provided to the model, while the forecast length, T, indicated the number of future time steps to be predicted. A sliding-window technique was then applied to increase the number of intervals. A Multi-Layer Perceptron (MLP) was trained to perform multivariate forecasting across all features simultaneously. Model performance was evaluated using the Mean Squared Error (MSE) and the Normalized Median Absolute Deviation (NMAD), and it was also benchmarked against a Long Short-Term Memory (LSTM) network. The MLP model demonstrated consistent results across different features and I-T configurations, and matched the performance of the LSTM while converging faster. It achieved an MSE of 0.019+/-0.003 and an NMAD of 0.032+/-0.009 on the test set under its best configuration (4 hidden layers, 720 units per layer, and I-T lengths of 300 samples each, corresponding to 5 hours at 1-minute resolution). Extending the forecast horizon up to 6.5 hours-the maximum allowed by this configuration-did not degrade performance, confirming the model's effectiveness in providing reliable hour-scale predictions. The proposed NBM provides a powerful tool for enabling early anomaly detection in online ASTRI-Horn monitoring time series, offering a basis for the future development of a prognostics and health management system that supports predictive maintenance.",
      "authors": [
        "Federico Incardona",
        "Alessandro Costa",
        "Farida Farsian",
        "Francesco Franchina",
        "Giuseppe Leto",
        "Emilio Mastriani",
        "Kevin Munari",
        "Giovanni Pareschi",
        "Salvatore Scuderi",
        "Sebastiano Spinello",
        "Gino Tosti"
      ],
      "url": "https://arxiv.org/abs/2602.19984",
      "published": "2026-02-23T15:51:50+00:00",
      "categories": [
        "astro-ph.IM",
        "astro-ph.HE",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19983",
      "title": "Contextual Safety Reasoning and Grounding for Open-World Robots",
      "abstract": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE.",
      "authors": [
        "Zachary Ravichandran",
        "David Snyder",
        "Alexander Robey",
        "Hamed Hassani",
        "Vijay Kumar",
        "George J. Pappas"
      ],
      "url": "https://arxiv.org/abs/2602.19983",
      "published": "2026-02-23T15:51:23+00:00",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19982",
      "title": "A Computationally Efficient Multidimensional Vision Transformer",
      "abstract": "Vision Transformers have achieved state-of-the-art performance in a wide range   of computer vision tasks, but their practical deployment is limited by high   computational and memory costs. In this paper, we introduce a novel tensor-based   framework for Vision Transformers built upon the Tensor Cosine Product   (Cproduct). By exploiting multilinear structures inherent in image data and the   orthogonality of cosine transforms, the proposed approach enables efficient   attention mechanisms and structured feature representations. We develop the   theoretical foundations of the tensor cosine product, analyze its algebraic   properties, and integrate it into a new Cproduct-based Vision Transformer   architecture (TCP-ViT). Numerical experiments on standard classification and   segmentation benchmarks demonstrate that the proposed method achieves a uniform   1/C parameter reduction (where C is the number of channels) while   maintaining competitive accuracy.",
      "authors": [
        "Alaa El Ichi",
        "Khalide Jbilou"
      ],
      "url": "https://arxiv.org/abs/2602.19982",
      "published": "2026-02-23T15:49:46+00:00",
      "categories": [
        "cs.LG",
        "math.NA"
      ]
    },
    {
      "id": "2602.19980",
      "title": "Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks",
      "abstract": "While Autoregressive (AR) Transformer-based Generative Language Models are frequently employed for lookahead tasks, recent research suggests a potential discrepancy in their ability to perform planning tasks that require multi-step lookahead. In this work, we investigate the distinct emergent mechanisms that arise when training AR versus Non-Autoregressive (NAR) models, such as Discrete Diffusion Models (dLLMs), on lookahead tasks. By requiring the models to plan ahead to reach the correct conclusion, we analyze how these two paradigms fundamentally differ in their approach to the problem. We identify a critical asymmetry in planning problems: while forward generation requires complex lookahead at branching junctions, reverse generation is often deterministic. This asymmetry creates an opportunity for NAR models. Through mechanistic analysis of training and inference dynamics, we demonstrate that NAR models learn to solve planning tasks by utilizing future tokens to decode backwards, avoiding the need to learn complex traversal mechanisms entirely. Consequently, we report that both AR and NAR models are able to achieve perfect accuracy on the lookahead task. However, NAR models require exponentially fewer training examples and shallower architectures compared to AR models, which often fail to converge without specific curriculum adjustments.",
      "authors": [
        "Itamar Trainin",
        "Shauli Ravfogel",
        "Omri Abend",
        "Amir Feder"
      ],
      "url": "https://arxiv.org/abs/2602.19980",
      "published": "2026-02-23T15:47:27+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19969",
      "title": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting",
      "abstract": "The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \\textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.",
      "authors": [
        "Yuxing Tian",
        "Fengran Mo",
        "Weixu Zhang",
        "Yiyan Qi",
        "Jian-Yun Nie"
      ],
      "url": "https://arxiv.org/abs/2602.19969",
      "published": "2026-02-23T15:30:52+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19967",
      "title": "Unlearning Noise in PINNs: A Selective Pruning Framework for PDE Inverse Problems",
      "abstract": "Physics-informed neural networks (PINNs) provide a promising framework for solving inverse problems governed by partial differential equations (PDEs) by integrating observational data and physical constraints in a unified optimization objective. However, the ill-posed nature of PDE inverse problems makes them highly sensitive to noise. Even a small fraction of corrupted observations can distort internal neural representations, severely impairing accuracy and destabilizing training. Motivated by recent advances in machine unlearning and structured network pruning, we propose P-PINN, a selective pruning framework designed to unlearn the influence of corrupted data in a pretrained PINN. Specifically, starting from a PINN trained on the full dataset, P-PINN evaluates a joint residual--data fidelity indicator, a weighted combination of data misfit and PDE residuals, to partition the training set into reliable and corrupted subsets. Next, we introduce a bias-based neuron importance measure that quantifies directional activation discrepancies between the two subsets, identifying neurons whose representations are predominantly driven by corrupted samples. Building on this, an iterative pruning strategy then removes noise-sensitive neurons layer by layer. The resulting pruned network is fine-tuned on the reliable data subject to the original PDE constraints, acting as a lightweight post-processing stage rather than a complete retraining. Numerical experiments on extensive PDE inverse-problem benchmarks demonstrate that P-PINN substantially improves robustness, accuracy, and training stability under noisy conditions, achieving up to a 96.6\\% reduction in relative error compared with baseline PINNs. These results indicate that activation-level post hoc pruning is a promising mechanism for enhancing the reliability of physics-informed learning in noise-contaminated settings.",
      "authors": [
        "Yongsheng Chen",
        "Yong Chen",
        "Wei Guo",
        "Xinghui Zhong"
      ],
      "url": "https://arxiv.org/abs/2602.19967",
      "published": "2026-02-23T15:29:50+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19964",
      "title": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference",
      "abstract": "Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \\textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods.",
      "authors": [
        "Moritz A. Zanger",
        "Yijun Wu",
        "Pascal R. Van der Vaart",
        "Wendelin Böhmer",
        "Matthijs T. J. Spaan"
      ],
      "url": "https://arxiv.org/abs/2602.19964",
      "published": "2026-02-23T15:28:27+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.PR",
        "stat.ML"
      ]
    },
    {
      "id": "2602.19961",
      "title": "Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval",
      "abstract": "With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through the lens of the Multimodal Large Language Model (MLLM) era. We begin by examining the benchmark landscape, and subsequently dive into the methodological evolution, categorizing approaches into three primary aspects: multimodal embedding models, multimodal reranker models, and the integration of Retrieval-Augmented Generation (RAG) and Agentic systems for complex document intelligence. Finally, we identify persistent challenges and outline promising future directions, aiming to provide a clear roadmap for future multimodal document intelligence.",
      "authors": [
        "Yibo Yan",
        "Jiahao Huo",
        "Guanbo Feng",
        "Mingdong Ou",
        "Yi Cao",
        "Xin Zou",
        "Shuliang Liu",
        "Yuanhuiyi Lyu",
        "Yu Huang",
        "Jungang Li",
        "Kening Zheng",
        "Xu Zheng",
        "Philip S. Yu",
        "James Kwok",
        "Xuming Hu"
      ],
      "url": "https://arxiv.org/abs/2602.19961",
      "published": "2026-02-23T15:27:41+00:00",
      "categories": [
        "cs.CL",
        "cs.IR"
      ]
    },
    {
      "id": "2602.19956",
      "title": "Sparse Masked Attention Policies for Reliable Generalization",
      "abstract": "In reinforcement learning, abstraction methods that remove unnecessary information from the observation are commonly used to learn policies which generalize better to unseen tasks. However, these methods often overlook a crucial weakness: the function which extracts the reduced-information representation has unknown generalization ability in unseen observations. In this paper, we address this problem by presenting an information removal method which more reliably generalizes to new states. We accomplish this by using a learned masking function which operates on, and is integrated with, the attention weights within an attention-based policy network. We demonstrate that our method significantly improves policy generalization to unseen tasks in the Procgen benchmark compared to standard PPO and masking approaches.",
      "authors": [
        "Caroline Horsch",
        "Laurens Engwegen",
        "Max Weltevrede",
        "Matthijs T. J. Spaan",
        "Wendelin Böhmer"
      ],
      "url": "https://arxiv.org/abs/2602.19956",
      "published": "2026-02-23T15:23:17+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19948",
      "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming",
      "abstract": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.   Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.",
      "authors": [
        "Ian Steenstra",
        "Paola Pedrelli",
        "Weiyan Shi",
        "Stacy Marsella",
        "Timothy W. Bickmore"
      ],
      "url": "https://arxiv.org/abs/2602.19948",
      "published": "2026-02-23T15:17:18+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.MA"
      ]
    },
    {
      "id": "2602.19946",
      "title": "When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators",
      "abstract": "Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data, and evaluate them on real test data. Despite observable advances in visual fidelity and prompt adherence, classification accuracy on real test data consistently declines with newer T2I models as training data generators. Our analysis reveals a hidden trend: These models collapse to a narrow, aesthetic-centric distribution that undermines diversity and label-image alignment. Overall, our findings challenge a growing assumption in vision research, namely that progress in generative realism implies progress in data realism. We thus highlight an urgent need to rethink the capabilities of modern T2I models as reliable training data generators.",
      "authors": [
        "Krzysztof Adamkiewicz",
        "Brian Moser",
        "Stanislav Frolov",
        "Tobias Christian Nauen",
        "Federico Raue",
        "Andreas Dengel"
      ],
      "url": "https://arxiv.org/abs/2602.19946",
      "published": "2026-02-23T15:15:53+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19945",
      "title": "DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models",
      "abstract": "Balancing convergence efficiency and robustness under Differential Privacy (DP) is a central challenge in Federated Learning (FL). While AdamW accelerates training and fine-tuning in large-scale models, we find that directly applying it to Differentially Private FL (DPFL) suffers from three major issues: (i) data heterogeneity and privacy noise jointly amplify the variance of second-moment estimator, (ii) DP perturbations bias the second-moment estimator, and (iii) DP amplify AdamW sensitivity to local overfitting, worsening client drift. We propose DP-FedAdamW, the first AdamW-based optimizer for DPFL. It restores AdamW under DP by stabilizing second-moment variance, removing DP-induced bias, and aligning local updates to the global descent to curb client drift. Theoretically, we establish an unbiased second-moment estimator and prove a linearly accelerated convergence rate without any heterogeneity assumption, while providing tighter $(\\varepsilon,δ)$-DP guarantees. Our empirical results demonstrate the effectiveness of DP-FedAdamW across language and vision Transformers and ResNet-18. On Tiny-ImageNet (Swin-Base, $\\varepsilon=1$), DP-FedAdamW outperforms the state-of-the-art (SOTA) by 5.83\\%. The code is available in Appendix.",
      "authors": [
        "Jin Liu",
        "Yinbin Miao",
        "Ning Xi",
        "Junkang Liu"
      ],
      "url": "https://arxiv.org/abs/2602.19945",
      "published": "2026-02-23T15:15:47+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19938",
      "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
      "abstract": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.   We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.   We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference.",
      "authors": [
        "Zijie Liu",
        "Jie Peng",
        "Jinhao Duan",
        "Zirui Liu",
        "Kaixiong Zhou",
        "Mingfu Liang",
        "Luke Simon",
        "Xi Liu",
        "Zhaozhuo Xu",
        "Tianlong Chen"
      ],
      "url": "https://arxiv.org/abs/2602.19938",
      "published": "2026-02-23T15:11:16+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19931",
      "title": "Expanding the Role of Diffusion Models for Robust Classifier Training",
      "abstract": "Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations that are both diverse and partially robust, and that explicitly incorporating diffusion representations as an auxiliary learning signal during AT consistently improves robustness across settings. Furthermore, our representation analysis indicates that incorporating diffusion models into AT encourages more disentangled features, while diffusion representations and diffusion-generated synthetic data play complementary roles in shaping representations. Experiments on CIFAR-10, CIFAR-100, and ImageNet validate these findings, demonstrating the effectiveness of jointly leveraging diffusion representations and synthetic data within AT.",
      "authors": [
        "Pin-Han Huang",
        "Shang-Tse Chen",
        "Hsuan-Tien Lin"
      ],
      "url": "https://arxiv.org/abs/2602.19931",
      "published": "2026-02-23T15:06:52+00:00",
      "categories": [
        "cs.LG",
        "cs.CV"
      ]
    },
    {
      "id": "2602.19930",
      "title": "Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning",
      "abstract": "Imitation learning stands at a crossroads: despite decades of progress, current imitation learning agents remain sophisticated memorisation machines, excelling at replay but failing when contexts shift or goals evolve. This paper argues that this failure is not technical but foundational: imitation learning has been optimised for the wrong objective. We propose a research agenda that redefines success from perfect replay to compositional adaptability. Such adaptability hinges on learning behavioural primitives once and recombining them through novel contexts without retraining. We establish metrics for compositional generalisation, propose hybrid architectures, and outline interdisciplinary research directions drawing on cognitive science and cultural evolution. Agents that embed adaptability at the core of imitation learning thus have an essential capability for operating in an open-ended world.",
      "authors": [
        "Nathan Gavenski",
        "Felipe Meneguzzi",
        "Odinaldo Rodrigues"
      ],
      "url": "https://arxiv.org/abs/2602.19930",
      "published": "2026-02-23T15:06:33+00:00",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19926",
      "title": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models",
      "abstract": "Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\\textbf{L}ocal \\textbf{A}lternating \\textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($ε= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\\% in test accuracy. Code is provided in \\repolink.",
      "authors": [
        "Jin Liu",
        "Yinbin Miao",
        "Ning Xi",
        "Junkang Liu"
      ],
      "url": "https://arxiv.org/abs/2602.19926",
      "published": "2026-02-23T15:05:28+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}