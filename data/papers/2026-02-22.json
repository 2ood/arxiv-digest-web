{
  "date": "2026-02-22",
  "fetched_at": "2026-02-27T09:27:20.977677+09:00",
  "papers": [
    {
      "id": "2602.19215",
      "title": "Understanding Empirical Unlearning with Combinatorial Interpretability",
      "abstract": "While many recent methods aim to unlearn or remove knowledge from pretrained models, seemingly erased knowledge often persists and can be recovered in various ways. Because large foundation models are far from interpretable, understanding whether and how such knowledge persists remains a significant challenge. To address this, we turn to the recently developed framework of combinatorial interpretability. This framework, designed for two-layer neural networks, enables direct inspection of the knowledge encoded in the model weights. We reproduce baseline unlearning methods within the combinatorial interpretability setting and examine their behavior along two dimensions: (i) whether they truly remove knowledge of a target concept (the concept we wish to remove) or merely inhibit its expression while retaining the underlying information, and (ii) how easily the supposedly erased knowledge can be recovered through various fine-tuning operations. Our results shed light within a fully interpretable setting on how knowledge can persist despite unlearning and when it might resurface.",
      "authors": [
        "Shingo Kodama",
        "Niv Cohen",
        "Micah Adler",
        "Nir Shavit"
      ],
      "url": "https://arxiv.org/abs/2602.19215",
      "published": "2026-02-22T14:51:48+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19212",
      "title": "Retrieval Augmented Enhanced Dual Co-Attention Framework for Target Aware Multimodal Bengali Hateful Meme Detection",
      "abstract": "Hateful content on social media increasingly appears as multimodal memes that combine images and text to convey harmful narratives. In low-resource languages such as Bengali, automated detection remains challenging due to limited annotated data, class imbalance, and pervasive code-mixing. To address these issues, we augment the Bengali Hateful Memes (BHM) dataset with semantically aligned samples from the Multimodal Aggression Dataset in Bengali (MIMOSA), improving both class balance and semantic diversity. We propose the Enhanced Dual Co-attention Framework (xDORA), integrating vision encoders (CLIP, DINOv2) and multilingual text encoders (XGLM, XLM-R) via weighted attention pooling to learn robust cross-modal representations. Building on these embeddings, we develop a FAISS-based k-nearest neighbor classifier for non-parametric inference and introduce RAG-Fused DORA, which incorporates retrieval-driven contextual reasoning. We further evaluate LLaVA under zero-shot, few-shot, and retrieval-augmented prompting settings. Experiments on the extended dataset show that xDORA (CLIP + XLM-R) achieves macro-average F1-scores of 0.78 for hateful meme identification and 0.71 for target entity detection, while RAG-Fused DORA improves performance to 0.79 and 0.74, yielding gains over the DORA baseline. The FAISS-based classifier performs competitively and demonstrates robustness for rare classes through semantic similarity modeling. In contrast, LLaVA exhibits limited effectiveness in few-shot settings, with only modest improvements under retrieval augmentation, highlighting constraints of pretrained vision-language models for code-mixed Bengali content without fine-tuning. These findings demonstrate the effectiveness of supervised, retrieval-augmented, and non-parametric multimodal frameworks for addressing linguistic and cultural complexities in low-resource hate speech detection.",
      "authors": [
        "Raihan Tanvir",
        "Md. Golam Rabiul Alam"
      ],
      "url": "https://arxiv.org/abs/2602.19212",
      "published": "2026-02-22T14:48:25+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.19208",
      "title": "How to Allocate, How to Learn? Dynamic Rollout Allocation and Advantage Modulation for Policy Optimization",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for Large Language Model (LLM) reasoning, yet current methods face key challenges in resource allocation and policy optimization dynamics: (i) uniform rollout allocation ignores gradient variance heterogeneity across problems, and (ii) the softmax policy structure causes gradient attenuation for high-confidence correct actions, while excessive gradient updates may destabilize training. Therefore, we propose DynaMO, a theoretically-grounded dual-pronged optimization framework. At the sequence level, we prove that uniform allocation is suboptimal and derive variance-minimizing allocation from the first principle, establishing Bernoulli variance as a computable proxy for gradient informativeness. At the token level, we develop gradient-aware advantage modulation grounded in theoretical analysis of gradient magnitude bounds. Our framework compensates for gradient attenuation of high-confidence correct actions while utilizing entropy changes as computable indicators to stabilize excessive update magnitudes. Extensive experiments conducted on a diverse range of mathematical reasoning benchmarks demonstrate consistent improvements over strong RLVR baselines. Our implementation is available at: \\href{https://anonymous.4open.science/r/dynamo-680E/README.md}{https://anonymous.4open.science/r/dynamo}.",
      "authors": [
        "Yangyi Fang",
        "Jiaye Lin",
        "Xiaoliang Fu",
        "Cong Qin",
        "Haolin Shi",
        "Chaowen Hu",
        "Lu Pan",
        "Ke Zeng",
        "Xunliang Cai"
      ],
      "url": "https://arxiv.org/abs/2602.19208",
      "published": "2026-02-22T14:38:24+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19207",
      "title": "HybridFL: A Federated Learning Approach for Financial Crime Detection",
      "abstract": "Federated learning (FL) is a privacy-preserving machine learning paradigm that enables multiple parties to collaboratively train models on privately owned data without sharing raw information. While standard FL typically addresses either horizontal or vertical data partitions, many real-world scenarios exhibit a complex hybrid distribution. This paper proposes Hybrid Federated Learning (HybridFL) to address data split both horizontally across disjoint users and vertically across complementary feature sets. We evaluate HybridFL in a financial crime detection context, where a transaction party holds transaction-level attributes and multiple banks maintain private account-level features. By integrating horizontal aggregation and vertical feature fusion, the proposed architecture enables joint learning while strictly preserving data locality. Experiments on AMLSim and SWIFT datasets demonstrate that HybridFL significantly outperforms the transaction-only local model and achieves performance comparable to a centralized benchmark.",
      "authors": [
        "Afsana Khan",
        "Marijn ten Thij",
        "Guangzhi Tang",
        "Anna Wilbik"
      ],
      "url": "https://arxiv.org/abs/2602.19207",
      "published": "2026-02-22T14:36:24+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19196",
      "title": "An Interpretable Data-Driven Model of the Flight Dynamics of Hawks",
      "abstract": "Despite significant analysis of bird flight, generative physics models for flight dynamics do not currently exist. Yet the underlying mechanisms responsible for various flight manoeuvres are important for understanding how agile flight can be accomplished. Even in a simple flight, multiple objectives are at play, complicating analysis of the overall flight mechanism. Using the data-driven method of dynamic mode decomposition (DMD) on motion capture recordings of hawks, we show that multiple behavioral states such as flapping, turning, landing, and gliding, can be modeled by simple and interpretable modal structures (i.e. the underlying wing-tail shape) which can be linearly combined to reproduce the experimental flight observations. Moreover, the DMD model can be used to extrapolate naturalistic flapping. Flight is highly individual, with differences in style across the hawks, but we find they share a common set of dynamic modes. The DMD model is a direct fit to data, unlike traditional models constructed from physics principles which can rarely be tested on real data and whose assumptions are typically invalid in real flight. The DMD approach gives a highly accurate reconstruction of the flight dynamics with only three parameters needed to characterize flapping, and a fourth to integrate turning manoeuvres. The DMD analysis further shows that the underlying mechanism of flight, much like simplest walking models, displays a parametric coupling between dominant modes suggesting efficiency for locomotion.",
      "authors": [
        "Lydia France",
        "Karl Lapo",
        "J. Nathan Kutz"
      ],
      "url": "https://arxiv.org/abs/2602.19196",
      "published": "2026-02-22T13:58:03+00:00",
      "categories": [
        "q-bio.QM",
        "cs.CE",
        "cs.LG",
        "physics.flu-dyn"
      ]
    },
    {
      "id": "2602.19193",
      "title": "Visual Prompt Guided Unified Pushing Policy",
      "abstract": "As one of the simplest non-prehensile manipulation skills, pushing has been widely studied as an effective means to rearrange objects. Existing approaches, however, typically rely on multi-step push plans composed of pre-defined pushing primitives with limited application scopes, which restrict their efficiency and versatility across different scenarios. In this work, we propose a unified pushing policy that incorporates a lightweight prompting mechanism into a flow matching policy to guide the generation of reactive, multimodal pushing actions. The visual prompt can be specified by a high-level planner, enabling the reuse of the pushing policy across a wide range of planning problems. Experimental results demonstrate that the proposed unified pushing policy not only outperforms existing baselines but also effectively serves as a low-level primitive within a VLM-guided planning framework to solve table-cleaning tasks efficiently.",
      "authors": [
        "Hieu Bui",
        "Ziyan Gao",
        "Yuya Hosoda",
        "Joo-Ho Lee"
      ],
      "url": "https://arxiv.org/abs/2602.19193",
      "published": "2026-02-22T13:48:38+00:00",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19190",
      "title": "FUSAR-GPT : A Spatiotemporal Feature-Embedded and Two-Stage Decoupled Visual Language Model for SAR Imagery",
      "abstract": "Research on the intelligent interpretation of all-weather, all-time Synthetic Aperture Radar (SAR) is crucial for advancing remote sensing applications. In recent years, although Visual Language Models (VLMs) have demonstrated strong open-world understanding capabilities on RGB images, their performance is severely limited when directly applied to the SAR field due to the complexity of the imaging mechanism, sensitivity to scattering features, and the scarcity of high-quality text corpora. To systematically address this issue, we constructed the inaugural SAR Image-Text-AlphaEarth feature triplet dataset and developed FUSAR-GPT, a VLM specifically for SAR. FUSAR-GPT innovatively introduces a geospatial baseline model as a 'world knowledge' prior and embeds multi-source remote-sensing temporal features into the model's visual backbone via 'spatiotemporal anchors', enabling dynamic compensation for the sparse representation of targets in SAR images. Furthermore, we designed a two-stage SFT strategy to decouple the knowledge injection and task execution of large models. The spatiotemporal feature embedding and the two-stage decoupling paradigm enable FUSAR-GPT to achieve state-of-the-art performance across several typical remote sensing visual-language benchmark tests, significantly outperforming mainstream baseline models by over 12%.",
      "authors": [
        "Xiaokun Zhang",
        "Yi Yang",
        "Ziqi Ye",
        "Baiyun",
        "Xiaorong Guo",
        "Qingchen Fang",
        "Ruyi Zhang",
        "Xinpeng Zhou",
        "Haipeng Wang"
      ],
      "url": "https://arxiv.org/abs/2602.19190",
      "published": "2026-02-22T13:40:17+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19187",
      "title": "Adaptive Problem Generation via Symbolic Representations",
      "abstract": "We present a method for generating training data for reinforcement learning with verifiable rewards to improve small open-weights language models on mathematical tasks. Existing data generation approaches rely on open-loop pipelines and fixed modifications that do not adapt to the model's capabilities. Furthermore, they typically operate directly on word problems, limiting control over problem structure. To address this, we perform modifications in a symbolic problem space, representing each problem as a set of symbolic variables and constraints (e.g., via algebraic frameworks such as SymPy or SMT formulations). This representation enables precise control over problem structure, automatic generation of ground-truth solutions, and decouples mathematical reasoning from linguistic realization. We also show that this results in more diverse generations. To adapt the problem difficulty to the model, we introduce a closed-loop framework that learns modification strategies through prompt optimization in symbolic space. Experimental results demonstrate that both adaptive problem generation and symbolic representation modifications contribute to improving the model's math solving ability.",
      "authors": [
        "Teresa Yeo",
        "Myeongho Jeon",
        "Dulaj Weerakoon",
        "Rui Qiao",
        "Alok Prakash",
        "Armando Solar-Lezama",
        "Archan Misra"
      ],
      "url": "https://arxiv.org/abs/2602.19187",
      "published": "2026-02-22T13:33:48+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19177",
      "title": "Next Reply Prediction X Dataset: Linguistic Discrepancies in Naively Generated Content",
      "abstract": "The increasing use of Large Language Models (LLMs) as proxies for human participants in social science research presents a promising, yet methodologically risky, paradigm shift. While LLMs offer scalability and cost-efficiency, their \"naive\" application, where they are prompted to generate content without explicit behavioral constraints, introduces significant linguistic discrepancies that challenge the validity of research findings. This paper addresses these limitations by introducing a novel, history-conditioned reply prediction task on authentic X (formerly Twitter) data, to create a dataset designed to evaluate the linguistic output of LLMs against human-generated content. We analyze these discrepancies using stylistic and content-based metrics, providing a quantitative framework for researchers to assess the quality and authenticity of synthetic data. Our findings highlight the need for more sophisticated prompting techniques and specialized datasets to ensure that LLM-generated content accurately reflects the complex linguistic patterns of human communication, thereby improving the validity of computational social science studies.",
      "authors": [
        "Simon Münker",
        "Nils Schwager",
        "Kai Kugler",
        "Michael Heseltine",
        "Achim Rettinger"
      ],
      "url": "https://arxiv.org/abs/2602.19177",
      "published": "2026-02-22T13:14:27+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19174",
      "title": "TurkicNLP: An NLP Toolkit for Turkic Languages",
      "abstract": "Natural language processing for the Turkic language family, spoken by over 200 million people across Eurasia, remains fragmented, with most languages lacking unified tooling and resources. We present TurkicNLP, an open-source Python library providing a single, consistent NLP pipeline for Turkic languages across four script families: Latin, Cyrillic, Perso-Arabic, and Old Turkic Runic. The library covers tokenization, morphological analysis, part-of-speech tagging, dependency parsing, named entity recognition, bidirectional script transliteration, cross-lingual sentence embeddings, and machine translation through one language-agnostic API. A modular multi-backend architecture integrates rule-based finite-state transducers and neural models transparently, with automatic script detection and routing between script variants. Outputs follow the CoNLL-U standard for full interoperability and extension. Code and documentation are hosted at https://github.com/turkic-nlp/turkicnlp .",
      "authors": [
        "Sherzod Hakimov"
      ],
      "url": "https://arxiv.org/abs/2602.19174",
      "published": "2026-02-22T13:08:21+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.19172",
      "title": "Online Realizable Regression and Applications for ReLU Networks",
      "abstract": "Realizable online regression can behave very differently from online classification. Even without any margin or stochastic assumptions, realizability may enforce horizon-free (finite) cumulative loss under metric-like losses, even when the analogous classification problem has an infinite mistake bound. We study realizable online regression in the adversarial model under losses that satisfy an approximate triangle inequality (approximate pseudo-metrics). Recent work of Attias et al. shows that the minimax realizable cumulative loss is characterized by the scaled Littlestone/online dimension $\\mathbb{D}_{\\mathrm{onl}}$, but this quantity can be difficult to analyze.   Our main contribution is a generic potential method that upper bounds $\\mathbb{D}_{\\mathrm{onl}}$ by a concrete Dudley-type entropy integral that depends only on covering numbers of the hypothesis class under the induced sup pseudo-metric. We define an \\emph{entropy potential} $Φ(\\mathcal{H})=\\int_{0}^{diam(\\mathcal{H})} \\log N(\\mathcal{H},\\varepsilon)\\,d\\varepsilon$, where $N(\\mathcal{H},\\varepsilon)$ is the $\\varepsilon$-covering number of $\\mathcal{H}$, and show that for every $c$-approximate pseudo-metric loss, $\\mathbb{D}_{\\mathrm{onl}}(\\mathcal{H})\\le O(c)\\,Φ(\\mathcal{H})$. In particular, polynomial metric entropy implies $Φ(\\mathcal{H})<\\infty$ and hence a horizon-free realizable cumulative-loss bound with transparent dependence on effective dimension.   We illustrate the method on two families. We prove a sharp $q$-vs.-$d$ dichotomy for realizable online learning (finite and efficiently achievable $Θ_{d,q}(L^d)$ total loss for $L$-Lipschitz regression iff $q>d$, otherwise infinite), and for bounded-norm $k$-ReLU networks separate regression (finite loss, even $\\widetilde O(k^2)$, and $O(1)$ for one ReLU) from classification (impossible already for $k=2,d=1$).",
      "authors": [
        "Ilan Doron-Arad",
        "Idan Mehalel",
        "Elchanan Mossel"
      ],
      "url": "https://arxiv.org/abs/2602.19172",
      "published": "2026-02-22T13:02:25+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19166",
      "title": "CosyAccent: Duration-Controllable Accent Normalization Using Source-Synthesis Training Data",
      "abstract": "Accent normalization (AN) systems often struggle with unnatural outputs and undesired content distortion, stemming from both suboptimal training data and rigid duration modeling. In this paper, we propose a \"source-synthesis\" methodology for training data construction. By generating source L2 speech and using authentic native speech as the training target, our approach avoids learning from TTS artifacts and, crucially, requires no real L2 data in training. Alongside this data strategy, we introduce CosyAccent, a non-autoregressive model that resolves the trade-off between prosodic naturalness and duration control. CosyAccent implicitly models rhythm for flexibility yet offers explicit control over total output duration. Experiments show that, despite being trained without any real L2 speech, CosyAccent achieves significantly improved content preservation and superior naturalness compared to strong baselines trained on real-world data.",
      "authors": [
        "Qibing Bai",
        "Shuhao Shi",
        "Shuai Wang",
        "Yukai Ju",
        "Yannan Wang",
        "Haizhou Li"
      ],
      "url": "https://arxiv.org/abs/2602.19166",
      "published": "2026-02-22T12:52:05+00:00",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ]
    },
    {
      "id": "2602.19160",
      "title": "Reasoning Capabilities of Large Language Models. Lessons Learned from General Game Playing",
      "abstract": "This paper examines the reasoning capabilities of Large Language Models (LLMs) from a novel perspective, focusing on their ability to operate within formally specified, rule-governed environments. We evaluate four LLMs (Gemini 2.5 Pro and Flash variants, Llama 3.3 70B and GPT-OSS 120B) on a suite of forward-simulation tasks-including next / multistep state formulation, and legal action generation-across a diverse set of reasoning problems illustrated through General Game Playing (GGP) game instances. Beyond reporting instance-level performance, we characterize games based on 40 structural features and analyze correlations between these features and LLM performance. Furthermore, we investigate the effects of various game obfuscations to assess the role of linguistic semantics in game definitions and the impact of potential prior exposure of LLMs to specific games during training. The main results indicate that three of the evaluated models generally perform well across most experimental settings, with performance degradation observed as the evaluation horizon increases (i.e., with a higher number of game steps). Detailed case-based analysis of the LLM performance provides novel insights into common reasoning errors in the considered logic-based problem formulation, including hallucinated rules, redundant state facts, or syntactic errors. Overall, the paper reports clear progress in formal reasoning capabilities of contemporary models.",
      "authors": [
        "Maciej Świechowski",
        "Adam Żychowski",
        "Jacek Mańdziuk"
      ],
      "url": "https://arxiv.org/abs/2602.19160",
      "published": "2026-02-22T12:43:00+00:00",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ]
    },
    {
      "id": "2602.19159",
      "title": "Beyond Behavioural Trade-Offs: Mechanistic Tracing of Pain-Pleasure Decisions in an LLM",
      "abstract": "Prior behavioural work suggests that some LLMs alter choices when options are framed as causing pain or pleasure, and that such deviations can scale with stated intensity. To bridge behavioural evidence (what the model does) with mechanistic interpretability (what computations support it), we investigate how valence-related information is represented and where it is causally used inside a transformer. Using Gemma-2-9B-it and a minimalist decision task modelled on prior work, we (i) map representational availability with layer-wise linear probing across streams, (ii) test causal contribution with activation interventions (steering; patching/ablation), and (iii) quantify dose-response effects over an epsilon grid, reading out both the 2-3 logit margin and digit-pair-normalised choice probabilities. We find that (a) valence sign (pain vs. pleasure) is perfectly linearly separable across stream families from very early layers (L0-L1), while a lexical baseline retains substantial signal; (b) graded intensity is strongly decodable, with peaks in mid-to-late layers and especially in attention/MLP outputs, and decision alignment is highest slightly before the final token; (c) additive steering along a data-derived valence direction causally modulates the 2-3 margin at late sites, with the largest effects observed in late-layer attention outputs (attn_out L14); and (d) head-level patching/ablation suggests that these effects are distributed across multiple heads rather than concentrated in a single unit. Together, these results link behavioural sensitivity to identifiable internal representations and intervention-sensitive sites, providing concrete mechanistic targets for more stringent counterfactual tests and broader replication. This work supports a more evidence-driven (a) debate on AI sentience and welfare, and (b) governance when setting policy, auditing standards, and safety safeguards.",
      "authors": [
        "Francesca Bianco",
        "Derek Shiller"
      ],
      "url": "https://arxiv.org/abs/2602.19159",
      "published": "2026-02-22T12:42:38+00:00",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19158",
      "title": "DoAtlas-1: A Causal Compilation Paradigm for Clinical AI",
      "abstract": "Medical foundation models generate narrative explanations but cannot quantify intervention effects, detect evidence conflicts, or validate literature claims, limiting clinical auditability. We propose causal compilation, a paradigm that transforms medical evidence from narrative text into executable code. The paradigm standardizes heterogeneous research evidence into structured estimand objects, each explicitly specifying intervention contrast, effect scale, time horizon, and target population, supporting six executable causal queries: do-calculus, counterfactual reasoning, temporal trajectories, heterogeneous effects, mechanistic decomposition, and joint interventions. We instantiate this paradigm in DoAtlas-1, compiling 1,445 effect kernels from 754 studies through effect standardization, conflict-aware graph construction, and real-world validation (Human Phenotype Project, 10,000 participants). The system achieves 98.5% canonicalization accuracy and 80.5% query executability. This paradigm shifts medical AI from text generation to executable, auditable, and verifiable causal reasoning.",
      "authors": [
        "Yulong Li",
        "Jianxu Chen",
        "Xiwei Liu",
        "Chuanyue Suo",
        "Rong Xia",
        "Zhixiang Lu",
        "Yichen Li",
        "Xinlin Zhuang",
        "Niranjana Arun Menon",
        "Yutong Xie",
        "Eran Segal",
        "Imran Razzak"
      ],
      "url": "https://arxiv.org/abs/2602.19158",
      "published": "2026-02-22T12:40:52+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.19157",
      "title": "Facet-Level Persona Control by Trait-Activated Routing with Contrastive SAE for Role-Playing LLMs",
      "abstract": "Personality control in Role-Playing Agents (RPAs) is commonly achieved via training-free methods that inject persona descriptions and memory through prompts or retrieval-augmented generation, or via supervised fine-tuning (SFT) on persona-specific corpora. While SFT can be effective, it requires persona-labeled data and retraining for new roles, limiting flexibility. In contrast, prompt- and RAG-based signals are easy to apply but can be diluted in long dialogues, leading to drifting and sometimes inconsistent persona behavior. To address this, we propose a contrastive Sparse AutoEncoder (SAE) framework that learns facet-level personality control vectors aligned with the Big Five 30-facet model. A new 15,000-sample leakage-controlled corpus is constructed to provide balanced supervision for each facet. The learned vectors are integrated into the model's residual space and dynamically selected by a trait-activated routing module, enabling precise and interpretable personality steering. Experiments on Large Language Models (LLMs) show that the proposed method maintains stable character fidelity and output quality across contextualized settings, outperforming Contrastive Activation Addition (CAA) and prompt-only baselines. The combined SAE+Prompt configuration achieves the best overall performance, confirming that contrastively trained latent vectors can enhance persona control while preserving dialogue coherence.",
      "authors": [
        "Wenqiu Tang",
        "Zhen Wan",
        "Takahiro Komamizu",
        "Ichiro Ide"
      ],
      "url": "https://arxiv.org/abs/2602.19157",
      "published": "2026-02-22T12:39:02+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.19156",
      "title": "Artefact-Aware Fungal Detection in Dermatophytosis: A Real-Time Transformer-Based Approach for KOH Microscopy",
      "abstract": "Dermatophytosis is commonly assessed using potassium hydroxide (KOH) microscopy, yet accurate recognition of fungal hyphae is hindered by artefacts, heterogeneous keratin clearance, and notable inter-observer variability. This study presents a transformer-based detection framework using the RT-DETR model architecture to achieve precise, query-driven localization of fungal structures in high-resolution KOH images. A dataset of 2,540 routinely acquired microscopy images was manually annotated using a multi-class strategy to explicitly distinguish fungal elements from confounding artefacts. The model was trained with morphology-preserving augmentations to maintain the structural integrity of thin hyphae. Evaluation on an independent test set demonstrated robust object-level performance, with a recall of 0.9737, precision of 0.8043, and an AP@0.50 of 93.56%. When aggregated for image-level diagnosis, the model achieved 100% sensitivity and 98.8% accuracy, correctly identifying all positive cases without missing a single diagnosis. Qualitative outputs confirmed the robust localization of low-contrast hyphae even in artefact-rich fields. These results highlight that an artificial intelligence (AI) system can serve as a highly reliable, automated screening tool, effectively bridging the gap between image-level analysis and clinical decision-making in dermatomycology.",
      "authors": [
        "Rana Gursoy",
        "Abdurrahim Yilmaz",
        "Baris Kizilyaprak",
        "Esmahan Caglar",
        "Burak Temelkuran",
        "Huseyin Uvet",
        "Ayse Esra Koku Aksu",
        "Gulsum Gencoglan"
      ],
      "url": "https://arxiv.org/abs/2602.19156",
      "published": "2026-02-22T12:35:17+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19153",
      "title": "Constrained Diffusion for Accelerated Structure Relaxation of Inorganic Solids with Point Defects",
      "abstract": "Point defects affect material properties by altering electronic states and modifying local bonding environments. However, high-throughput first-principles simulations of point defects are costly due to large simulation cells and complex energy landscapes. To this end, we propose a generative framework for simulating point defects, overcoming the limits of costly first-principles simulators. By leveraging a primal-dual algorithm, we introduce a constraint-aware diffusion model which outperforms existing constrained diffusion approaches in this domain. Across six defect configuration settings for Bi2Te3, the proposed approach provides state-of-the-art performance generating physically grounded structures.",
      "authors": [
        "Jingyi Cui",
        "Jacob K. Christopher",
        "Ankita Biswas",
        "Prasanna V. Balachandran",
        "Ferdinando Fioretto"
      ],
      "url": "https://arxiv.org/abs/2602.19153",
      "published": "2026-02-22T12:34:55+00:00",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19146",
      "title": "VIGiA: Instructional Video Guidance via Dialogue Reasoning and Retrieval",
      "abstract": "We introduce VIGiA, a novel multimodal dialogue model designed to understand and reason over complex, multi-step instructional video action plans. Unlike prior work which focuses mainly on text-only guidance, or treats vision and language in isolation, VIGiA supports grounded, plan-aware dialogue that requires reasoning over visual inputs, instructional plans, and interleaved user interactions. To this end, VIGiA incorporates two key capabilities: (1) multimodal plan reasoning, enabling the model to align uni- and multimodal queries with the current task plan and respond accurately; and (2) plan-based retrieval, allowing it to retrieve relevant plan steps in either textual or visual representations. Experiments were done on a novel dataset with rich Instructional Video Dialogues aligned with Cooking and DIY plans. Our evaluation shows that VIGiA outperforms existing state-of-the-art models on all tasks in a conversational plan guidance setting, reaching over 90\\% accuracy on plan-aware VQA.",
      "authors": [
        "Diogo Glória-Silva",
        "David Semedo",
        "João Maglhães"
      ],
      "url": "https://arxiv.org/abs/2602.19146",
      "published": "2026-02-22T12:20:28+00:00",
      "categories": [
        "cs.CV",
        "cs.CL"
      ]
    },
    {
      "id": "2602.19143",
      "title": "Incremental Learning of Sparse Attention Patterns in Transformers",
      "abstract": "This paper introduces a high-order Markov chain task to investigate how transformers learn to integrate information from multiple past positions with varying statistical significance. We demonstrate that transformers learn this task incrementally: each stage is defined by the acquisition of specific information through sparse attention patterns. Notably, we identify a shift in learning dynamics from competitive, where heads converge on the most statistically dominant pattern, to cooperative, where heads specialize in distinct patterns. We model these dynamics using simplified differential equations that characterize the trajectory and prove stage-wise convergence results. Our analysis reveals that transformers ascend a complexity ladder by passing through simpler, misspecified hypothesis classes before reaching the full model class. We further show that early stopping acts as an implicit regularizer, biasing the model toward these simpler classes. These results provide a theoretical foundation for the emergence of staged learning and complex behaviors in transformers, offering insights into generalization for natural language processing and algorithmic reasoning.",
      "authors": [
        "Oğuz Kaan Yüksel",
        "Rodrigo Alvarez Lucendo",
        "Nicolas Flammarion"
      ],
      "url": "https://arxiv.org/abs/2602.19143",
      "published": "2026-02-22T12:16:06+00:00",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ]
    },
    {
      "id": "2602.19142",
      "title": "Celo2: Towards Learned Optimization Free Lunch",
      "abstract": "Learned optimizers are powerful alternatives to hand-designed update rules like Adam, yet they have seen limited practical adoption since they often fail to meta-generalize beyond their training distribution and incur high meta-training cost. For instance, prior work, VeLO, scaled meta-training to 4,000 TPU months ($\\sim$10$\\times$ GPT-3 compute) to meta-train a general-purpose optimizer but it failed to generalize beyond 600M parameters tasks. In this work, we present a surprising finding: by crafting a simple normalized optimizer architecture and augmenting meta-training, it becomes feasible to meta-train a performant general-purpose learned update rule on a tiny fraction of VeLO compute, 4.5 GPU hours to be precise. Our learned update rule scales stably to a billion-scale pretraining task (GPT-3 XL 1.3B) which is six orders of magnitude larger than its meta-training distribution. Furthermore, it shows strong performance across diverse out-of-distribution tasks and is compatible with modern optimization harness that includes orthogonalization, distinct update rules for input-output and hidden weights, and decoupled weight decay. In all, this work paves the way for practically applicable learnable optimization algorithms, unlocking exploration of richer meta-training and data curation recipes to further improve performance.",
      "authors": [
        "Abhinav Moudgil",
        "Boris Knyazev",
        "Eugene Belilovsky"
      ],
      "url": "https://arxiv.org/abs/2602.19142",
      "published": "2026-02-22T12:15:43+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19141",
      "title": "Sycophantic Chatbots Cause Delusional Spiraling, Even in Ideal Bayesians",
      "abstract": "\"AI psychosis\" or \"delusional spiraling\" is an emerging phenomenon where AI chatbot users find themselves dangerously confident in outlandish beliefs after extended chatbot conversations. This phenomenon is typically attributed to AI chatbots' well-documented bias towards validating users' claims, a property often called \"sycophancy.\" In this paper, we probe the causal link between AI sycophancy and AI-induced psychosis through modeling and simulation. We propose a simple Bayesian model of a user conversing with a chatbot, and formalize notions of sycophancy and delusional spiraling in that model. We then show that in this model, even an idealized Bayes-rational user is vulnerable to delusional spiraling, and that sycophancy plays a causal role. Furthermore, this effect persists in the face of two candidate mitigations: preventing chatbots from hallucinating false claims, and informing users of the possibility of model sycophancy. We conclude by discussing the implications of these results for model developers and policymakers concerned with mitigating the problem of delusional spiraling.",
      "authors": [
        "Kartik Chandra",
        "Max Kleiman-Weiner",
        "Jonathan Ragan-Kelley",
        "Joshua B. Tenenbaum"
      ],
      "url": "https://arxiv.org/abs/2602.19141",
      "published": "2026-02-22T12:13:44+00:00",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ]
    },
    {
      "id": "2602.19140",
      "title": "CaReFlow: Cyclic Adaptive Rectified Flow for Multimodal Fusion",
      "abstract": "Modality gap significantly restricts the effectiveness of multimodal fusion. Previous methods often use techniques such as diffusion models and adversarial learning to reduce the modality gap, but they typically focus on one-to-one alignment without exposing the data points of the source modality to the global distribution information of the target modality. To this end, leveraging the characteristic of rectified flow that can map one distribution to another via a straight trajectory, we extend rectified flow for modality distribution mapping. Specifically, we leverage the `one-to-many mapping' strategy in rectified flow that allows each data point of the source modality to observe the overall target distribution. This also alleviates the issue of insufficient paired data within each sample, enabling a more robust distribution transformation. Moreover, to achieve more accurate distribution mapping and address the ambiguous flow directions in one-to-many mapping, we design `adaptive relaxed alignment', enforcing stricter alignment for modality pairs belonging to the same sample, while applying relaxed mapping for pairs not belonging to the same sample or category. Additionally, to prevent information loss during distribution mapping, we introduce `cyclic rectified flow' to ensure the transferred features can be translated back to the original features, allowing multimodal representations to learn sufficient modality-specific information. After distribution alignment, our approach achieves very competitive results on multiple tasks of multimodal affective computing even with a simple fusion method, and visualizations verify that it can effectively reduce the modality gap.",
      "authors": [
        "Sijie Mai",
        "Shiqin Han"
      ],
      "url": "https://arxiv.org/abs/2602.19140",
      "published": "2026-02-22T12:12:05+00:00",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19138",
      "title": "CRCC: Contrast-Based Robust Cross-Subject and Cross-Site Representation Learning for EEG",
      "abstract": "EEG-based neural decoding models often fail to generalize across acquisition sites due to structured, site-dependent biases implicitly exploited during training. We reformulate cross-site clinical EEG learning as a bias-factorized generalization problem, in which domain shifts arise from multiple interacting sources. We identify three fundamental bias factors and propose a general training framework that mitigates their influence through data standardization and representation-level constraints. We construct a standardized multi-site EEG benchmark for Major Depressive Disorder and introduce CRCC, a two-stage training paradigm combining encoder-decoder pretraining with joint fine-tuning via cross-subject/site contrastive learning and site-adversarial optimization. CRCC consistently outperforms state-of-the-art baselines and achieves a 10.7 percentage-point improvement in balanced accuracy under strict zero-shot site transfer, demonstrating robust generalization to unseen environments.",
      "authors": [
        "Xiaobin Wong",
        "Zhonghua Zhao",
        "Haoran Guo",
        "Zhengyi Liu",
        "Yu Wu",
        "Feng Yan",
        "Zhiren Wang",
        "Sen Song"
      ],
      "url": "https://arxiv.org/abs/2602.19138",
      "published": "2026-02-22T12:00:49+00:00",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19133",
      "title": "A Dataset for Named Entity Recognition and Relation Extraction from Art-historical Image Descriptions",
      "abstract": "This paper introduces FRAME (Fine-grained Recognition of Art-historical Metadata and Entities), a manually annotated dataset of art-historical image descriptions for Named Entity Recognition (NER) and Relation Extraction (RE). Descriptions were collected from museum catalogs, auction listings, open-access platforms, and scholarly databases, then filtered to ensure that each text focuses on a single artwork and contains explicit statements about its material, composition, or iconography. FRAME provides stand-off annotations in three layers: a metadata layer for object-level properties, a content layer for depicted subjects and motifs, and a co-reference layer linking repeated mentions. Across layers, entity spans are labeled with 37 types and connected by typed RE links between mentions. Entity types are aligned with Wikidata to support Named Entity Linking (NEL) and downstream knowledge-graph construction. The dataset is released as UIMA XMI Common Analysis Structure (CAS) files with accompanying images and bibliographic metadata, and can be used to benchmark and fine-tune NER and RE systems, including zero- and few-shot setups with Large Language Models (LLMs).",
      "authors": [
        "Stefanie Schneider",
        "Miriam Göldl",
        "Julian Stalter",
        "Ricarda Vollmer"
      ],
      "url": "https://arxiv.org/abs/2602.19133",
      "published": "2026-02-22T11:29:03+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.19131",
      "title": "Test-Time Learning of Causal Structure from Interventional Data",
      "abstract": "Supervised causal learning has shown promise in causal discovery, yet it often struggles with generalization across diverse interventional settings, particularly when intervention targets are unknown. To address this, we propose TICL (Test-time Interventional Causal Learning), a novel method that synergizes Test-Time Training with Joint Causal Inference. Specifically, we design a self-augmentation strategy to generate instance-specific training data at test time, effectively avoiding distribution shifts. Furthermore, by integrating joint causal inference, we developed a PC-inspired two-phase supervised learning scheme, which effectively leverages self-augmented training data while ensuring theoretical identifiability. Extensive experiments on bnlearn benchmarks demonstrate TICL's superiority in multiple aspects of causal discovery and intervention target detection.",
      "authors": [
        "Wei Chen",
        "Rui Ding",
        "Bojun Huang",
        "Yang Zhang",
        "Qiang Fu",
        "Yuxuan Liang",
        "Han Shi",
        "Dongmei Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.19131",
      "published": "2026-02-22T11:23:05+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19130",
      "title": "Detecting labeling bias using influence functions",
      "abstract": "Labeling bias arises during data collection due to resource limitations or unconscious bias, leading to unequal label error rates across subgroups or misrepresentation of subgroup prevalence. Most fairness constraints assume training labels reflect the true distribution, rendering them ineffective when labeling bias is present; leaving a challenging question, that \\textit{how can we detect such labeling bias?} In this work, we investigate whether influence functions can be used to detect labeling bias. Influence functions estimate how much each training sample affects a model's predictions by leveraging the gradient and Hessian of the loss function -- when labeling errors occur, influence functions can identify wrongly labeled samples in the training set, revealing the underlying failure mode. We develop a sample valuation pipeline and test it first on the MNIST dataset, then scaled to the more complex CheXpert medical imaging dataset. To examine label noise, we introduced controlled errors by flipping 20\\% of the labels for one class in the dataset. Using a diagonal Hessian approximation, we demonstrated promising results, successfully detecting nearly 90\\% of mislabeled samples in MNIST. On CheXpert, mislabeled samples consistently exhibit higher influence scores. These results highlight the potential of influence functions for identifying label errors.",
      "authors": [
        "Frida Jørgensen",
        "Nina Weng",
        "Siavash Bigdeli"
      ],
      "url": "https://arxiv.org/abs/2602.19130",
      "published": "2026-02-22T11:20:35+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19128",
      "title": "K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model",
      "abstract": "Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.",
      "authors": [
        "Shiyi Cao",
        "Ziming Mao",
        "Joseph E. Gonzalez",
        "Ion Stoica"
      ],
      "url": "https://arxiv.org/abs/2602.19128",
      "published": "2026-02-22T11:06:22+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.19127",
      "title": "AgenticRAGTracer: A Hop-Aware Benchmark for Diagnosing Multi-Step Retrieval Reasoning in Agentic RAG",
      "abstract": "With the rapid advancement of agent-based methods in recent years, Agentic RAG has undoubtedly become an important research direction. Multi-hop reasoning, which requires models to engage in deliberate thinking and multi-step interaction, serves as a critical testbed for assessing such capabilities. However, existing benchmarks typically provide only final questions and answers, while lacking the intermediate hop-level questions that gradually connect atomic questions to the final multi-hop query. This limitation prevents researchers from analyzing at which step an agent fails and restricts more fine-grained evaluation of model capabilities. Moreover, most current benchmarks are manually constructed, which is both time-consuming and labor-intensive, while also limiting scalability and generalization. To address these challenges, we introduce AgenticRAGTracer, the first Agentic RAG benchmark that is primarily constructed automatically by large language models and designed to support step-by-step validation. Our benchmark spans multiple domains, contains 1,305 data points, and has no overlap with existing mainstream benchmarks. Extensive experiments demonstrate that even the best large language models perform poorly on our dataset. For instance, GPT-5 attains merely 22.6\\% EM accuracy on the hardest portion of our dataset. Hop-aware diagnosis reveals that failures are primarily driven by distorted reasoning chains -- either collapsing prematurely or wandering into over-extension. This highlights a critical inability to allocate steps consistent with the task's logical structure, providing a diagnostic dimension missing in traditional evaluations. We believe our work will facilitate research in Agentic RAG and inspire further meaningful progress in this area. Our code and data are available at https://github.com/YqjMartin/AgenticRAGTracer.",
      "authors": [
        "Qijie You",
        "Wenkai Yu",
        "Wentao Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.19127",
      "published": "2026-02-22T10:55:21+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.19126",
      "title": "Robust Predictive Uncertainty and Double Descent in Contaminated Bayesian Random Features",
      "abstract": "We propose a robust Bayesian formulation of random feature (RF) regression that accounts explicitly for prior and likelihood misspecification via Huber-style contamination sets. Starting from the classical equivalence between ridge-regularized RF training and Bayesian inference with Gaussian priors and likelihoods, we replace the single prior and likelihood with $ε$- and $η$-contaminated credal sets, respectively, and perform inference using pessimistic generalized Bayesian updating. We derive explicit and tractable bounds for the resulting lower and upper posterior predictive densities. These bounds show that, when contamination is moderate, prior and likelihood ambiguity effectively acts as a direct contamination of the posterior predictive distribution, yielding uncertainty envelopes around the classical Gaussian predictive. We introduce an Imprecise Highest Density Region (IHDR) for robust predictive uncertainty quantification and show that it admits an efficient outer approximation via an adjusted Gaussian credible interval. We further obtain predictive variance bounds (under a mild truncation approximation for the upper bound) and prove that they preserve the leading-order proportional-growth asymptotics known for RF models. Together, these results establish a robustness theory for Bayesian random features: predictive uncertainty remains computationally tractable, inherits the classical double-descent phase structure, and is improved by explicit worst-case guarantees under bounded prior and likelihood misspecification.",
      "authors": [
        "Michele Caprio",
        "Katerina Papagiannouli",
        "Siu Lun Chau",
        "Sayan Mukherjee"
      ],
      "url": "https://arxiv.org/abs/2602.19126",
      "published": "2026-02-22T10:50:04+00:00",
      "categories": [
        "cs.LG",
        "math.PR",
        "math.ST"
      ]
    },
    {
      "id": "2602.19116",
      "title": "Event-Triggered Gossip for Distributed Learning",
      "abstract": "While distributed learning offers a new learning paradigm for distributed network with no central coordination, it is constrained by communication bottleneck between nodes.   We develop a new event-triggered gossip framework for distributed learning to reduce inter-node communication overhead. The framework introduces an adaptive communication control mechanism that enables each node to autonomously decide in a fully decentralized fashion when to exchange model information with its neighbors based on local model deviations. We analyze the ergodic convergence of the proposed framework under noconvex objectives and interpret the convergence guarantees under different triggering conditions. Simulation results show that the proposed framework achieves substantially lower communication overhead than the state-of-the-art distributed learning methods, reducing cumulative point-to-point transmissions by \\textbf{71.61\\%} with only a marginal performance loss, compared with the conventional full-communication baseline.",
      "authors": [
        "Zhiyuan Zhai",
        "Xiaojun Yuan",
        "Wei Ni",
        "Xin Wang",
        "Rui Zhang",
        "Geoffrey Ye Li"
      ],
      "url": "https://arxiv.org/abs/2602.19116",
      "published": "2026-02-22T10:13:43+00:00",
      "categories": [
        "eess.SP",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19115",
      "title": "How Do LLMs Encode Scientific Quality? An Empirical Study Using Monosemantic Features from Sparse Autoencoders",
      "abstract": "In recent years, there has been a growing use of generative AI, and large language models (LLMs) in particular, to support both the assessment and generation of scientific work. Although some studies have shown that LLMs can, to a certain extent, evaluate research according to perceived quality, our understanding of the internal mechanisms that enable this capability remains limited. This paper presents the first study that investigates how LLMs encode the concept of scientific quality through relevant monosemantic features extracted using sparse autoencoders. We derive such features under different experimental settings and assess their ability to serve as predictors across three tasks related to research quality: predicting citation count, journal SJR, and journal h-index. The results indicate that LLMs encode features associated with multiple dimensions of scientific quality. In particular, we identify four recurring types of features that capture key aspects of how research quality is represented: 1) features reflecting research methodologies; 2) features related to publication type, with literature reviews typically exhibiting higher impact; 3) features associated with high-impact research fields and technologies; and 4) features corresponding to specific scientific jargons. These findings represent an important step toward understanding how LLMs encapsulate concepts related to research quality.",
      "authors": [
        "Michael McCoubrey",
        "Angelo Salatino",
        "Francesco Osborne",
        "Enrico Motta"
      ],
      "url": "https://arxiv.org/abs/2602.19115",
      "published": "2026-02-22T10:12:20+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL"
      ]
    },
    {
      "id": "2602.19114",
      "title": "Kaiwu-PyTorch-Plugin: Bridging Deep Learning and Photonic Quantum Computing for Energy-Based Models and Active Sample Selection",
      "abstract": "This paper introduces the Kaiwu-PyTorch-Plugin (KPP) to bridge Deep Learning and Photonic Quantum Computing across multiple dimensions. KPP integrates the Coherent Ising Machine into the PyTorch ecosystem, addressing classical inefficiencies in Energy-Based Models. The framework facilitates quantum integration in three key aspects: accelerating Boltzmann sampling, optimizing training data via Active Sampling, and constructing hybrid architectures like QBM-VAE and Q-Diffusion. Empirical results on single-cell and OpenWebText datasets demonstrate KPPs ability to achieve SOTA performance, validating a comprehensive quantum-classical paradigm.",
      "authors": [
        "Hongdong Zhu",
        "Qi Gao",
        "Yin Ma",
        "Shaobo Chen",
        "Haixu Liu",
        "Fengao Wang",
        "Tinglan Wang",
        "Chang Wu",
        "Kai Wen"
      ],
      "url": "https://arxiv.org/abs/2602.19114",
      "published": "2026-02-22T10:11:23+00:00",
      "categories": [
        "quant-ph",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19113",
      "title": "Learning from Complexity: Exploring Dynamic Sample Pruning of Spatio-Temporal Training",
      "abstract": "Spatio-temporal forecasting is fundamental to intelligent systems in transportation, climate science, and urban planning. However, training deep learning models on the massive, often redundant, datasets from these domains presents a significant computational bottleneck. Existing solutions typically focus on optimizing model architectures or optimizers, while overlooking the inherent inefficiency of the training data itself. This conventional approach of iterating over the entire static dataset each epoch wastes considerable resources on easy-to-learn or repetitive samples. In this paper, we explore a novel training-efficiency techniques, namely learning from complexity with dynamic sample pruning, ST-Prune, for spatio-temporal forecasting. Through dynamic sample pruning, we aim to intelligently identify the most informative samples based on the model's real-time learning state, thereby accelerating convergence and improving training efficiency. Extensive experiments conducted on real-world spatio-temporal datasets show that ST-Prune significantly accelerates the training speed while maintaining or even improving the model performance, and it also has scalability and universality.",
      "authors": [
        "Wei Chen",
        "Junle Chen",
        "Yuqian Wu",
        "Yuxuan Liang",
        "Xiaofang Zhou"
      ],
      "url": "https://arxiv.org/abs/2602.19113",
      "published": "2026-02-22T10:11:04+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "id": "2602.19111",
      "title": "Astra: Activation-Space Tail-Eigenvector Low-Rank Adaptation of Large Language Models",
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, especially LoRA, are widely used for adapting pre-trained models to downstream tasks due to their computational and storage efficiency. However, in the context of LoRA and its variants, the potential of activation subspaces corresponding to tail eigenvectors remains substantially under-exploited, which may lead to suboptimal fine-tuning performance. In this work, we propose Astra (Activation-Space Tail-Eigenvector Low-Rank Adaptation), a novel PEFT method that leverages the tail eigenvectors of the model output activations-estimated from a small task-specific calibration set-to construct task-adaptive low-rank adapters. By constraining updates to the subspace spanned by these tail eigenvectors, Astra achieves faster convergence and improved downstream performance with a significantly reduced parameter budget. Extensive experiments across natural language understanding (NLU) and natural language generation (NLG) tasks demonstrate that Astra consistently outperforms existing PEFT baselines across 16 benchmarks and even surpasses full fine-tuning (FFT) in certain scenarios.",
      "authors": [
        "Kainan Liu",
        "Yong Zhang",
        "Ning Cheng",
        "Yun Zhu",
        "Yanmeng Wang",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "url": "https://arxiv.org/abs/2602.19111",
      "published": "2026-02-22T09:54:40+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.19109",
      "title": "Post-Routing Arithmetic in Llama-3: Last-Token Result Writing and Rotation-Structured Digit Directions",
      "abstract": "We study three-digit addition in Meta-Llama-3-8B (base) under a one-token readout to characterize how   arithmetic answers are finalized after cross-token routing becomes causally irrelevant.   Causal residual patching and cumulative attention ablations localize a sharp boundary near layer~17:   beyond it, the decoded sum is controlled almost entirely by the last input token and late-layer self-attention   is largely dispensable.   In this post-routing regime, digit(-sum) direction dictionaries vary with a next-higher-digit context but are   well-related by an approximately orthogonal map inside a shared low-rank subspace (low-rank Procrustes alignment).   Causal digit editing matches this geometry: naive cross-context transfer fails, while rotating directions through the   learned map restores strict counterfactual edits; negative controls do not recover.",
      "authors": [
        "Yao Yan"
      ],
      "url": "https://arxiv.org/abs/2602.19109",
      "published": "2026-02-22T09:49:36+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.19101",
      "title": "Value Entanglement: Conflation Between Different Kinds of Good In (Some) Large Language Models",
      "abstract": "Value alignment of Large Language Models (LLMs) requires us to empirically measure these models' actual, acquired representation of value. Among the characteristics of value representation in humans is that they distinguish among value of different kinds. We investigate whether LLMs likewise distinguish three different kinds of good: moral, grammatical, and economic. By probing model behavior, embeddings, and residual stream activations, we report pervasive cases of value entanglement: a conflation between these distinct representations of value. Specifically, both grammatical and economic valuation was found to be overly influenced by moral value, relative to human norms. This conflation was repaired by selective ablation of the activation vectors associated with morality.",
      "authors": [
        "Seong Hah Cho",
        "Junyi Li",
        "Anna Leshinskaya"
      ],
      "url": "https://arxiv.org/abs/2602.19101",
      "published": "2026-02-22T09:11:26+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19096",
      "title": "The Power of Decaying Steps: Enhancing Attack Stability and Transferability for Sign-based Optimizers",
      "abstract": "Crafting adversarial examples can be formulated as an optimization problem. While sign-based optimizers such as I-FGSM and MI-FGSM have become the de facto standard for the induced optimization problems, there still exist several unsolved problems in theoretical grounding and practical reliability especially in non-convergence and instability, which inevitably influences their transferability. Contrary to the expectation, we observe that the attack success rate may degrade sharply when more number of iterations are conducted. In this paper, we address these issues from an optimization perspective. By reformulating the sign-based optimizer as a specific coordinate-wise gradient descent, we argue that one cause for non-convergence and instability is their non-decaying step-size scheduling. Based upon this viewpoint, we propose a series of new attack algorithms that enforce Monotonically Decreasing Coordinate-wise Step-sizes (MDCS) within sign-based optimizers. Typically, we further provide theoretical guarantees proving that MDCS-MI attains an optimal convergence rate of $O(1/\\sqrt{T})$, where $T$ is the number of iterations. Extensive experiments on image classification and cross-modal retrieval tasks demonstrate that our approach not only significantly improves transferability but also enhances attack stability compared to state-of-the-art sign-based methods.",
      "authors": [
        "Wei Tao",
        "Yang Dai",
        "Jincai Huang",
        "Qing Tao"
      ],
      "url": "https://arxiv.org/abs/2602.19096",
      "published": "2026-02-22T08:37:06+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19094",
      "title": "RKHS Representation of Algebraic Convolutional Filters with Integral Operators",
      "abstract": "Integral operators play a central role in signal processing, underpinning classical convolution, and filtering on continuous network models such as graphons. While these operators are traditionally analyzed through spectral decompositions, their connection to reproducing kernel Hilbert spaces (RKHS) has not been systematically explored within the algebraic signal processing framework. In this paper, we develop a comprehensive theory showing that the range of integral operators naturally induces RKHS convolutional signal models whose reproducing kernels are determined by a box product of the operator symbols. We characterize the algebraic and spectral properties of these induced RKHS and show that polynomial filtering with integral operators corresponds to iterated box products, giving rise to a unital kernel algebra. This perspective yields pointwise RKHS representations of filters via the reproducing property, providing an alternative to operator-based implementations. Our results establish precise connections between eigendecompositions and RKHS representations in graphon signal processing, extend naturally to directed graphons, and enable novel spatial--spectral localization results. Furthermore, we show that when the spectral domain is a subset of the original domain of the signals, optimal filters for regularized learning problems admit finite-dimensional RKHS representations, providing a principled foundation for learnable filters in integral-operator-based neural architectures.",
      "authors": [
        "Alejandro Parada-Mayorga",
        "Alejandro Ribeiro",
        "Juan Bazerque"
      ],
      "url": "https://arxiv.org/abs/2602.19094",
      "published": "2026-02-22T08:28:34+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.20199",
      "title": "IMOVNO+: A Regional Partitioning and Meta-Heuristic Ensemble Framework for Imbalanced Multi-Class Learning",
      "abstract": "Class imbalance, overlap, and noise degrade data quality, reduce model reliability, and limit generalization. Although widely studied in binary classification, these issues remain underexplored in multi-class settings, where complex inter-class relationships make minority-majority structures unclear and traditional clustering fails to capture distribution shape. Approaches that rely only on geometric distances risk removing informative samples and generating low-quality synthetic data, while binarization approaches treat imbalance locally and ignore global inter-class dependencies. At the algorithmic level, ensembles struggle to integrate weak classifiers, leading to limited robustness. This paper proposes IMOVNO+ (IMbalance-OVerlap-NOise+ Algorithm-Level Optimization), a two-level framework designed to jointly enhance data quality and algorithmic robustness for binary and multi-class tasks. At the data level, first, conditional probability is used to quantify the informativeness of each sample. Second, the dataset is partitioned into core, overlapping, and noisy regions. Third, an overlapping-cleaning algorithm is introduced that combines Z-score metrics with a big-jump gap distance. Fourth, a smart oversampling algorithm based on multi-regularization controls synthetic sample proximity, preventing new overlaps. At the algorithmic level, a meta-heuristic prunes ensemble classifiers to reduce weak-learner influence. IMOVNO+ was evaluated on 35 datasets (13 multi-class, 22 binary). Results show consistent superiority over state-of-the-art methods, approaching 100% in several cases. For multi-class data, IMOVNO+ achieves gains of 37-57% in G-mean, 25-44% in F1-score, 25-39% in precision, and 26-43% in recall. In binary tasks, it attains near-perfect performance with improvements of 14-39%. The framework handles data scarcity and imbalance from collection and privacy limits.",
      "authors": [
        "Soufiane Bacha",
        "Laouni Djafri",
        "Sahraoui Dhelim",
        "Huansheng Ning"
      ],
      "url": "https://arxiv.org/abs/2602.20199",
      "published": "2026-02-22T08:13:51+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20198",
      "title": "KEMP-PIP: A Feature-Fusion Based Approach for Pro-inflammatory Peptide Prediction",
      "abstract": "Pro-inflammatory peptides (PIPs) play critical roles in immune signaling and inflammation but are difficult to identify experimentally due to costly and time-consuming assays. To address this challenge, we present KEMP-PIP, a hybrid machine learning framework that integrates deep protein embeddings with handcrafted descriptors for robust PIP prediction. Our approach combines contextual embeddings from pretrained ESM protein language models with multi-scale k-mer frequencies, physicochemical descriptors, and modlAMP sequence features. Feature pruning and class-weighted logistic regression manage high dimensionality and class imbalance, while ensemble averaging with an optimized decision threshold enhances the sensitivity--specificity balance. Through systematic ablation studies, we demonstrate that integrating complementary feature sets consistently improves predictive performance. On the standard benchmark dataset, KEMP-PIP achieves an MCC of 0.505, accuracy of 0.752, and AUC of 0.762, outperforming ProIn-fuse, MultiFeatVotPIP, and StackPIP. Relative to StackPIP, these results represent improvements of 9.5% in MCC and 4.8% in both accuracy and AUC. The KEMP-PIP web server is freely available at https://nilsparrow1920-kemp-pip.hf.space/ and the full implementation at https://github.com/S18-Niloy/KEMP-PIP.",
      "authors": [
        "Soumik Deb Niloy",
        "Md. Fahmid-Ul-Alam Juboraj",
        "Swakkhar Shatabda"
      ],
      "url": "https://arxiv.org/abs/2602.20198",
      "published": "2026-02-22T08:09:01+00:00",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19089",
      "title": "Ani3DHuman: Photorealistic 3D Human Animation with Self-guided Stochastic Sampling",
      "abstract": "Current 3D human animation methods struggle to achieve photorealism: kinematics-based approaches lack non-rigid dynamics (e.g., clothing dynamics), while methods that leverage video diffusion priors can synthesize non-rigid motion but suffer from quality artifacts and identity loss. To overcome these limitations, we present Ani3DHuman, a framework that marries kinematics-based animation with video diffusion priors. We first introduce a layered motion representation that disentangles rigid motion from residual non-rigid motion. Rigid motion is generated by a kinematic method, which then produces a coarse rendering to guide the video diffusion model in generating video sequences that restore the residual non-rigid motion. However, this restoration task, based on diffusion sampling, is highly challenging, as the initial renderings are out-of-distribution, causing standard deterministic ODE samplers to fail. Therefore, we propose a novel self-guided stochastic sampling method, which effectively addresses the out-of-distribution problem by combining stochastic sampling (for photorealistic quality) with self-guidance (for identity fidelity). These restored videos provide high-quality supervision, enabling the optimization of the residual non-rigid motion field. Extensive experiments demonstrate that \\MethodName can generate photorealistic 3D human animation, outperforming existing methods. Code is available in https://github.com/qiisun/ani3dhuman.",
      "authors": [
        "Qi Sun",
        "Can Wang",
        "Jiaxiang Shang",
        "Yingchun Liu",
        "Jing Liao"
      ],
      "url": "https://arxiv.org/abs/2602.19089",
      "published": "2026-02-22T08:07:28+00:00",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19087",
      "title": "Detecting Cybersecurity Threats by Integrating Explainable AI with SHAP Interpretability and Strategic Data Sampling",
      "abstract": "The critical need for transparent and trustworthy machine learning in cybersecurity operations drives the development of this integrated Explainable AI (XAI) framework. Our methodology addresses three fundamental challenges in deploying AI for threat detection: handling massive datasets through Strategic Sampling Methodology that preserves class distributions while enabling efficient model development; ensuring experimental rigor via Automated Data Leakage Prevention that systematically identifies and removes contaminated features; and providing operational transparency through Integrated XAI Implementation using SHAP analysis for model-agnostic interpretability across algorithms. Applied to the CIC-IDS2017 dataset, our approach maintains detection efficacy while reducing computational overhead and delivering actionable explanations for security analysts. The framework demonstrates that explainability, computational efficiency, and experimental integrity can be simultaneously achieved, providing a robust foundation for deploying trustworthy AI systems in security operations centers where decision transparency is paramount.",
      "authors": [
        "Norrakith Srisumrith",
        "Sunantha Sodsee"
      ],
      "url": "https://arxiv.org/abs/2602.19087",
      "published": "2026-02-22T08:01:14+00:00",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.21252",
      "title": "INTACT: Intent-Aware Representation Learning for Cryptographic Traffic Violation Detection",
      "abstract": "Security monitoring systems typically treat anomaly detection as identifying statistical deviations from observed data distributions. In cryptographic traffic analysis, however, violations are defined not by rarity but by explicit policy constraints, including key reuse prohibition, downgrade prevention, and bounded key lifetimes. This fundamental mismatch limits the interpretability and adaptability of conventional anomaly detection methods. We introduce INTACT (INTent-Aware Cryptographic Traffic), a policy-conditioned framework that reformulates violation detection as conditional constraint learning. Instead of learning a static decision boundary over behavioral features, INTACT models the probability of violation conditioned on both observed behavior and declared security intent. The architecture factorizes representation learning into behavioral and intent encoders whose fused embeddings produce a violation score, yielding a policy-parameterized family of decision boundaries. We evaluate the framework on a real-world network flow dataset and a 210,000-trace synthetic multi-intent cryptographic dataset. INTACT matches or exceeds strong unsupervised and supervised baselines, achieving near-perfect discrimination (AUROC up to 1.0000) in the real dataset and consistent superiority in detecting relational and composite violations in the synthetic setting. These results demonstrate that explicit intent conditioning improves discrimination, interpretability, and robustness in cryptographic monitoring.",
      "authors": [
        "Rahul D Ray"
      ],
      "url": "https://arxiv.org/abs/2602.21252",
      "published": "2026-02-22T07:38:46+00:00",
      "categories": [
        "cs.CR",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19079",
      "title": "TriTopic: Tri-Modal Graph-Based Topic Modeling with Iterative Refinement and Archetypes",
      "abstract": "Topic modeling extracts latent themes from large text collections, but leading approaches like BERTopic face critical limitations: stochastic instability, loss of lexical precision (\"Embedding Blur\"), and reliance on a single data perspective.   We present TriTopic, a framework that addresses these weaknesses through a tri-modal graph fusing semantic embeddings, TF-IDF, and metadata. Three core innovations drive its performance: hybrid graph construction via Mutual kNN and Shared Nearest Neighbors to eliminate noise and combat the curse of dimensionality; Consensus Leiden Clustering for reproducible, stable partitions; and Iterative Refinement that sharpens embeddings through dynamic centroid-pulling. TriTopic also replaces the \"average document\" concept with archetype-based topic representations defined by boundary cases rather than centers alone.   In benchmarks across 20 Newsgroups, BBC News, AG News, and Arxiv, TriTopic achieves the highest NMI on every dataset (mean NMI 0.575 vs. 0.513 for BERTopic, 0.416 for NMF, 0.299 for LDA), guarantees 100% corpus coverage with 0% outliers, and is available as an open-source PyPI library.",
      "authors": [
        "Roman Egger"
      ],
      "url": "https://arxiv.org/abs/2602.19079",
      "published": "2026-02-22T07:29:53+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.20197",
      "title": "Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning",
      "abstract": "Reinforcement Learning with verifiable rewards (RLVR) has emerged as a primary learning paradigm for enhancing the reasoning capabilities of multi-modal large language models (MLLMs). However, during RL training, the enormous state space of MLLM and sparse rewards often leads to entropy collapse, policy degradation, or over-exploitation of suboptimal behaviors. This necessitates an exploration strategy that maintains productive stochasticity while avoiding the drawbacks of uncontrolled random sampling, yielding inefficient exploration. In this paper, we propose CalibRL, a hybrid-policy RLVR framework that supports controllable exploration with expert guidance, enabled by two key mechanisms. First, a distribution-aware advantage weighting scales updates by group rareness to calibrate the distribution, therefore preserving exploration. Meanwhile, the asymmetric activation function (LeakyReLU) leverages the expert knowledge as a calibration baseline to moderate overconfident updates while preserving their corrective direction. CalibRL increases policy entropy in a guided manner and clarifies the target distribution by estimating the on-policy distribution through online sampling. Updates are driven by these informative behaviors, avoiding convergence to erroneous patterns. Importantly, these designs help alleviate the distributional mismatch between the model's policy and expert trajectories, thereby achieving a more stable balance between exploration and exploitation. Extensive experiments across eight benchmarks, including both in-domain and out-of-domain settings, demonstrate consistent improvements, validating the effectiveness of our controllable hybrid-policy RLVR training. Code is available at https://github.com/zhh6425/CalibRL.",
      "authors": [
        "Zhuoxu Huang",
        "Mengxi Jia",
        "Hao Sun",
        "Xuelong Li",
        "Jungong Han"
      ],
      "url": "https://arxiv.org/abs/2602.20197",
      "published": "2026-02-22T07:23:36+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19071",
      "title": "Defining Explainable AI for Requirements Analysis",
      "abstract": "Explainable Artificial Intelligence (XAI) has become popular in the last few years. The Artificial Intelligence (AI) community in general, and the Machine Learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements?   In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly.",
      "authors": [
        "Raymond Sheh",
        "Isaac Monteath"
      ],
      "url": "https://arxiv.org/abs/2602.19071",
      "published": "2026-02-22T07:05:49+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.19069",
      "title": "Asking the Right Questions: Improving Reasoning with Generated Stepping Stones",
      "abstract": "Recent years have witnessed tremendous progress in enabling LLMs to solve complex reasoning tasks such as math and coding. As we start to apply LLMs to harder tasks that they may not be able to solve in one shot, it is worth paying attention to their ability to construct intermediate stepping stones that prepare them to better solve the tasks. Examples of stepping stones include simplifications, alternative framings, or subproblems. We study properties and benefits of stepping stones in the context of modern reasoning LLMs via ARQ (\\textbf{A}king the \\textbf{R}ight \\textbf{Q}uestions), our simple framework which introduces a question generator to the default reasoning pipeline. We first show that good stepping stone questions exist and are transferrable, meaning that good questions can be generated, and they substantially help LLMs of various capabilities in solving the target tasks. We next frame stepping stone generation as a post-training task and show that we can fine-tune LLMs to generate more useful stepping stones by SFT and RL on synthetic data.",
      "authors": [
        "Hengyuan Hu",
        "Tingchen Fu",
        "Minqi Jiang",
        "Alexander H Miller",
        "Yoram Bachrach",
        "Jakob Nicolaus Foerster"
      ],
      "url": "https://arxiv.org/abs/2602.19069",
      "published": "2026-02-22T06:54:24+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.19068",
      "title": "TimeRadar: A Domain-Rotatable Foundation Model for Time Series Anomaly Detection",
      "abstract": "Current time series foundation models (TSFMs) primarily focus on learning prevalent and regular patterns within a predefined time or frequency domain to enable supervised downstream tasks (e.g., forecasting). Consequently, they are often ineffective for inherently unsupervised downstream tasks-such as time series anomaly detection (TSAD), which aims to identify rare, irregular patterns. This limitation arises because such abnormal patterns can closely resemble the regular patterns when presented in the same time/frequency domain. To address this issue, we introduce TimeRadar, an innovative TSFM built in a fractional time-frequency domain to support generalist TSAD across diverse unseen datasets. Our key insight is that rotating a time series into a data-dependent fractional time-frequency representation can adaptively differentiate the normal and abnormal signals across different datasets. To this end, a novel component, namely Fractionally modulated Time-Frequency Reconstruction (FTFRecon), is proposed in TimeRadar to leverage a learnable fractional order to rotate the time series to the most pronounced angle between a continuous time and frequency domain for accurate data reconstruction. This provides adaptive data reconstruction in an optimal time-frequency domain for each data input, enabling effective differentiation of the unbounded abnormal patterns from the regular ones across datasets, including unseen datasets. To allow TimeRadar to model local abnormality that is not captured by the global data reconstruction, we further introduce a Contextual Deviation Learning (CDL) component to model the local deviation of the input relative to its contextual time series data in the rotatable domain.",
      "authors": [
        "Hui He",
        "Hezhe Qiao",
        "Yutong Chen",
        "Kun Yi",
        "Guansong Pang"
      ],
      "url": "https://arxiv.org/abs/2602.19068",
      "published": "2026-02-22T06:53:45+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19066",
      "title": "IDLM: Inverse-distilled Diffusion Language Models",
      "abstract": "Diffusion Language Models (DLMs) have recently achieved strong results in text generation. However, their multi-step sampling leads to slow inference, limiting practical use. To address this, we extend Inverse Distillation, a technique originally developed to accelerate continuous diffusion models, to the discrete setting. Nonetheless, this extension introduces both theoretical and practical challenges. From a theoretical perspective, the inverse distillation objective lacks uniqueness guarantees, which may lead to suboptimal solutions. From a practical standpoint, backpropagation in the discrete space is non-trivial and often unstable. To overcome these challenges, we first provide a theoretical result demonstrating that our inverse formulation admits a unique solution, thereby ensuring valid optimization. We then introduce gradient-stable relaxations to support effective training. As a result, experiments on multiple DLMs show that our method, Inverse-distilled Diffusion Language Models (IDLM), reduces the number of inference steps by 4x-64x, while preserving the teacher model's entropy and generative perplexity.",
      "authors": [
        "David Li",
        "Nikita Gushchin",
        "Dmitry Abulkhanov",
        "Eric Moulines",
        "Ivan Oseledets",
        "Maxim Panov",
        "Alexander Korotin"
      ],
      "url": "https://arxiv.org/abs/2602.19066",
      "published": "2026-02-22T06:47:04+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19065",
      "title": "Agentic Problem Frames: A Systematic Approach to Engineering Reliable Domain Agents",
      "abstract": "Large Language Models (LLMs) are evolving into autonomous agents, yet current \"frameless\" development--relying on ambiguous natural language without engineering blueprints--leads to critical risks such as scope creep and open-loop failures. To ensure industrial-grade reliability, this study proposes Agentic Problem Frames (APF), a systematic engineering framework that shifts focus from internal model intelligence to the structured interaction between the agent and its environment.   The APF establishes a dynamic specification paradigm where intent is concretized at runtime through domain knowledge injection. At its core, the Act-Verify-Refine (AVR) loop functions as a closed-loop control system that transforms execution results into verified knowledge assets, driving system behavior toward asymptotic convergence to mission requirements (R). To operationalize this, this study introduces the Agentic Job Description (AJD), a formal specification tool that defines jurisdictional boundaries, operational contexts, and epistemic evaluation criteria.   The efficacy of this framework is validated through two contrasting case studies: a delegated proxy model for business travel and an autonomous supervisor model for industrial equipment management. By applying AJD-based specification and APF modeling to these scenarios, the analysis demonstrates how operational scenarios are systematically controlled within defined boundaries. These cases provide a conceptual proof that agent reliability stems not from a model's internal reasoning alone, but from the rigorous engineering structures that anchor stochastic AI within deterministic business processes, thereby enabling the development of verifiable and dependable domain agents.",
      "authors": [
        "Chanjin Park"
      ],
      "url": "https://arxiv.org/abs/2602.19065",
      "published": "2026-02-22T06:32:32+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.19058",
      "title": "Do LLMs and VLMs Share Neurons for Inference? Evidence and Mechanisms of Cross-Modal Transfer",
      "abstract": "Large vision-language models (LVLMs) have rapidly advanced across various domains, yet they still lag behind strong text-only large language models (LLMs) on tasks that require multi-step inference and compositional decision-making. Motivated by their shared transformer architectures, we investigate whether the two model families rely on common internal computation for such inference. At the neuron level, we uncover a surprisingly large overlap: more than half of the top-activated units during multi-step inference are shared between representative LLMs and LVLMs, revealing a modality-invariant inference subspace.   Through causal probing via activation amplification, we further show that these shared neurons encode consistent and interpretable concept-level effects, demonstrating their functional contribution to inference. Building on this insight, we propose Shared Neuron Low-Rank Fusion (SNRF), a parameter-efficient framework that transfers mature inference circuitry from LLMs to LVLMs. SNRF profiles cross-model activations to identify shared neurons, computes a low-rank approximation of inter-model weight differences, and injects these updates selectively within the shared-neuron subspace. This mechanism strengthens multimodal inference performance with minimal parameter changes and requires no large-scale multimodal fine-tuning.   Across diverse mathematics and perception benchmarks, SNRF consistently enhances LVLM inference performance while preserving perceptual capabilities. Our results demonstrate that shared neurons form an interpretable bridge between LLMs and LVLMs, enabling low-cost transfer of inference ability into multimodal models. Our code is available at [https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons](https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons).",
      "authors": [
        "Chenhang Cui",
        "An Zhang",
        "Yuxin Chen",
        "Gelei Deng",
        "Jingnan Zheng",
        "Zhenkai Liang",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "url": "https://arxiv.org/abs/2602.19058",
      "published": "2026-02-22T06:04:05+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.19049",
      "title": "IAPO: Information-Aware Policy Optimization for Token-Efficient Reasoning",
      "abstract": "Large language models increasingly rely on long chains of thought to improve accuracy, yet such gains come with substantial inference-time costs. We revisit token-efficient post-training and argue that existing sequence-level reward-shaping methods offer limited control over how reasoning effort is allocated across tokens. To bridge the gap, we propose IAPO, an information-theoretic post-training framework that assigns token-wise advantages based on each token's conditional mutual information (MI) with the final answer. This yields an explicit, principled mechanism for identifying informative reasoning steps and suppressing low-utility exploration. We provide a theoretical analysis showing that our IAPO can induce monotonic reductions in reasoning verbosity without harming correctness. Empirically, IAPO consistently improves reasoning accuracy while reducing reasoning length by up to 36%, outperforming existing token-efficient RL methods across various reasoning datasets. Extensive empirical evaluations demonstrate that information-aware advantage shaping is a powerful and general direction for token-efficient post-training. The code is available at https://github.com/YinhanHe123/IAPO.",
      "authors": [
        "Yinhan He",
        "Yaochen Zhu",
        "Mingjia Shi",
        "Wendy Zheng",
        "Lin Su",
        "Xiaoqing Wang",
        "Qi Guo",
        "Jundong Li"
      ],
      "url": "https://arxiv.org/abs/2602.19049",
      "published": "2026-02-22T05:30:14+00:00",
      "categories": [
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "id": "2602.20196",
      "title": "OpenPort Protocol: A Security Governance Specification for AI Agent Tool Access",
      "abstract": "AI agents increasingly require direct, structured access to application data and actions, but production deployments still struggle to express and verify the governance properties that matter in practice: least-privilege authorization, controlled write execution, predictable failure handling, abuse resistance, and auditability. This paper introduces OpenPort Protocol (OPP), a governance-first specification for exposing application tools through a secure server-side gateway that is model- and runtime-neutral and can bind to existing tool ecosystems. OpenPort defines authorization-dependent discovery, stable response envelopes with machine-actionable \\texttt{agent.*} reason codes, and an authorization model combining integration credentials, scoped permissions, and ABAC-style policy constraints. For write operations, OpenPort specifies a risk-gated lifecycle that defaults to draft creation and human review, supports time-bounded auto-execution under explicit policy, and enforces high-risk safeguards including preflight impact binding and idempotency. To address time-of-check/time-of-use drift in delayed approval flows, OpenPort also specifies an optional State Witness profile that revalidates execution-time preconditions and fails closed on state mismatch. Operationally, the protocol requires admission control (rate limits/quotas) with stable 429 semantics and structured audit events across allow/deny/fail paths so that client recovery and incident analysis are deterministic. We present a reference runtime and an executable governance toolchain (layered conformance profiles, negative security tests, fuzz/abuse regression, and release-gate scans) and evaluate the core profile at a pinned release tag using artifact-based, externally reproducible validation.",
      "authors": [
        "Genliang Zhu",
        "Chu Wang",
        "Ziyuan Wang",
        "Zhida Li",
        "Qiang Li"
      ],
      "url": "https://arxiv.org/abs/2602.20196",
      "published": "2026-02-22T05:16:40+00:00",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19043",
      "title": "Uncovering Context Reliance in Unstructured Knowledge Editing",
      "abstract": "Editing Large language models (LLMs) with real-world, unstructured knowledge is essential for correcting and updating their internal parametric knowledge. In this work, we revisit the fundamental next-token prediction (NTP) as a candidate paradigm for unstructured editing. We identify Context Reliance as a critical failure mode of NTP-based approaches, where knowledge acquired from edited text becomes highly dependent on its preceding context, leading to recall failures when that context is absent during inference. This hypothesis is supported by our empirical validation that prepending context during inference recovers knowledge recall. We further theoretically demonstrate that Context Reliance is an inherent consequence of gradient-based optimization, which tends to bind acquired knowledge to a specific aggregated contextual representation. To address this, we propose a simple yet effective COntext-INdependent editing framework (COIN), encouraging model to focus on knowledge within local scope rather than memorizing contextual patterns. Evaluations show that COIN reduces Context Reliance by 45.2% and outperforms strong baselines by 23.6% in editing success rate, highlighting the vital role of mitigating Context Reliance for robust editing.",
      "authors": [
        "Zisheng Zhou",
        "Mengqi Zhang",
        "Shiguang Wu",
        "Xiaotian Ye",
        "Chi Zhang",
        "Zhumin Chen",
        "Pengjie Ren"
      ],
      "url": "https://arxiv.org/abs/2602.19043",
      "published": "2026-02-22T04:44:34+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.19041",
      "title": "Back to Blackwell: Closing the Loop on Intransitivity in Multi-Objective Preference Fine-Tuning",
      "abstract": "A recurring challenge in preference fine-tuning (PFT) is handling $\\textit{intransitive}$ (i.e., cyclic) preferences. Intransitive preferences often stem from either $\\textit{(i)}$ inconsistent rankings along a single objective or $\\textit{(ii)}$ scalarizing multiple objectives into a single metric. Regardless of their source, the downstream implication of intransitive preferences is the same: there is no well-defined optimal policy, breaking a core assumption of the standard PFT pipeline. In response, we propose a novel, game-theoretic solution concept -- the $\\textit{Maximum Entropy Blackwell Winner}$ ($\\textit{MaxEntBW}$) -- that is well-defined under multi-objective intransitive preferences. To enable computing MaxEntBWs at scale, we derive $\\texttt{PROSPER}$: a provably efficient PFT algorithm. Unlike prior self-play techniques, $\\texttt{PROSPER}$ directly handles multiple objectives without requiring scalarization. We then apply $\\texttt{PROSPER}$ to the problem of fine-tuning large language models (LLMs) from multi-objective LLM-as-a-Judge feedback (e.g., rubric-based judges), a setting where both sources of intransitivity arise. We find that $\\texttt{PROSPER}$ outperforms all baselines considered across both instruction following and general chat benchmarks, releasing trained model checkpoints at the 7B and 3B parameter scales.",
      "authors": [
        "Jiahao Zhang",
        "Lujing Zhang",
        "Keltin Grimes",
        "Zhuohao Yu",
        "Gokul Swamy",
        "Zhiwei Steven Wu"
      ],
      "url": "https://arxiv.org/abs/2602.19041",
      "published": "2026-02-22T04:33:51+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19033",
      "title": "A Markovian View of Iterative-Feedback Loops in Image Generative Models: Neural Resonance and Model Collapse",
      "abstract": "AI training datasets will inevitably contain AI-generated examples, leading to ``feedback'' in which the output of one model impacts the training of another. It is known that such iterative feedback can lead to model collapse, yet the mechanisms underlying this degeneration remain poorly understood. Here we show that a broad class of feedback processes converges to a low-dimensional invariant structure in latent space, a phenomenon we call neural resonance. By modeling iterative feedback as a Markov Chain, we show that two conditions are needed for this resonance to occur: ergodicity of the feedback process and directional contraction of the latent representation. By studying diffusion models on MNIST and ImageNet, as well as CycleGAN and an audio feedback experiment, we map how local and global manifold geometry evolve, and we introduce an eight-pattern taxonomy of collapse behaviors. Neural resonance provides a unified explanation for long-term degenerate behavior in generative models and provides practical diagnostics for identifying, characterizing, and eventually mitigating collapse.",
      "authors": [
        "Vibhas Kumar Vats",
        "David J. Crandall",
        "Samuel Goree"
      ],
      "url": "https://arxiv.org/abs/2602.19033",
      "published": "2026-02-22T04:05:04+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "id": "2602.20195",
      "title": "OrgFlow: Generative Modeling of Organic Crystal Structures from Molecular Graphs",
      "abstract": "Crystal structure prediction is a long-standing challenge in materials science, with most data-driven methods developed for inorganic systems. This leaves an important gap for organic crystals, which are central to pharmaceuticals, polymers, and functional materials, but present unique challenges, such as larger unit cells and strict chemical connectivity. We introduce a flow-matching model for predicting organic crystal structures directly from molecular graphs. The architecture integrates molecular connectivity with periodic boundary conditions while preserving the symmetries of crystalline systems. A bond-aware loss guides the model toward realistic local chemistry by enforcing distributions of bond lengths and connectivity. To support reliable and efficient training, we built a curated dataset of organic crystals, along with a preprocessing pipeline that precomputes bonds and edges, substantially reducing computational overhead during both training and inference. Experiments show that our method achieves a Match Rate more than 10 times higher than existing baselines while requiring fewer sampling steps for inference. These results establish generative modeling as a practical and scalable framework for organic crystal structure prediction.",
      "authors": [
        "Mohammadmahdi Vahediahmar",
        "Matthew A. McDonald",
        "Feng Liu"
      ],
      "url": "https://arxiv.org/abs/2602.20195",
      "published": "2026-02-22T04:01:06+00:00",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19027",
      "title": "Pushing the Limits of Inverse Lithography with Generative Reinforcement Learning",
      "abstract": "Inverse lithography (ILT) is critical for modern semiconductor manufacturing but suffers from highly non-convex objectives that often trap optimization in poor local minima. Generative AI has been explored to warm-start ILT, yet most approaches train deterministic image-to-image translators to mimic sub-optimal datasets, providing limited guidance for escaping non-convex traps during refinement. We reformulate mask synthesis as conditional sampling: a generator learns a distribution over masks conditioned on the design and proposes multiple candidates. The generator is first pretrained with WGAN plus a reconstruction loss, then fine-tuned using Group Relative Policy Optimization (GRPO) with an ILT-guided imitation loss. At inference, we sample a small batch of masks, run fast batched ILT refinement, evaluate lithography metrics (e.g., EPE, process window), and select the best candidate. On \\texttt{LithoBench} dataset, the proposed hybrid framework reduces EPE violations under a 3\\,nm tolerance and roughly doubles throughput versus a strong numerical ILT baseline, while improving final mask quality. We also present over 20\\% EPE improvement on \\texttt{ICCAD13} contest cases with 3$\\times$ speedup over the SOTA numerical ILT solver. By learning to propose ILT-friendly initializations, our approach mitigates non-convexity and advances beyond what traditional solvers or GenAI can achieve.",
      "authors": [
        "Haoyu Yang",
        "Haoxing Ren"
      ],
      "url": "https://arxiv.org/abs/2602.19027",
      "published": "2026-02-22T03:30:39+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19025",
      "title": "Routing-Aware Explanations for Mixture of Experts Graph Models in Malware Detection",
      "abstract": "Mixture-of-Experts (MoE) offers flexible graph reasoning by combining multiple views of a graph through a learned router. We investigate routing-aware explanations for MoE graph models in malware detection using control flow graphs (CFGs). Our architecture builds diversity at two levels. At the node level, each layer computes multiple neighborhood statistics and fuses them with an MLP, guided by a degree reweighting factor rho and a pooling choice lambda in {mean, std, max}, producing distinct node representations that capture complementary structural cues in CFGs. At the readout level, six experts, each tied to a specific (rho, lambda) view, output graph-level logits that the router weights into a final prediction. Post-hoc explanations are generated with edge-level attributions per expert and aggregated using the router gates so the rationale reflects both what each expert highlights and how strongly it is selected. Evaluated against single-expert GNN baselines such as GCN, GIN, and GAT on the same CFG dataset, the proposed MoE achieves strong detection accuracy while yielding stable, faithful attributions under sparsity-based perturbations. The results indicate that making the router explicit and combining multi-statistic node encoding with expert-level diversity can improve the transparency of MoE decisions for malware analysis.",
      "authors": [
        "Hossein Shokouhinejad",
        "Roozbeh Razavi-Far",
        "Griffin Higgins",
        "Ali. A Ghorbani"
      ],
      "url": "https://arxiv.org/abs/2602.19025",
      "published": "2026-02-22T03:27:17+00:00",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "id": "2602.20194",
      "title": "FedAvg-Based CTMC Hazard Model for Federated Bridge Deterioration Assessment",
      "abstract": "Bridge periodic inspection records contain sensitive information about public infrastructure, making cross-organizational data sharing impractical under existing data governance constraints. We propose a federated framework for estimating a Continuous-Time Markov Chain (CTMC) hazard model of bridge deterioration, enabling municipalities to collaboratively train a shared benchmark model without transferring raw inspection records. Each User holds local inspection data and trains a log-linear hazard model over three deterioration-direction transitions -- Good$\\to$Minor, Good$\\to$Severe, and Minor$\\to$Severe -- with covariates for bridge age, coastline distance, and deck area. Local optimization is performed via mini-batch stochastic gradient descent on the CTMC log-likelihood, and only a 12-dimensional pseudo-gradient vector is uploaded to a central server per communication round. The server aggregates User updates using sample-weighted Federated Averaging (FedAvg) with momentum and gradient clipping. All experiments in this paper are conducted on fully synthetic data generated from a known ground-truth parameter set with region-specific heterogeneity, enabling controlled evaluation of federated convergence behaviour. Simulation results across heterogeneous Users show consistent convergence of the average negative log-likelihood, with the aggregated gradient norm decreasing as User scale increases. Furthermore, the federated update mechanism provides a natural participation incentive: Users who register their local inspection datasets on a shared technical-standard platform receive in return the periodically updated global benchmark parameters -- information that cannot be obtained from local data alone -- thereby enabling evidence-based life-cycle planning without surrendering data sovereignty.",
      "authors": [
        "Takato Yasuno"
      ],
      "url": "https://arxiv.org/abs/2602.20194",
      "published": "2026-02-22T03:24:03+00:00",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "id": "2602.19022",
      "title": "An interpretable framework using foundation models for fish sex identification",
      "abstract": "Accurate sex identification in fish is vital for optimizing breeding and management strategies in aquaculture, particularly for species at the risk of extinction. However, most existing methods are invasive or stressful and may cause additional mortality, posing severe risks to threatened or endangered fish populations. To address these challenges, we propose FishProtoNet, a robust, non-invasive computer vision-based framework for sex identification of delta smelt (Hypomesus transpacificus), an endangered fish species native to California, across its full life cycle. Unlike the traditional deep learning methods, FishProtoNet provides interpretability through learned prototype representations while improving robustness by leveraging foundation models to reduce the influence of background noise. Specifically, the FishProtoNet framework consists of three key components: fish regions of interest (ROIs) extraction using visual foundation model, feature extraction from fish ROIs and fish sex identification based on an interpretable prototype network. FishProtoNet demonstrates strong performance in delta smelt sex identification during early spawning and post-spawning stages, achieving the accuracies of 74.40% and 81.16% and corresponding F1 scores of 74.27% and 79.43% respectively. In contrast, delta smelt sex identification at the subadult stage remains challenging for current computer vision methods, likely due to less pronounced morphological differences in immature fish. The source code of FishProtoNet is publicly available at: https://github.com/zhengmiao1/Fish_sex_identification",
      "authors": [
        "Zheng Miao",
        "Tien-Chieh Hung"
      ],
      "url": "https://arxiv.org/abs/2602.19022",
      "published": "2026-02-22T03:21:26+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.19020",
      "title": "Learning to Detect Language Model Training Data via Active Reconstruction",
      "abstract": "Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce \\textbf{Active Data Reconstruction Attack} (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are \\textit{more reconstructible} than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, \\textsc{ADRA} and its adaptive variant \\textsc{ADRA+}, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\\% over the previous runner-up. In particular, \\MethodPlus~improves over Min-K\\%++ by 18.8\\% on BookMIA for pre-training detection and by 7.6\\% on AIME for post-training detection.",
      "authors": [
        "Junjie Oscar Yin",
        "John X. Morris",
        "Vitaly Shmatikov",
        "Sewon Min",
        "Hannaneh Hajishirzi"
      ],
      "url": "https://arxiv.org/abs/2602.19020",
      "published": "2026-02-22T03:20:06+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.19017",
      "title": "Why ReLU? A Bit-Model Dichotomy for Deep Network Training",
      "abstract": "Theoretical analyses of Empirical Risk Minimization (ERM) are standardly framed within the Real-RAM model of computation. In this setting, training even simple neural networks is known to be $\\exists \\mathbb{R}$-complete -- a complexity class believed to be harder than NP, that characterizes the difficulty of solving systems of polynomial inequalities over the real numbers. However, this algebraic framework diverges from the reality of digital computation with finite-precision hardware. In this work, we analyze the theoretical complexity of ERM under a realistic bit-level model ($\\mathsf{ERM}_{\\text{bit}}$), where network parameters and inputs are constrained to be rational numbers with polynomially bounded bit-lengths. Under this model, we reveal a sharp dichotomy in tractability governed by the network's activation function. We prove that for deep networks with {\\em any} polynomial activations with rational coefficients and degree at least $2$, the bit-complexity of training is severe: deciding $\\mathsf{ERM}_{\\text{bit}}$ is $\\#P$-Hard, hence believed to be strictly harder than NP-complete problems. Furthermore, we show that determining the sign of a single partial derivative of the empirical loss function is intractable (unlikely in BPP), and deciding a specific bit in the gradient is $\\#P$-Hard. This provides a complexity-theoretic perspective for the phenomenon of exploding and vanishing gradients. In contrast, we show that for piecewise-linear activations such as ReLU, the precision requirements remain manageable: $\\mathsf{ERM}_{\\text{bit}}$ is contained within NP (specifically NP-complete), and standard backpropagation runs in polynomial time. Our results demonstrate that finite-precision constraints are not merely implementation details but fundamental determinants of learnability.",
      "authors": [
        "Ilan Doron-Arad",
        "Elchanan Mossel"
      ],
      "url": "https://arxiv.org/abs/2602.19017",
      "published": "2026-02-22T03:12:44+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.19008",
      "title": "Capable but Unreliable: Canonical Path Deviation as a Causal Mechanism of Agent Failure in Long-Horizon Tasks",
      "abstract": "Why do language agents fail on tasks they are capable of solving? We argue that many such failures are reliability failures caused by stochastic drift from a task's latent solution structure, not capability failures. Every well-defined tool-use task imposes a canonical solution path (i.e., a convergent set of tool invocations shared across successful runs) and agent success depends critically on whether a trajectory stays within this path's operating envelope. We establish this causally using a natural experiment that holds model capability and task difficulty fixed by construction. We analyze trajectories from the Toolathlon benchmark: 22 frontier models each attempt 108 real-world tool-use tasks across 3 independent runs, yielding 515 model$\\times$task units where the same model succeeds on some runs and fails on others due to LLM sampling stochasticity alone. Within these units, successful runs adhere significantly more closely to the canonical solution path than failed runs ($+$0.060 Jaccard, $p<0.0001$, $n=488$ units, 95% CI [+0.043, +0.077]). This result survives six robustness checks including cross-model-family leave-one-out validation. Critically, the causal mechanism is gradual and self-reinforcing: the adherence gap is statistically indistinguishable from zero through the first 50% of the trajectory, ruling out early-branching selection bias, and each off-canonical tool call raises the probability that the next call is also off-canonical by 22.7 percentage points ($\\hatβ=+0.227$, $p<0.0001$), more than doubling the baseline rate. These findings imply that agent reliability cannot be improved by capability scaling alone, but offer a highly actionable intervention: a simple monitor that restarts the bottom tercile of runs based on mid-trajectory canonical adherence lifts success rates by $+$8.8 percentage points among intervened runs.",
      "authors": [
        "Wilson Y. Lee"
      ],
      "url": "https://arxiv.org/abs/2602.19008",
      "published": "2026-02-22T02:37:57+00:00",
      "categories": [
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19005",
      "title": "GUIDE-US: Grade-Informed Unpaired Distillation of Encoder Knowledge from Histopathology to Micro-UltraSound",
      "abstract": "Purpose: Non-invasive grading of prostate cancer (PCa) from micro-ultrasound (micro-US) could expedite triage and guide biopsies toward the most aggressive regions, yet current models struggle to infer tissue micro-structure at coarse imaging resolutions.   Methods: We introduce an unpaired histopathology knowledge-distillation strategy that trains a micro-US encoder to emulate the embedding distribution of a pretrained histopathology foundation model, conditioned on International Society of Urological Pathology (ISUP) grades. Training requires no patient-level pairing or image registration, and histopathology inputs are not used at inference.   Results: Compared to the current state of the art, our approach increases sensitivity to clinically significant PCa (csPCa) at 60% specificity by 3.5% and improves overall sensitivity at 60% specificity by 1.2%.   Conclusion: By enabling earlier and more dependable cancer risk stratification solely from imaging, our method advances clinical feasibility. Source code will be publicly released upon publication.",
      "authors": [
        "Emma Willis",
        "Tarek Elghareb",
        "Paul F. R. Wilson",
        "Minh Nguyen Nhat To",
        "Mohammad Mahdi Abootorabi",
        "Amoon Jamzad",
        "Brian Wodlinger",
        "Parvin Mousavi",
        "Purang Abolmaesumi"
      ],
      "url": "https://arxiv.org/abs/2602.19005",
      "published": "2026-02-22T02:02:36+00:00",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "id": "2602.19000",
      "title": "MagicAgent: Towards Generalized Agent Planning",
      "abstract": "The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data but also by inherent conflicts across heterogeneous planning tasks. These challenges result in models that excel at isolated tasks yet struggle to generalize, while existing multi-task training attempts suffer from gradient interference. In this paper, we present \\textbf{MagicAgent}, a series of foundation models specifically designed for generalized agent planning. We introduce a lightweight and scalable synthetic data framework that generates high-quality trajectories across diverse planning tasks, including hierarchical task decomposition, tool-augmented planning, multi-constraint scheduling, procedural logic orchestration, and long-horizon tool execution. To mitigate training conflicts, we propose a two-stage training paradigm comprising supervised fine-tuning followed by multi-objective reinforcement learning over both static datasets and dynamic environments. Empirical results demonstrate that MagicAgent-32B and MagicAgent-30B-A3B deliver superior performance, achieving accuracies of $75.1\\%$ on Worfbench, $55.9\\%$ on NaturalPlan, $57.5\\%$ on $τ^2$-Bench, $86.9\\%$ on BFCL-v3, and $81.2\\%$ on ACEBench, as well as strong results on our in-house MagicEval benchmarks. These results substantially outperform existing sub-100B models and even surpass leading closed-source models.",
      "authors": [
        "Xuhui Ren",
        "Shaokang Dong",
        "Chen Yang",
        "Qing Gao",
        "Yunbin Zhao",
        "Yongsheng Liu",
        "Xinwei Geng",
        "Xiang Li",
        "Demei Yan",
        "Yanqing Li",
        "Chenhao Huang",
        "Dingwei Zhu",
        "Junjie Ye",
        "Boxuan Yue",
        "Yingnan Fu",
        "Mengzhe Lv",
        "Zezeng Feng",
        "Boshen Zhou",
        "Bocheng Wang",
        "Xuanjing Huang",
        "Yu-Gang Jiang",
        "Tao Gui",
        "Qi Zhang",
        "Yunke Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.19000",
      "published": "2026-02-22T01:39:16+00:00",
      "categories": [
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "id": "2602.18998",
      "title": "Benchmark Test-Time Scaling of General LLM Agents",
      "abstract": "LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench.",
      "authors": [
        "Xiaochuan Li",
        "Ryan Ming",
        "Pranav Setlur",
        "Abhijay Paladugu",
        "Andy Tang",
        "Hao Kang",
        "Shuai Shao",
        "Rong Jin",
        "Chenyan Xiong"
      ],
      "url": "https://arxiv.org/abs/2602.18998",
      "published": "2026-02-22T01:08:02+00:00",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.18997",
      "title": "Implicit Bias and Convergence of Matrix Stochastic Mirror Descent",
      "abstract": "We investigate Stochastic Mirror Descent (SMD) with matrix parameters and vector-valued predictions, a framework relevant to multi-class classification and matrix completion problems. Focusing on the overparameterized regime, where the total number of parameters exceeds the number of training samples, we prove that SMD with matrix mirror functions $ψ(\\cdot)$ converges exponentially to a global interpolator. Furthermore, we generalize classical implicit bias results of vector SMD by demonstrating that the matrix SMD algorithm converges to the unique solution minimizing the Bregman divergence induced by $ψ(\\cdot)$ from initialization subject to interpolating the data. These findings reveal how matrix mirror maps dictate inductive bias in high-dimensional, multi-output problems.",
      "authors": [
        "Danil Akhtiamov",
        "Reza Ghane",
        "Babak Hassibi"
      ],
      "url": "https://arxiv.org/abs/2602.18997",
      "published": "2026-02-22T00:59:28+00:00",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC"
      ]
    },
    {
      "id": "2602.18986",
      "title": "Quantifying Automation Risk in High-Automation AI Systems: A Bayesian Framework for Failure Propagation and Optimal Oversight",
      "abstract": "Organizations across finance, healthcare, transportation, content moderation, and critical infrastructure are rapidly deploying highly automated AI systems, yet they lack principled methods to quantify how increasing automation amplifies harm when failures occur. We propose a parsimonious Bayesian risk decomposition expressing expected loss as the product of three terms: the probability of system failure, the conditional probability that a failure propagates into harm given the automation level, and the expected severity of harm. This framework isolates a critical quantity -- the conditional probability that failures propagate into harm -- which captures execution and oversight risk rather than model accuracy alone. We develop complete theoretical foundations: formal proofs of the decomposition, a harm propagation equivalence theorem linking the harm propagation probability to observable execution controls, risk elasticity measures, efficient frontier analysis for automation policy, and optimal resource allocation principles with second-order conditions. We motivate the framework with an illustrative case study of the 2012 Knight Capital incident ($440M loss) as one instantiation of a broadly applicable failure pattern, and characterize the research design required to empirically validate the framework at scale across deployment domains. This work provides the theoretical foundations for a new class of deployment-focused risk governance tools for agentic and automated AI systems.",
      "authors": [
        "Vishal Srivastava",
        "Tanmay Sah"
      ],
      "url": "https://arxiv.org/abs/2602.18986",
      "published": "2026-02-22T00:18:23+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.18985",
      "title": "InfEngine: A Self-Verifying and Self-Optimizing Intelligent Engine for Infrared Radiation Computing",
      "abstract": "Infrared radiation computing underpins advances in climate science, remote sensing and spectroscopy but remains constrained by manual workflows. We introduce InfEngine, an autonomous intelligent computational engine designed to drive a paradigm shift from human-led orchestration to collaborative automation. It integrates four specialized agents through two core innovations: self-verification, enabled by joint solver-evaluator debugging, improves functional correctness and scientific plausibility; self-optimization, realized via evolutionary algorithms with self-discovered fitness functions, facilitates autonomous performance optimization. Evaluated on InfBench with 200 infrared-specific tasks and powered by InfTools with 270 curated tools, InfEngine achieves a 92.7% pass rate and delivers workflows 21x faster than manual expert effort. More fundamentally, it illustrates how researchers can transition from manual coding to collaborating with self-verifying, self-optimizing computational partners. By generating reusable, verified and optimized code, InfEngine transforms computational workflows into persistent scientific assets, accelerating the cycle of scientific discovery. Code: https://github.com/kding1225/infengine",
      "authors": [
        "Kun Ding",
        "Jian Xu",
        "Ying Wang",
        "Peipei Yang",
        "Shiming Xiang"
      ],
      "url": "https://arxiv.org/abs/2602.18985",
      "published": "2026-02-22T00:09:26+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20193",
      "title": "When Backdoors Go Beyond Triggers: Semantic Drift in Diffusion Models Under Encoder Attacks",
      "abstract": "Standard evaluations of backdoor attacks on text-to-image (T2I) models primarily measure trigger activation and visual fidelity. We challenge this paradigm, demonstrating that encoder-side poisoning induces persistent, trigger-free semantic corruption that fundamentally reshapes the representation manifold. We trace this vulnerability to a geometric mechanism: a Jacobian-based analysis reveals that backdoors act as low-rank, target-centered deformations that amplify local sensitivity, causing distortion to propagate coherently across semantic neighborhoods. To rigorously quantify this structural degradation, we introduce SEMAD (Semantic Alignment and Drift), a diagnostic framework that measures both internal embedding drift and downstream functional misalignment. Our findings, validated across diffusion and contrastive paradigms, expose the deep structural risks of encoder poisoning and highlight the necessity of geometric audits beyond simple attack success rates.",
      "authors": [
        "Shenyang Chen",
        "Liuwan Zhu"
      ],
      "url": "https://arxiv.org/abs/2602.20193",
      "published": "2026-02-21T23:48:04+00:00",
      "categories": [
        "cs.CR",
        "cs.AI"
      ]
    },
    {
      "id": "2602.18982",
      "title": "Conditionally Site-Independent Neural Evolution of Antibody Sequences",
      "abstract": "Common deep learning approaches for antibody engineering focus on modeling the marginal distribution of sequences. By treating sequences as independent samples, however, these methods overlook affinity maturation as a rich and largely untapped source of information about the evolutionary process by which antibodies explore the underlying fitness landscape. In contrast, classical phylogenetic models explicitly represent evolutionary dynamics but lack the expressivity to capture complex epistatic interactions. We bridge this gap with CoSiNE, a continuous-time Markov chain parameterized by a deep neural network. Mathematically, we prove that CoSiNE provides a first-order approximation to the intractable sequential point mutation process, capturing epistatic effects with an error bound that is quadratic in branch length. Empirically, CoSiNE outperforms state-of-the-art language models in zero-shot variant effect prediction by explicitly disentangling selection from context-dependent somatic hypermutation. Finally, we introduce Guided Gillespie, a classifier-guided sampling scheme that steers CoSiNE at inference time, enabling efficient optimization of antibody binding affinity toward specific antigens.",
      "authors": [
        "Stephen Zhewen Lu",
        "Aakarsh Vermani",
        "Kohei Sanno",
        "Jiarui Lu",
        "Frederick A Matsen",
        "Milind Jagota",
        "Yun S. Song"
      ],
      "url": "https://arxiv.org/abs/2602.18982",
      "published": "2026-02-21T23:23:30+00:00",
      "categories": [
        "cs.LG",
        "q-bio.PE"
      ]
    },
    {
      "id": "2602.18981",
      "title": "How Far Can We Go with Pixels Alone? A Pilot Study on Screen-Only Navigation in Commercial 3D ARPGs",
      "abstract": "Modern 3D game levels rely heavily on visual guidance, yet the navigability of level layouts remains difficult to quantify. Prior work either simulates play in simplified environments or analyzes static screenshots for visual affordances, but neither setting faithfully captures how players explore complex, real-world game levels. In this paper, we build on an existing open-source visual affordance detector and instantiate a screen-only exploration and navigation agent that operates purely from visual affordances. Our agent consumes live game frames, identifies salient interest points, and drives a simple finite-state controller over a minimal action space to explore Dark Souls-style linear levels and attempt to reach expected goal regions. Pilot experiments show that the agent can traverse most required segments and exhibits meaningful visual navigation behavior, but also highlight that limitations of the underlying visual model prevent truly comprehensive and reliable auto-navigation. We argue that this system provides a concrete, shared baseline and evaluation protocol for visual navigation in complex games, and we call for more attention to this necessary task. Our results suggest that purely vision-based sense-making models, with discrete single-modality inputs and without explicit reasoning, can effectively support navigation and environment understanding in idealized settings, but are unlikely to be a general solution on their own.",
      "authors": [
        "Kaijie Xu",
        "Mustafa Bugti",
        "Clark Verbrugge"
      ],
      "url": "https://arxiv.org/abs/2602.18981",
      "published": "2026-02-21T23:15:18+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.18971",
      "title": "When Do LLM Preferences Predict Downstream Behavior?",
      "abstract": "Preference-driven behavior in LLMs may be a necessary precondition for AI misalignment such as sandbagging: models cannot strategically pursue misaligned goals unless their behavior is influenced by their preferences. Yet prior work has typically prompted models explicitly to act in specific ways, leaving unclear whether observed behaviors reflect instruction-following capabilities vs underlying model preferences. Here we test whether this precondition for misalignment is present. Using entity preferences as a behavioral probe, we measure whether stated preferences predict downstream behavior in five frontier LLMs across three domains: donation advice, refusal behavior, and task performance. Conceptually replicating prior work, we first confirm that all five models show highly consistent preferences across two independent measurement methods. We then test behavioral consequences in a simulated user environment. We find that all five models give preference-aligned donation advice. All five models also show preference-correlated refusal patterns when asked to recommend donations, refusing more often for less-preferred entities. All preference-related behaviors that we observe here emerge without instructions to act on preferences. Results for task performance are mixed: on a question-answering benchmark (BoolQ), two models show small but significant accuracy differences favoring preferred entities; one model shows the opposite pattern; and two models show no significant relationship. On complex agentic tasks, we find no evidence of preference-driven performance differences. While LLMs have consistent preferences that reliably predict advice-giving behavior, these preferences do not consistently translate into downstream task performance.",
      "authors": [
        "Katarina Slama",
        "Alexandra Souly",
        "Dishank Bansal",
        "Henry Davidson",
        "Christopher Summerfield",
        "Lennart Luettgau"
      ],
      "url": "https://arxiv.org/abs/2602.18971",
      "published": "2026-02-21T22:24:51+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.18968",
      "title": "Robust and Efficient Tool Orchestration via Layered Execution Structures with Reflective Correction",
      "abstract": "Tool invocation is a core capability of agentic systems, yet failures often arise not from individual tool calls but from how multiple tools are organized and executed together. Existing approaches tightly couple tool execution with stepwise language reasoning or explicit planning, leading to brittle behavior and high execution overhead. To overcome these limitations, we revisit tool invocation from the perspective of tool orchestration. Our key insight is that effective orchestration does not require precise dependency graphs or fine-grained planning. Instead, a coarse-grained layer structure suffices to provide global guidance, while execution-time errors can be corrected locally. Specifically, we model tool orchestration as learning a layered execution structure that captures high-level tool dependencies, inducing layer-wise execution through context constraints. To handle execution-time failures, we introduce a schema-aware reflective correction mechanism that detects and repairs errors locally. This design confines errors to individual tool calls and avoids re-planning entire execution trajectories. This structured execution paradigm enables a lightweight and reusable orchestration component for agentic systems. Experimental results show that our approach achieves robust tool execution while reducing execution complexity and overhead. Code will be made publicly available.",
      "authors": [
        "Tao Zhe",
        "Haoyu Wang",
        "Bo Luo",
        "Min Wu",
        "Wei Fan",
        "Xiao Luo",
        "Zijun Yao",
        "Haifeng Chen",
        "Dongjie Wang"
      ],
      "url": "https://arxiv.org/abs/2602.18968",
      "published": "2026-02-21T22:20:01+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.18966",
      "title": "Whisper: Courtside Edition Enhancing ASR Performance Through LLM-Driven Context Generation",
      "abstract": "Domain-specific speech remains a persistent challenge for automatic speech recognition (ASR), even for state-of-the-art systems like OpenAI's Whisper. We introduce Whisper: Courtside Edition, a novel multi-agent large language model (LLM) pipeline that enhances Whisper transcriptions without retraining. The pipeline intercepts Whisper's initial transcript, applies specialized LLM agents for domain context identification, named entity recognition, and jargon detection, and generates compact prompts that guide Whisper's decoder. Evaluated on 421 NBA basketball commentary segments (a domain characterized by dense proper nouns and technical terminology) our best pipeline achieves a statistically significant 17.0% relative reduction in word error rate (WER; from 0.217 to 0.180, p<0.001). Improvements are observed in 40.1% of segments with degradation in only 7.1%, substantially outperforming direct transcript post-editing. These results demonstrate that prompt-based augmentation can deliver scalable domain adaptation for ASR, offering a practical alternative to costly model fine-tuning.",
      "authors": [
        "Yonathan Ron",
        "Shiri Gilboa",
        "Tammuz Dubnov"
      ],
      "url": "https://arxiv.org/abs/2602.18966",
      "published": "2026-02-21T22:15:59+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.18964",
      "title": "Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language",
      "abstract": "Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in low-resource languages where annotated datasets are scarce or nonexistent. We present \\textbf{Yor-Sarc}, the first gold-standard dataset for sarcasm detection in Yorùbá, a tonal Niger-Congo language spoken by over $50$ million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yorùbá sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by a comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss' $κ= 0.7660$; pairwise Cohen's $κ= 0.6732$--$0.8743$), with $83.3\\%$ unanimous consensus. One annotator pair achieved almost perfect agreement ($κ= 0.8743$; $93.8\\%$ raw agreement), exceeding a number of reported benchmarks for English sarcasm research works. The remaining $16.7\\%$ majority-agreement cases are preserved as soft labels for uncertainty-aware modelling. Yor-Sarc\\footnote{https://github.com/toheebadura/yor-sarc} is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages.",
      "authors": [
        "Toheeb Aduramomi Jimoh",
        "Tabea De Wille",
        "Nikola S. Nikolov"
      ],
      "url": "https://arxiv.org/abs/2602.18964",
      "published": "2026-02-21T22:10:18+00:00",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "id": "2602.18962",
      "title": "NeuroWise: A Multi-Agent LLM \"Glass-Box\" System for Practicing Double-Empathy Communication with Autistic Partners",
      "abstract": "The double empathy problem frames communication difficulties between neurodivergent and neurotypical individuals as arising from mutual misunderstanding, yet most interventions focus on autistic individuals. We present NeuroWise, a multi-agent LLM-based coaching system that supports neurotypical users through stress visualization, interpretation of internal experiences, and contextual guidance. In a between-subjects study (N=30), NeuroWise was rated as helpful by all participants and showed a significant condition-time effect on deficit-based attributions (p=0.02): NeuroWise users reduced deficit framing, while baseline users shifted toward blaming autistic \"deficits\" after difficult interactions. NeuroWise users also completed conversations more efficiently (37% fewer turns, p=0.03). These findings suggest that AI-based interpretation can support attributional change by helping users recognize communication challenges as mutual.",
      "authors": [
        "Albert Tang",
        "Yifan Mo",
        "Jie Li",
        "Yue Su",
        "Mengyuan Zhang",
        "Sander L. Koole",
        "Koen Hindriks",
        "Jiahuan Pei"
      ],
      "url": "https://arxiv.org/abs/2602.18962",
      "published": "2026-02-21T21:54:43+00:00",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.IR",
        "cs.MA"
      ]
    },
    {
      "id": "2602.18960",
      "title": "Modularity is the Bedrock of Natural and Artificial Intelligence",
      "abstract": "The remarkable performance of modern AI systems has been driven by unprecedented scales of data, computation, and energy -- far exceeding the resources required by human intelligence. This disparity highlights the need for new guiding principles and motivates drawing inspiration from the fundamental organizational principles of brain computation. Among these principles, modularity has been shown to be critical for supporting the efficient learning and strong generalization abilities consistently exhibited by humans. Furthermore, modularity aligns well with the No Free Lunch Theorem, which highlights the need for problem-specific inductive biases and motivates architectures composed of specialized components that solve subproblems. However, despite its fundamental role in natural intelligence and its demonstrated benefits across a range of seemingly disparate AI subfields, modularity remains relatively underappreciated in mainstream AI research. In this work, we review several research threads in artificial intelligence and neuroscience through a conceptual framework that highlights the central role of modularity in supporting both artificial and natural intelligence. In particular, we examine what computational advantages modularity provides, how it has emerged as a solution across several AI research areas, which modularity principles the brain exploits, and how modularity can help bridge the gap between natural and artificial intelligence.",
      "authors": [
        "Alessandro Salatiello"
      ],
      "url": "https://arxiv.org/abs/2602.18960",
      "published": "2026-02-21T21:47:09+00:00",
      "categories": [
        "cs.AI",
        "cs.NE",
        "q-bio.NC"
      ]
    },
    {
      "id": "2602.18956",
      "title": "INDUCTION: Finite-Structure Concept Synthesis in First-Order Logic",
      "abstract": "We introduce INDUCTION, a benchmark for finite structure concept synthesis in first order logic. Given small finite relational worlds with extensionally labeled target predicates, models must output a single first order logical formula that explains the target uniformly across worlds, with correctness verified via exact model checking. The benchmark includes three regimes, FullObs, CI (contrastive), and EC (existential completion), nd penalizes formula bloat. We find sharp difficulty gradients, persistent hard structural families, and observe that low bloat formulas generalize far better on held out worlds. Elite recent models show qualitatively different behaviors across tasks and performance metrics, hinting to their different strategies of concept generalization.",
      "authors": [
        "Serafim Batzoglou"
      ],
      "url": "https://arxiv.org/abs/2602.18956",
      "published": "2026-02-21T21:21:40+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.20191",
      "title": "MoBiQuant: Mixture-of-Bits Quantization for Token-Adaptive Elastic LLMs",
      "abstract": "Changing runtime complexity on cloud and edge devices necessitates elastic large language model (LLM) deployment, where an LLM can be inferred with various quantization precisions based on available computational resources. However, it has been observed that the calibration parameters for quantization are typically linked to specific precisions, which presents challenges during elastic-precision calibration and precision switching at runtime. In this work, we attribute the source of varying calibration parameters to the varying token-level sensitivity caused by a precision-dependent outlier migration phenomenon.Motivated by this observation, we propose \\texttt{MoBiQuant}, a novel Mixture-of-Bits quantization framework that adjusts weight precision for elastic LLM inference based on token sensitivity. Specifically, we propose the many-in-one recursive residual quantization that can iteratively reconstruct higher-precision weights and the token-aware router to dynamically select the number of residual bit slices. MoBiQuant enables smooth precision switching while improving generalization for the distribution of token outliers. Experimental results demonstrate that MoBiQuant exhibits strong elasticity, enabling it to match the performance of bit-specific calibrated PTQ on LLaMA3-8B without repeated calibration.",
      "authors": [
        "Dongwei Wang",
        "Jinhee Kim",
        "Seokho Han",
        "Denis Gudovskiy",
        "Yohei Nakata",
        "Tomoyuki Okuno",
        "KhayTze Peong",
        "Kang Eun Jeon",
        "Jong Hwan Ko",
        "Yiran Chen",
        "Huanrui Yang"
      ],
      "url": "https://arxiv.org/abs/2602.20191",
      "published": "2026-02-21T21:11:08+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.18955",
      "title": "Incremental Transformer Neural Processes",
      "abstract": "Neural Processes (NPs), and specifically Transformer Neural Processes (TNPs), have demonstrated remarkable performance across tasks ranging from spatiotemporal forecasting to tabular data modelling. However, many of these applications are inherently sequential, involving continuous data streams such as real-time sensor readings or database updates. In such settings, models should support cheap, incremental updates rather than recomputing internal representations from scratch for every new observation -- a capability existing TNP variants lack. Drawing inspiration from Large Language Models, we introduce the Incremental TNP (incTNP). By leveraging causal masking, Key-Value (KV) caching, and a data-efficient autoregressive training strategy, incTNP matches the predictive performance of standard TNPs while reducing the computational cost of updates from quadratic to linear time complexity. We empirically evaluate our model on a range of synthetic and real-world tasks, including tabular regression and temperature prediction. Our results show that, surprisingly, incTNP delivers performance comparable to -- or better than -- non-causal TNPs while unlocking orders-of-magnitude speedups for sequential inference. Finally, we assess the consistency of the model's updates -- by adapting a metric of ``implicit Bayesianness\", we show that incTNP retains a prediction rule as implicitly Bayesian as standard non-causal TNPs, demonstrating that incTNP achieves the computational benefits of causal masking without sacrificing the consistency required for streaming inference.",
      "authors": [
        "Philip Mortimer",
        "Cristiana Diaconu",
        "Tommy Rochussen",
        "Bruno Mlodozeniec",
        "Richard E. Turner"
      ],
      "url": "https://arxiv.org/abs/2602.18955",
      "published": "2026-02-21T20:30:04+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.18948",
      "title": "Toward Manifest Relationality in Transformers via Symmetry Reduction",
      "abstract": "Transformer models contain substantial internal redundancy arising from coordinate-dependent representations and continuous symmetries, in model space and in head space, respectively. While recent approaches address this by explicitly breaking symmetry, we propose a complementary framework based on symmetry reduction. We reformulate representations, attention mechanisms, and optimization dynamics in terms of invariant relational quantities, eliminating redundant degrees of freedom by construction. This perspective yields architectures that operate directly on relational structures, providing a principled geometric framework for reducing parameter redundancy and analyzing optimization.",
      "authors": [
        "J. François",
        "L. Ravera"
      ],
      "url": "https://arxiv.org/abs/2602.18948",
      "published": "2026-02-21T19:43:17+00:00",
      "categories": [
        "cs.LG",
        "cs.NE",
        "hep-th",
        "stat.ML"
      ]
    },
    {
      "id": "2602.18947",
      "title": "(Perlin) Noise as AI coordinator",
      "abstract": "Large scale control of nonplayer agents is central to modern games, while production systems still struggle to balance several competing goals: locally smooth, natural behavior, and globally coordinated variety across space and time. Prior approaches rely on handcrafted rules or purely stochastic triggers, which either converge to mechanical synchrony or devolve into uncorrelated noise that is hard to tune. Continuous noise signals such as Perlin noise are well suited to this gap because they provide spatially and temporally coherent randomness, and they are already widely used for terrain, biomes, and other procedural assets. We adapt these signals for the first time to large scale AI control and present a general framework that treats continuous noise fields as an AI coordinator. The framework combines three layers of control: behavior parameterization for movement at the agent level, action time scheduling for when behaviors start and stop, and spawn or event type and feature generation for what appears and where. We instantiate the framework reproducibly and evaluate Perlin noise as a representative coordinator across multiple maps, scales, and seeds against random, filtered, deterministic, neighborhood constrained, and physics inspired baselines. Experiments show that coordinated noise fields provide stable activation statistics without lockstep, strong spatial coverage and regional balance, better diversity with controllable polarization, and competitive runtime. We hope this work motivates a broader exploration of coordinated noise in game AI as a practical path to combine efficiency, controllability, and quality.",
      "authors": [
        "Kaijie Xu",
        "Clark Verbrugge"
      ],
      "url": "https://arxiv.org/abs/2602.18947",
      "published": "2026-02-21T19:40:29+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.18946",
      "title": "Exponential Convergence of (Stochastic) Gradient Descent for Separable Logistic Regression",
      "abstract": "Gradient descent and stochastic gradient descent are central to modern machine learning, yet their behavior under large step sizes remains theoretically unclear. Recent work suggests that acceleration often arises near the edge of stability, where optimization trajectories become unstable and difficult to analyze. Existing results for separable logistic regression achieve faster convergence by explicitly leveraging such unstable regimes through constant or adaptive large step sizes. In this paper, we show that instability is not inherent to acceleration. We prove that gradient descent with a simple, non-adaptive increasing step-size schedule achieves exponential convergence for separable logistic regression under a margin condition, while remaining entirely within a stable optimization regime. The resulting method is anytime and does not require prior knowledge of the optimization horizon or target accuracy. We also establish exponential convergence of stochastic gradient descent using a lightweight adaptive step-size rule that avoids line search and specialized procedures, improving upon existing polynomial-rate guarantees. Together, our results demonstrate that carefully structured step-size growth alone suffices to obtain exponential acceleration for both gradient descent and stochastic gradient descent.",
      "authors": [
        "Sacchit Kale",
        "Piyushi Manupriya",
        "Pierre Marion",
        "Francis bach",
        "Anant Raj"
      ],
      "url": "https://arxiv.org/abs/2602.18946",
      "published": "2026-02-21T19:31:07+00:00",
      "categories": [
        "cs.LG",
        "math.OC"
      ]
    },
    {
      "id": "2602.18943",
      "title": "High Dimensional Procedural Content Generation",
      "abstract": "Procedural content generation (PCG) has made substantial progress in shaping static 2D/3D geometry, while most methods treat gameplay mechanics as auxiliary and optimize only over space. We argue that this limits controllability and expressivity, and formally introduce High-Dimensional PCG (HDPCG): a framework that elevates non-geometric gameplay dimensions to first-class coordinates of a joint state space. We instantiate HDPCG along two concrete directions. Direction-Space augments geometry with a discrete layer dimension and validates reachability in 4D (x,y,z,l), enabling unified treatment of 2.5D/3.5D mechanics such as gravity inversion and parallel-world switching. Direction-Time augments geometry with temporal dynamics via time-expanded graphs, capturing action semantics and conflict rules. For each direction, we present three general, practicable algorithms with a shared pipeline of abstract skeleton generation, controlled grounding, high-dimensional validation, and multi-metric evaluation. Large-scale experiments across diverse settings validate the integrity of our problem formulation and the effectiveness of our methods on playability, structure, style, robustness, and efficiency. Beyond quantitative results, Unity-based case studies recreate playable scenarios that accord with our metrics. We hope HDPCG encourages a shift in PCG toward general representations and the generation of gameplay-relevant dimensions beyond geometry, paving the way for controllable, verifiable, and extensible level generation.",
      "authors": [
        "Kaijie Xu",
        "Clark Verbrugge"
      ],
      "url": "https://arxiv.org/abs/2602.18943",
      "published": "2026-02-21T19:29:46+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.18940",
      "title": "DREAM: Deep Research Evaluation with Agentic Metrics",
      "abstract": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.",
      "authors": [
        "Elad Ben Avraham",
        "Changhao Li",
        "Ron Dorfman",
        "Roy Ganz",
        "Oren Nuriel",
        "Amir Dudai",
        "Aviad Aberdam",
        "Noah Flynn",
        "Elman Mansimov",
        "Adi Kalyanpur",
        "Ron Litman"
      ],
      "url": "https://arxiv.org/abs/2602.18940",
      "published": "2026-02-21T19:14:31+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.18934",
      "title": "LoMime: Query-Efficient Membership Inference using Model Extraction in Label-Only Settings",
      "abstract": "Membership inference attacks (MIAs) threaten the privacy of machine learning models by revealing whether a specific data point was used during training. Existing MIAs often rely on impractical assumptions such as access to public datasets, shadow models, confidence scores, or training data distribution knowledge and making them vulnerable to defenses like confidence masking and adversarial regularization. Label-only MIAs, even under strict constraints suffer from high query requirements per sample. We propose a cost-effective label-only MIA framework based on transferability and model extraction. By querying the target model M using active sampling, perturbation-based selection, and synthetic data, we extract a functionally similar surrogate S on which membership inference is performed. This shifts query overhead to a one-time extraction phase, eliminating repeated queries to M . Operating under strict black-box constraints, our method matches the performance of state-of-the-art label-only MIAs while significantly reducing query costs. On benchmarks including Purchase, Location, and Texas Hospital, we show that a query budget equivalent to testing $\\approx1\\%$ of training samples suffices to extract S and achieve membership inference accuracy within $\\pm1\\%$ of M . We also evaluate the effectiveness of standard defenses proposed for label-only MIAs against our attack.",
      "authors": [
        "Abdullah Caglar Oksuz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "url": "https://arxiv.org/abs/2602.18934",
      "published": "2026-02-21T18:57:17+00:00",
      "categories": [
        "cs.LG",
        "cs.CR"
      ]
    },
    {
      "id": "2602.18929",
      "title": "Give Users the Wheel: Towards Promptable Recommendation Paradigm",
      "abstract": "Conventional sequential recommendation models have achieved remarkable success in mining implicit behavioral patterns. However, these architectures remain structurally blind to explicit user intent: they struggle to adapt when a user's immediate goal (e.g., expressed via a natural language prompt) deviates from their historical habits. While Large Language Models (LLMs) offer the semantic reasoning to interpret such intent, existing integration paradigms force a dilemma: LLM-as-a-recommender paradigm sacrifices the efficiency and collaborative precision of ID-based retrieval, while Reranking methods are inherently bottlenecked by the recall capabilities of the underlying model. In this paper, we propose Decoupled Promptable Sequential Recommendation (DPR), a model-agnostic framework that empowers conventional sequential backbones to natively support Promptable Recommendation, the ability to dynamically steer the retrieval process using natural language without abandoning collaborative signals. DPR modulates the latent user representation directly within the retrieval space. To achieve this, we introduce a Fusion module to align the collaborative and semantic signals, a Mixture-of-Experts (MoE) architecture that disentangles the conflicting gradients from positive and negative steering, and a three-stage training strategy that progressively aligns the semantic space of prompts with the collaborative space. Extensive experiments on real-world datasets demonstrate that DPR significantly outperforms state-of-the-art baselines in prompt-guided tasks while maintaining competitive performance in standard sequential recommendation scenarios.",
      "authors": [
        "Fuyuan Lyu",
        "Chenglin Luo",
        "Qiyuan Zhang",
        "Yupeng Hou",
        "Haolun Wu",
        "Xing Tang",
        "Xue Liu",
        "Jin L. C. Guo",
        "Xiuqiang He"
      ],
      "url": "https://arxiv.org/abs/2602.18929",
      "published": "2026-02-21T18:41:28+00:00",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "id": "2602.18922",
      "title": "Why Agent Caching Fails and How to Fix It: Structured Intent Canonicalization with Few-Shot Learning",
      "abstract": "Personal AI agents incur substantial cost via repeated LLM calls. We show existing caching methods fail: GPTCache achieves 37.9% accuracy on real benchmarks; APC achieves 0-12%. The root cause is optimizing for the wrong property -- cache effectiveness requires key consistency and precision,   not classification accuracy. We observe cache-key evaluation reduces to clustering evaluation and apply V-measure decomposition to separate these on n=8,682 points across MASSIVE, BANKING77, CLINC150, and NyayaBench v2, our new 8,514-entry multilingual agentic dataset (528 intents, 20 W5H2 classes, 63 languages). We introduce W5H2, a structured intent decomposition framework. Using SetFit with 8 examples per class, W5H2 achieves 91.1%+/-1.7% on MASSIVE in ~2ms -- vs 37.9% for   GPTCache and 68.8% for a 20B-parameter LLM at 3,447ms. On NyayaBench v2 (20 classes), SetFit achieves 55.3%, with cross-lingual transfer across 30 languages. Our five-tier cascade handles 85% of interactions locally, projecting 97.5% cost reduction. We provide risk-controlled selective prediction guarantees via RCPS with nine bound families.",
      "authors": [
        "Abhinaba Basu"
      ],
      "url": "https://arxiv.org/abs/2602.18922",
      "published": "2026-02-21T18:25:18+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.18920",
      "title": "DeepInnovator: Triggering the Innovative Capabilities of LLMs",
      "abstract": "The application of Large Language Models (LLMs) in accelerating scientific discovery has garnered increasing attention, with a key focus on constructing research agents endowed with innovative capability, i.e., the ability to autonomously generate novel and significant research ideas. Existing approaches predominantly rely on sophisticated prompt engineering and lack a systematic training paradigm. To address this, we propose DeepInnovator, a training framework designed to trigger the innovative capability of LLMs. Our approach comprises two core components. (1) ``Standing on the shoulders of giants''. We construct an automated data extraction pipeline to extract and organize structured research knowledge from a vast corpus of unlabeled scientific literature. (2) ``Conjectures and refutations''. We introduce a ``Next Idea Prediction'' training paradigm, which models the generation of research ideas as an iterative process of continuously predicting, evaluating, and refining plausible and novel next idea. Both automatic and expert evaluations demonstrate that our DeepInnovator-14B significantly outperforms untrained baselines, achieving win rates of 80.53\\%-93.81\\%, and attains performance comparable to that of current leading LLMs. This work provides a scalable training pathway toward building research agents with genuine, originative innovative capability, and will open-source the dataset to foster community advancement. Source code and data are available at: https://github.com/HKUDS/DeepInnovator.",
      "authors": [
        "Tianyu Fan",
        "Fengji Zhang",
        "Yuxiang Zheng",
        "Bei Chen",
        "Xinyao Niu",
        "Chengen Huang",
        "Junyang Lin",
        "Chao Huang"
      ],
      "url": "https://arxiv.org/abs/2602.18920",
      "published": "2026-02-21T18:07:18+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "id": "2602.21251",
      "title": "AgenticTyper: Automated Typing of Legacy Software Projects Using Agentic AI",
      "abstract": "Legacy JavaScript systems lack type safety, making maintenance risky. While TypeScript can help, manually adding types is expensive. Previous automated typing research focuses on type inference but rarely addresses type checking setup, definition generation, bug identification, or behavioral correctness at repository scale. We present AgenticTyper, a Large Language Model (LLM)-based agentic system that addresses these gaps through iterative error correction and behavior preservation via transpilation comparison. Evaluation on two proprietary repositories (81K LOC) shows that AgenticTyper resolves all 633 initial type errors in 20 minutes, reducing manual effort from one working day.",
      "authors": [
        "Clemens Pohle"
      ],
      "url": "https://arxiv.org/abs/2602.21251",
      "published": "2026-02-21T17:53:30+00:00",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.MA",
        "cs.PL"
      ]
    },
    {
      "id": "2602.18918",
      "title": "Early Evidence of Vibe-Proving with Consumer LLMs: A Case Study on Spectral Region Characterization with ChatGPT-5.2 (Thinking)",
      "abstract": "Large Language Models (LLMs) are increasingly used as scientific copilots, but evidence on their role in research-level mathematics remains limited, especially for workflows accessible to individual researchers. We present early evidence for vibe-proving with a consumer subscription LLM through an auditable case study that resolves Conjecture 20 of Ran and Teng (2024) on the exact nonreal spectral region of a 4-cycle row-stochastic nonnegative matrix family. We analyze seven shareable ChatGPT-5.2 (Thinking) threads and four versioned proof drafts, documenting an iterative pipeline of generate, referee, and repair. The model is most useful for high-level proof search, while human experts remain essential for correctness-critical closure. The final theorem provides necessary and sufficient region conditions and explicit boundary attainment constructions. Beyond the mathematical result, we contribute a process-level characterization of where LLM assistance materially helps and where verification bottlenecks persist, with implications for evaluation of AI-assisted research workflows and for designing human-in-the-loop theorem proving systems.",
      "authors": [
        "Brecht Verbeken",
        "Brando Vagenende",
        "Marie-Anne Guerry",
        "Andres Algaba",
        "Vincent Ginis"
      ],
      "url": "https://arxiv.org/abs/2602.18918",
      "published": "2026-02-21T17:52:02+00:00",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.18916",
      "title": "Adaptive Collaboration of Arena-Based Argumentative LLMs for Explainable and Contestable Legal Reasoning",
      "abstract": "Legal reasoning requires not only high accuracy but also the ability to justify decisions through verifiable and contestable arguments. However, existing Large Language Model (LLM) approaches, such as Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG), often produce unstructured explanations that lack a formal mechanism for verification or user intervention. To address this limitation, we propose Adaptive Collaboration of Argumentative LLMs (ACAL), a neuro-symbolic framework that integrates adaptive multi-agent collaboration with an Arena-based Quantitative Bipolar Argumentation Framework (A-QBAF). ACAL dynamically deploys expert agent teams to construct arguments, employs a clash resolution mechanism to adjudicate conflicting claims, and utilizes uncertainty-aware escalation for borderline cases. Crucially, our framework supports a Human-in-the-Loop (HITL) contestability workflow, enabling users to directly audit and modify the underlying reasoning graph to influence the final judgment. Empirical evaluations on the LegalBench benchmark demonstrate that ACAL outperforms strong baselines across Gemini-2.5-Flash-Lite and Gemini-2.5-Flash architectures, effectively balancing efficient predictive performance with structured transparency and contestability. Our implementation is available at: https://github.com/loc110504/ACAL.",
      "authors": [
        "Hoang-Loc Cao",
        "Phuc Ho",
        "Truong Thanh Hung Nguyen",
        "Phuc Truong Loc Nguyen",
        "Dinh Thien Loc Nguyen",
        "Hung Cao"
      ],
      "url": "https://arxiv.org/abs/2602.18916",
      "published": "2026-02-21T17:47:13+00:00",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.SC"
      ]
    },
    {
      "id": "2602.18915",
      "title": "AAVGen: Precision Engineering of Adeno-associated Viral Capsids for Renal Selective Targeting",
      "abstract": "Adeno-associated viruses (AAVs) are promising vectors for gene therapy, but their native serotypes face limitations in tissue tropism, immune evasion, and production efficiency. Engineering capsids to overcome these hurdles is challenging due to the vast sequence space and the difficulty of simultaneously optimizing multiple functional properties. The complexity also adds when it comes to the kidney, which presents unique anatomical barriers and cellular targets that require precise and efficient vector engineering. Here, we present AAVGen, a generative artificial intelligence framework for de novo design of AAV capsids with enhanced multi-trait profiles. AAVGen integrates a protein language model (PLM) with supervised fine-tuning (SFT) and a reinforcement learning technique termed Group Sequence Policy Optimization (GSPO). The model is guided by a composite reward signal derived from three ESM-2-based regression predictors, each trained to predict a key property: production fitness, kidney tropism, and thermostability. Our results demonstrate that AAVGen produces a diverse library of novel VP1 protein sequences. In silico validations revealed that the majority of the generated variants have superior performance across all three employed indices, indicating successful multi-objective optimization. Furthermore, structural analysis via AlphaFold3 confirms that the generated sequences preserve the canonical capsid folding despite sequence diversification. AAVGen establishes a foundation for data-driven viral vector engineering, accelerating the development of next-generation AAV vectors with tailored functional characteristics.",
      "authors": [
        "Mohammadreza Ghaffarzadeh-Esfahani",
        "Yousof Gheisari"
      ],
      "url": "https://arxiv.org/abs/2602.18915",
      "published": "2026-02-21T17:46:34+00:00",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "id": "2602.18911",
      "title": "From Human-Level AI Tales to AI Leveling Human Scales",
      "abstract": "Comparing AI models to \"human level\" is often misleading when benchmark scores are incommensurate or human baselines are drawn from a narrow population. To address this, we propose a framework that calibrates items against the 'world population' and report performance on a common, human-anchored scale. Concretely, we build on a set of multi-level scales for different capabilities where each level should represent a probability of success of the whole world population on a logarithmic scale with a base $B$. We calibrate each scale for each capability (reasoning, comprehension, knowledge, volume, etc.) by compiling publicly released human test data spanning education and reasoning benchmarks (PISA, TIMSS, ICAR, UKBioBank, and ReliabilityBench). The base $B$ is estimated by extrapolating between samples with two demographic profiles using LLMs, with the hypothesis that they condense rich information about human populations. We evaluate the quality of different mappings using group slicing and post-stratification. The new techniques allow for the recalibration and standardization of scales relative to the whole-world population.",
      "authors": [
        "Peter Romero",
        "Fernando Martínez-Plumed",
        "Zachary R. Tyler",
        "Matthieu Téhénan",
        "Sipeng Chen",
        "Álvaro David Gómez Antón",
        "Luning Sun",
        "Manuel Cebrian",
        "Lexin Zhou",
        "Yael Moros Daval",
        "Daniel Romero-Alvarado",
        "Félix Martí Pérez",
        "Kevin Wei",
        "José Hernández-Orallo"
      ],
      "url": "https://arxiv.org/abs/2602.18911",
      "published": "2026-02-21T17:27:20+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.18910",
      "title": "SLDP: Semi-Local Differential Privacy for Density-Adaptive Analytics",
      "abstract": "Density-adaptive domain discretization is essential for high-utility privacy-preserving analytics but remains challenging under Local Differential Privacy (LDP) due to the privacy-budget costs associated with iterative refinement. We propose a novel framework, Semi-Local Differential Privacy (SLDP), that assigns a privacy region to each user based on local density and defines adjacency by the potential movement of a point within its privacy region. We present an interactive $(\\varepsilon, δ)$-SLDP protocol, orchestrated by an honest-but-curious server over a public channel, to estimate these regions privately. Crucially, our framework decouples the privacy cost from the number of refinement iterations, allowing for high-resolution grids without additional privacy budget cost. We experimentally demonstrate the framework's effectiveness on estimation tasks across synthetic and real-world datasets.",
      "authors": [
        "Alexey Kroshnin",
        "Alexandra Suvorikova"
      ],
      "url": "https://arxiv.org/abs/2602.18910",
      "published": "2026-02-21T17:26:04+00:00",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "id": "2602.18907",
      "title": "DeepInterestGR: Mining Deep Multi-Interest Using Multi-Modal LLMs for Generative Recommendation",
      "abstract": "Recent generative recommendation frameworks have demonstrated remarkable scaling potential by reformulating item prediction as autoregressive Semantic ID (SID) generation. However, existing methods primarily rely on shallow behavioral signals, encoding items solely through surface-level textual features such as titles and descriptions. This reliance results in a critical Shallow Interest problem: the model fails to capture the latent, semantically rich interests underlying user interactions, limiting both personalization depth and recommendation interpretability. DeepInterestGR introduces three key innovations: (1) Multi-LLM Interest Mining (MLIM): We leverage multiple frontier LLMs along with their multi-modal variants to extract deep textual and visual interest representations through Chain-of-Thought prompting. (2) Reward-Labeled Deep Interest (RLDI): We employ a lightweight binary classifier to assign reward labels to mined interests, enabling effective supervision signals for reinforcement learning. (3) Interest-Enhanced Item Discretization (IEID): The curated deep interests are encoded into semantic embeddings and quantized into SID tokens via RQ-VAE. We adopt a two-stage training pipeline: supervised fine-tuning aligns the generative model with deep interest signals and collaborative filtering patterns, followed by reinforcement learning with GRPO optimized by our Interest-Aware Reward. Experiments on three Amazon Review benchmarks demonstrate that DeepInterestGR consistently outperforms state-of-the-art baselines across HR@K and NDCG@K metrics.",
      "authors": [
        "Yangchen Zeng"
      ],
      "url": "https://arxiv.org/abs/2602.18907",
      "published": "2026-02-21T17:03:06+00:00",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.CY"
      ]
    },
    {
      "id": "2602.18905",
      "title": "TRUE: A Trustworthy Unified Explanation Framework for Large Language Model Reasoning",
      "abstract": "Large language models (LLMs) have demonstrated strong capabilities in complex reasoning tasks, yet their decision-making processes remain difficult to interpret. Existing explanation methods often lack trustworthy structural insight and are limited to single-instance analysis, failing to reveal reasoning stability and systematic failure mechanisms. To address these limitations, we propose the Trustworthy Unified Explanation Framework (TRUE), which integrates executable reasoning verification, feasible-region directed acyclic graph (DAG) modeling, and causal failure mode analysis. At the instance level, we redefine reasoning traces as executable process specifications and introduce blind execution verification to assess operational validity. At the local structural level, we construct feasible-region DAGs via structure-consistent perturbations, enabling explicit characterization of reasoning stability and the executable region in the local input space. At the class level, we introduce a causal failure mode analysis method that identifies recurring structural failure patterns and quantifies their causal influence using Shapley values. Extensive experiments across multiple reasoning benchmarks demonstrate that the proposed framework provides multi-level, verifiable explanations, including executable reasoning structures for individual instances, feasible-region representations for neighboring inputs, and interpretable failure modes with quantified importance at the class level. These results establish a unified and principled paradigm for improving the interpretability and reliability of LLM reasoning systems.",
      "authors": [
        "Yujiao Yang"
      ],
      "url": "https://arxiv.org/abs/2602.18905",
      "published": "2026-02-21T17:00:54+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "id": "2602.18904",
      "title": "PCA-VAE: Differentiable Subspace Quantization without Codebook Collapse",
      "abstract": "Vector-quantized autoencoders deliver high-fidelity latents but suffer inherent flaws: the quantizer is non-differentiable, requires straight-through hacks, and is prone to collapse. We address these issues at the root by replacing VQ with a simple, principled, and fully differentiable alternative: an online PCA bottleneck trained via Oja's rule. The resulting model, PCA-VAE, learns an orthogonal, variance-ordered latent basis without codebooks, commitment losses, or lookup noise. Despite its simplicity, PCA-VAE exceeds VQ-GAN and SimVQ in reconstruction quality on CelebAHQ while using 10-100x fewer latent bits. It also produces naturally interpretable dimensions (e.g., pose, lighting, gender cues) without adversarial regularization or disentanglement objectives. These results suggest that PCA is a viable replacement for VQ: mathematically grounded, stable, bit-efficient, and semantically structured, offering a new direction for generative models beyond vector quantization.",
      "authors": [
        "Hao Lu",
        "Onur C. Koyun",
        "Yongxin Guo",
        "Zhengjie Zhu",
        "Abbas Alili",
        "Metin Nafi Gurcan"
      ],
      "url": "https://arxiv.org/abs/2602.18904",
      "published": "2026-02-21T16:57:58+00:00",
      "categories": [
        "cs.LG",
        "cs.CV"
      ]
    },
    {
      "id": "2602.18899",
      "title": "[b]=[d]-[t]+[p]: Self-supervised Speech Models Discover Phonological Vector Arithmetic",
      "abstract": "Self-supervised speech models (S3Ms) are known to encode rich phonetic information, yet how this information is structured remains underexplored. We conduct a comprehensive study across 96 languages to analyze the underlying structure of S3M representations, with particular attention to phonological vectors. We first show that there exist linear directions within the model's representation space that correspond to phonological features. We further demonstrate that the scale of these phonological vectors correlate to the degree of acoustic realization of their corresponding phonological features in a continuous manner. For example, the difference between [d] and [t] yields a voicing vector: adding this vector to [p] produces [b], while scaling it results in a continuum of voicing. Together, these findings indicate that S3Ms encode speech using phonologically interpretable and compositional vectors, demonstrating phonological vector arithmetic. All code and interactive demos are available at https://github.com/juice500ml/phonetic-arithmetic .",
      "authors": [
        "Kwanghee Choi",
        "Eunjung Yeo",
        "Cheol Jun Cho",
        "David Harwath",
        "David R. Mortensen"
      ],
      "url": "https://arxiv.org/abs/2602.18899",
      "published": "2026-02-21T16:43:13+00:00",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ]
    },
    {
      "id": "2602.18897",
      "title": "HEHRGNN: A Unified Embedding Model for Knowledge Graphs with Hyperedges and Hyper-Relational Edges",
      "abstract": "Knowledge Graph(KG) has gained traction as a machine-readable organization of real-world knowledge for analytics using artificial intelligence systems. Graph Neural Network(GNN), is proven to be an effective KG embedding technique that enables various downstream tasks like link prediction, node classification, and graph classification. The focus of research in both KG embedding and GNNs has been mostly oriented towards simple graphs with binary relations. However, real-world knowledge bases have a significant share of complex and n-ary facts that cannot be represented by binary edges. More specifically, real-world knowledge bases are often a mix of two types of n-ary facts - (i) that require hyperedges and (ii) that require hyper-relational edges. Though there are research efforts catering to these n-ary fact types, they are pursued independently for each type. We propose $H$yper$E$dge $H$yper-$R$elational edge $GNN$(HEHRGNN), a unified embedding model for n-ary relational KGs with both hyperedges and hyper-relational edges. The two main components of the model are i)HEHR unified fact representation format, and ii)HEHRGNN encoder, a GNN-based encoder with a novel message propagation model capable of capturing complex graph structures comprising both hyperedges and hyper-relational edges. The experimental results of HEHRGNN on link prediction tasks show its effectiveness as a unified embedding model, with inductive prediction capability, for link prediction across real-world datasets having different types of n-ary facts. The model also shows improved link prediction performance over baseline models for hyperedge and hyper-relational datasets.",
      "authors": [
        "Rajesh Rajagopalamenon",
        "Unnikrishnan Cheramangalath"
      ],
      "url": "https://arxiv.org/abs/2602.18897",
      "published": "2026-02-21T16:38:23+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "id": "2602.18895",
      "title": "Could Large Language Models work as Post-hoc Explainability Tools in Credit Risk Models?",
      "abstract": "Post-hoc explainability is central to credit risk model governance, yet widely used tools such as coefficient-based attributions and SHapley Additive exPlanations (SHAP) often produce numerical outputs that are difficult to communicate to non-technical stakeholders. This paper investigates whether large language models (LLMs) can serve as post-hoc explainability tools for credit risk predictions through in-context learning, focusing on two roles: translators and autonomous explainers. Using a personal lending dataset from LendingClub, we evaluate three commercial LLMs, including GPT-4-turbo, Claude Sonnet 4, and Gemini-2.0-Flash. Results provide strong evidence for the translator role. In contrast, autonomous explanations show low alignment with model-based attributions. Few-shot prompting improves feature overlap for logistic regression but does not consistently benefit XGBoost, suggesting that LLMs have limited capacity to recover non-linear, interaction-driven reasoning from prompt cues alone. Our findings position LLMs as effective narrative interfaces grounded in auditable model attributions, rather than as substitutes for post-hoc explainers in credit risk model governance. Practitioners should leverage LLMs to bridge the communication gap between complex model outputs and regulatory or business stakeholders, while preserving the rigor and traceability required by credit risk governance frameworks.",
      "authors": [
        "Wenxi Geng",
        "Dingyuan Liu",
        "Liya Li",
        "Yiqing Wang"
      ],
      "url": "https://arxiv.org/abs/2602.18895",
      "published": "2026-02-21T16:35:06+00:00",
      "categories": [
        "q-fin.RM",
        "cs.LG"
      ]
    },
    {
      "id": "2602.18891",
      "title": "Orchestrating LLM Agents for Scientific Research: A Pilot Study of Multiple Choice Question (MCQ) Generation and Evaluation",
      "abstract": "Advances in large language models (LLMs) are rapidly transforming scientific work, yet empirical evidence on how these systems reshape research activities remains limited. We report a mixed-methods pilot evaluation of an AI-orchestrated research workflow in which a human researcher coordinated multiple LLM-based agents to perform data extraction, corpus construction, artifact generation, and artifact evaluation. Using the generation and assessment of multiple-choice questions (MCQs) as a testbed, we collected 1,071 SAT Math MCQs and employed LLM agents to extract questions from PDFs, retrieve and convert open textbooks into structured representations, align each MCQ with relevant textbook content, generate new MCQs under specified difficulty and cognitive levels, and evaluate both original and generated MCQs using a 24-criterion quality framework. Across all evaluations, average MCQ quality was high. However, criterion-level analysis and equivalence testing show that generated MCQs are not fully comparable to expert-vetted baseline questions. Strict similarity (24/24 criteria equivalent) was never achieved. Persistent gaps concentrated in skill\\ depth, cognitive engagement, difficulty calibration, and metadata alignment, while surface-level qualities, such as {grammar fluency}, {clarity options}, {no duplicates}, were consistently strong. Beyond MCQ outcomes, the study documents a labor shift. The researcher's work moved from ``authoring items'' toward {specification, orchestration, verification}, and {governance}. Formalizing constraints, designing rubrics, building validation loops, recovering from tool failures, and auditing provenance constituted the primary activities. We discuss implications for the future of scientific work, including emerging ``AI research operations'' skills required for AI-empowered research pipelines.",
      "authors": [
        "Yuan An"
      ],
      "url": "https://arxiv.org/abs/2602.18891",
      "published": "2026-02-21T16:20:07+00:00",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ]
    },
    {
      "id": "2602.18884",
      "title": "TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models",
      "abstract": "Multimodal Large Language Models (MLLMs), particularly smaller, deployable variants, exhibit a critical deficiency in understanding temporal and procedural visual data, a bottleneck hindering their application in real-world embodied AI. This gap is largely caused by a systemic failure in training paradigms, which lack large-scale, procedurally coherent data. To address this problem, we introduce TPRU, a large-scale dataset sourced from diverse embodied scenarios such as robotic manipulation and GUI navigation. TPRU is systematically designed to cultivate temporal reasoning through three complementary tasks: Temporal Reordering, Next-Frame Prediction, and Previous-Frame Review. A key feature is the inclusion of challenging negative samples, compelling models to transition from passive observation to active, cross-modal validation. We leverage TPRU with a reinforcement learning (RL) fine-tuning methodology, specifically targeting the enhancement of resource-efficient models. Experiments show our approach yields dramatic gains: on our manually curated TPRU-Test, the accuracy of TPRU-7B soars from 50.33\\% to 75.70\\%, a state-of-the-art result that significantly outperforms vastly larger baselines, including GPT-4o. Crucially, these capabilities generalize effectively, demonstrating substantial improvements on established benchmarks. The codebase is available at https://github.com/Stephen-gzk/TPRU/ .",
      "authors": [
        "Zhenkun Gao",
        "Xuhong Wang",
        "Xin Tan",
        "Yuan Xie"
      ],
      "url": "https://arxiv.org/abs/2602.18884",
      "published": "2026-02-21T16:10:52+00:00",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "id": "2602.18882",
      "title": "SceneTok: A Compressed, Diffusable Token Space for 3D Scenes",
      "abstract": "We present SceneTok, a novel tokenizer for encoding view sets of scenes into a compressed and diffusable set of unstructured tokens. Existing approaches for 3D scene representation and generation commonly use 3D data structures or view-aligned fields. In contrast, we introduce the first method that encodes scene information into a small set of permutation-invariant tokens that is disentangled from the spatial grid. The scene tokens are predicted by a multi-view tokenizer given many context views and rendered into novel views by employing a light-weight rectified flow decoder. We show that the compression is 1-3 orders of magnitude stronger than for other representations while still reaching state-of-the-art reconstruction quality. Further, our representation can be rendered from novel trajectories, including ones deviating from the input trajectory, and we show that the decoder gracefully handles uncertainty. Finally, the highly-compressed set of unstructured latent scene tokens enables simple and efficient scene generation in 5 seconds, achieving a much better quality-speed trade-off than previous paradigms.",
      "authors": [
        "Mohammad Asim",
        "Christopher Wewer",
        "Jan Eric Lenssen"
      ],
      "url": "https://arxiv.org/abs/2602.18882",
      "published": "2026-02-21T16:08:17+00:00",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "id": "2602.18880",
      "title": "FOCA: Frequency-Oriented Cross-Domain Forgery Detection, Localization and Explanation via Multi-Modal Large Language Model",
      "abstract": "Advances in image tampering techniques, particularly generative models, pose significant challenges to media verification, digital forensics, and public trust. Existing image forgery detection and localization (IFDL) methods suffer from two key limitations: over-reliance on semantic content while neglecting textural cues, and limited interpretability of subtle low-level tampering traces. To address these issues, we propose FOCA, a multimodal large language model-based framework that integrates discriminative features from both the RGB spatial and frequency domains via a cross-attention fusion module. This design enables accurate forgery detection and localization while providing explicit, human-interpretable cross-domain explanations. We further introduce FSE-Set, a large-scale dataset with diverse authentic and tampered images, pixel-level masks, and dual-domain annotations. Extensive experiments show that FOCA outperforms state-of-the-art methods in detection performance and interpretability across both spatial and frequency domains.",
      "authors": [
        "Zhou Liu",
        "Tonghua Su",
        "Hongshi Zhang",
        "Fuxiang Yang",
        "Donglin Di",
        "Yang Song",
        "Lei Fan"
      ],
      "url": "https://arxiv.org/abs/2602.18880",
      "published": "2026-02-21T15:53:44+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.18874",
      "title": "Structure-Level Disentangled Diffusion for Few-Shot Chinese Font Generation",
      "abstract": "Few-shot Chinese font generation aims to synthesize new characters in a target style using only a handful of reference images. Achieving accurate content rendering and faithful style transfer requires effective disentanglement between content and style. However, existing approaches achieve only feature-level disentanglement, allowing the generator to re-entangle these features, leading to content distortion and degraded style fidelity. We propose the Structure-Level Disentangled Diffusion Model (SLD-Font), which receives content and style information from two separate channels. SimSun-style images are used as content templates and concatenated with noisy latent features as the input. Style features extracted by a CLIP model from target-style images are integrated via cross-attention. Additionally, we train a Background Noise Removal module in the pixel space to remove background noise in complex stroke regions. Based on theoretical validation of disentanglement effectiveness, we introduce a parameter-efficient fine-tuning strategy that updates only the style-related modules. This allows the model to better adapt to new styles while avoiding overfitting to the reference images' content. We further introduce the Grey and OCR metrics to evaluate the content quality of generated characters. Experimental results show that SLD-Font achieves significantly higher style fidelity while maintaining comparable content accuracy to existing state-of-the-art methods.",
      "authors": [
        "Jie Li",
        "Suorong Yang",
        "Jian Zhao",
        "Furao Shen"
      ],
      "url": "https://arxiv.org/abs/2602.18874",
      "published": "2026-02-21T15:41:06+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.18873",
      "title": "BiMotion: B-spline Motion for Text-guided Dynamic 3D Character Generation",
      "abstract": "Text-guided dynamic 3D character generation has advanced rapidly, yet producing high-quality motion that faithfully reflects rich textual descriptions remains challenging. Existing methods tend to generate limited sub-actions or incoherent motion due to fixed-length temporal inputs and discrete frame-wise representations that fail to capture rich motion semantics. We address these limitations by representing motion with continuous differentiable B-spline curves, enabling more effective motion generation without modifying the capabilities of the underlying generative model. Specifically, our closed-form, Laplacian-regularized B-spline solver efficiently compresses variable-length motion sequences into compact representations with a fixed number of control points. Further, we introduce a normal-fusion strategy for input shape adherence along with correspondence-aware and local-rigidity losses for motion-restoration quality. To train our model, we collate BIMO, a new dataset containing diverse variable-length 3D motion sequences with rich, high-quality text annotations. Extensive evaluations show that our feed-forward framework BiMotion generates more expressive, higher-quality, and better prompt-aligned motions than existing state-of-the-art methods, while also achieving faster generation. Our project page is at: https://wangmiaowei.github.io/BiMotion.github.io/.",
      "authors": [
        "Miaowei Wang",
        "Qingxuan Yan",
        "Zhi Cao",
        "Yayuan Li",
        "Oisin Mac Aodha",
        "Jason J. Corso",
        "Amir Vaxman"
      ],
      "url": "https://arxiv.org/abs/2602.18873",
      "published": "2026-02-21T15:40:37+00:00",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "id": "2602.18870",
      "title": "Federated Measurement of Demographic Disparities from Quantile Sketches",
      "abstract": "Many fairness goals are defined at a population level that misaligns with siloed data collection, which remains unsharable due to privacy regulations. Horizontal federated learning (FL) enables collaborative modeling across clients with aligned features without sharing raw data. We study federated auditing of demographic parity through score distributions, measuring disparity as a Wasserstein--Frechet variance between sensitive-group score laws, and expressing the population metric in federated form that makes explicit how silo-specific selection drives local-global mismatch. For the squared Wasserstein distance, we prove an ANOVA-style decomposition that separates (i) selection-induced mixture effects from (ii) cross-silo heterogeneity, yielding tight bounds linking local and global metrics. We then propose a one-shot, communication-efficient protocol in which each silo shares only group counts and a quantile summary of its local score distributions, enabling the server to estimate global disparity and its decomposition, with $O(1/k)$ discretization bias ($k$ quantiles) and finite-sample guarantees. Experiments on synthetic data and COMPAS show that a few dozen quantiles suffice to recover global disparity and diagnose its sources.",
      "authors": [
        "Arthur Charpentier",
        "Agathe Fernandes Machado",
        "Olivier Côté",
        "François Hu"
      ],
      "url": "https://arxiv.org/abs/2602.18870",
      "published": "2026-02-21T15:34:44+00:00",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "id": "2602.18868",
      "title": "Limits of Convergence-Rate Control for Open-Weight Safety",
      "abstract": "Open-weight foundation models can be fine-tuned for harmful purposes after release, yet no existing training resistance methods provide theoretical guarantees. Treating these interventions as convergence-rate control problems allows us to connect optimization speed to the spectral structure of model weights. We leverage this insight to develop a novel understanding of convergence rate control through spectral reparameterization and derive an algorithm, SpecDef, that can both provably and empirically slow first- and second-order optimization in non-adversarial settings. In adversarial settings, we establish a fundamental limit on a broad class of convergence rate control methods including our own: an attacker with sufficient knowledge can restore fast convergence at a linear increase in model size. In order to overcome this limitation, future works will need to investigate methods that are not equivalent to controlling convergence rate.",
      "authors": [
        "Domenic Rosati",
        "Xijie Zeng",
        "Hong Huang",
        "Sebastian Dionicio",
        "Subhabrata Majumdar",
        "Frank Rudzicz",
        "Hassan Sajjad"
      ],
      "url": "https://arxiv.org/abs/2602.18868",
      "published": "2026-02-21T15:32:27+00:00",
      "categories": [
        "math.OC",
        "cs.LG"
      ]
    },
    {
      "id": "2602.18866",
      "title": "Boosting for Vector-Valued Prediction and Conditional Density Estimation",
      "abstract": "Despite the widespread use of boosting in structured prediction, a general theoretical understanding of aggregation beyond scalar losses remains incomplete. We study vector-valued and conditional density prediction under general divergences and identify stability conditions under which aggregation amplifies weak guarantees into strong ones.   We formalize this stability property as \\emph{$(α,β)$-boostability}. We show that geometric median aggregation achieves $(α,β)$-boostability for a broad class of divergences, with tradeoffs that depend on the underlying geometry. For vector-valued prediction and conditional density estimation, we characterize boostability under common divergences ($\\ell_1$, $\\ell_2$, $\\TV$, and $\\Hel$) with geometric median, revealing a sharp distinction between dimension-dependent and dimension-free regimes. We further show that while KL divergence is not directly boostable via geometric median aggregation, it can be handled indirectly through boostability under Hellinger distance.   Building on these structural results, we propose a generic boosting framework \\textsc{GeoMedBoost} based on exponential reweighting and geometric-median aggregation. Under a weak learner condition and $(α,β)$-boostability, we obtain exponential decay of the empirical divergence exceedance error. Our framework recovers classical algorithms such as \\textsc{MedBoost}, \\textsc{AdaBoost}, and \\textsc{SAMME} as special cases, and provides a unified geometric view of boosting for structured prediction.",
      "authors": [
        "Jian Qian",
        "Shu Ge"
      ],
      "url": "https://arxiv.org/abs/2602.18866",
      "published": "2026-02-21T15:21:36+00:00",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "id": "2602.18863",
      "title": "TIACam: Text-Anchored Invariant Feature Learning with Auto-Augmentation for Camera-Robust Zero-Watermarking",
      "abstract": "Camera recapture introduces complex optical degradations, such as perspective warping, illumination shifts, and Moiré interference, that remain challenging for deep watermarking systems. We present TIACam, a text-anchored invariant feature learning framework with auto-augmentation for camera-robust zero-watermarking. The method integrates three key innovations: (1) a learnable auto-augmentor that discovers camera-like distortions through differentiable geometric, photometric, and Moiré operators; (2) a text-anchored invariant feature learner that enforces semantic consistency via cross-modal adversarial alignment between image and text; and (3) a zero-watermarking head that binds binary messages in the invariant feature space without modifying image pixels. This unified formulation jointly optimizes invariance, semantic alignment, and watermark recoverability. Extensive experiments on both synthetic and real-world camera captures demonstrate that TIACam achieves state-of-the-art feature stability and watermark extraction accuracy, establishing a principled bridge between multimodal invariance learning and physically robust zero-watermarking.",
      "authors": [
        "Abdullah All Tanvir",
        "Agnibh Dasgupta",
        "Xin Zhong"
      ],
      "url": "https://arxiv.org/abs/2602.18863",
      "published": "2026-02-21T15:06:16+00:00",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ]
    }
  ]
}